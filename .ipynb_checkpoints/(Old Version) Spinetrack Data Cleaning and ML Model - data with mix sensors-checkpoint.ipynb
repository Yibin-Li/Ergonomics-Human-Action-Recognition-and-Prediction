{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and reformating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\\\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from random import shuffle, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['TimeSec', 'Sensor', 'Quatx', 'Quaty', 'Quatz', 'Quat0', 'Heading',\n",
    "       'Pitch', 'Roll', 'LinAccx', 'LinAccy', 'LinAccz', 'Vbat', 'Accx',\n",
    "       'Accy', 'Accz', 'Gyrox', 'Gyroy', 'Gyroz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Take a look at one dataset\n",
    "# file_path = \"./spinetrack data/Task_Drilling_Stoop.csv\"\n",
    "# df = pd.read_csv(file_path, error_bad_lines=False)\n",
    "# df.columns = header\n",
    "\n",
    "# df[\"Activity\"] = [1 for i in range(len(df))]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_lst = []\n",
    "index_dict = {}\n",
    "for i in sorted(os.listdir('./Spinetrack Data/data/Alex_data/Processed_redone')):\n",
    "    if i.endswith(\".csv\") and i != 'super_features.csv': \n",
    "        # should we combinme same activities together?\n",
    "        act_lst = i.split('.')[0].split('_')\n",
    "        if len(act_lst) > 1 and act_lst[1] == 'OneH':\n",
    "            act = act_lst[0] + '_' + act_lst[1]\n",
    "        else:\n",
    "            act = act_lst[0]\n",
    "        #act = i.split('.')[0]\n",
    "        index_lst.append(act)\n",
    "\n",
    "# delete multiple items\n",
    "index_set = set(index_lst)\n",
    "index_set.remove('Static')\n",
    "index_set.remove('L')\n",
    "index_set.remove('L_OneH')\n",
    "index_set.add('Static_Stoop')\n",
    "index_set.add('Lifting')\n",
    "index_set.add('Lifting_OneH')\n",
    "\n",
    "index_dict = {act:i for i, act in enumerate(index_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pulling_OneH': 0, 'Overhead': 1, 'Pulling': 2, 'Sitting': 3, 'Lifting': 4, 'Crawling': 5, 'Standing': 6, 'Carrying': 7, 'Walking': 8, 'Pushing': 9, 'Reaching': 10, 'Static_Stoop': 11, 'Kneeling': 12, 'Lifting_OneH': 13, 'Crouching': 14}\n"
     ]
    }
   ],
   "source": [
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex_data number of activity: 44\n",
      "Alexander_data number of activity: 43\n",
      "Charlotte_data number of activity: 43\n",
      "Christian_data number of activity: 34\n",
      "Elias_data number of activity: 44\n",
      "Jesse_data number of activity: 44\n",
      "Jiyoo_data number of activity: 43\n"
     ]
    }
   ],
   "source": [
    "directory = \"./Spinetrack Data/data/\"\n",
    "file = []\n",
    "\n",
    "for d in sorted(os.listdir(directory)):\n",
    "    if d != '.DS_Store':\n",
    "        files = directory + d + \"/Processed_redone\"\n",
    "        name = []\n",
    "        try:\n",
    "            for f in sorted(os.listdir(files)):\n",
    "                if f.endswith(\".csv\") and f != 'super_features.csv': \n",
    "                    name.append(files + \"/\" + f)\n",
    "            file.append(name)\n",
    "            print(\"%s number of activity: %s\" %(d, len(name)))\n",
    "        except:\n",
    "            pass\n",
    "#file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = []\n",
    "for person in file:\n",
    "    for file_path in person:\n",
    "        activity_name = ''\n",
    "        activity_lst = file_path.split(\"/\")[-1].split('.')[0].split('_')\n",
    "        \n",
    "        if activity_lst[0] == 'L' and activity_lst[1] == 'OneH':\n",
    "            activity_name = 'Lifting_OneH'\n",
    "        elif activity_lst[0] == 'L' and activity_lst[1] != 'OneH':\n",
    "            activity_name = 'Lifting'\n",
    "        elif activity_lst[0] == 'Static':\n",
    "            activity_name = 'Static_Stoop'\n",
    "        elif activity_lst[0] == 'Pulling' and activity_lst[1] == 'OneH':\n",
    "            activity_name = 'Pulling_OneH'\n",
    "        else:\n",
    "            activity_name = activity_lst[0]\n",
    "#         print(activity_name)\n",
    "#         print(\"processing: \", file_path)\n",
    "        if activity_name in index_dict.keys():\n",
    "            df = pd.read_csv(file_path, error_bad_lines=False)\n",
    "            df.columns = header\n",
    "            df[\"activity\"] = [index_dict[activity_name] for i in range(len(df))]\n",
    "            frame.append(df)\n",
    "            \n",
    "            # mkdir if not exist. Save to local csv file\n",
    "#             if not (os.path.exists('./Spinetrack Data/Yibin_Processed/' + category + folder_name)):\n",
    "#                 os.makedirs('./Spinetrack Data/Yibin_Processed/' + category + folder_name)\n",
    "#             csv_name = '/' + activity_name + '.csv'\n",
    "#             folder_name = file_path.split(\"/\")[-3] # person's name \n",
    "#             category = 'data/' # data or task\n",
    "#             file_name = './Spinetrack Data/Yibin_Processed/' + category + folder_name + csv_name\n",
    "#             df.to_csv(file_name) # save csv processed file to local\n",
    "            \n",
    "            #print(file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeSec</th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Quatx</th>\n",
       "      <th>Quaty</th>\n",
       "      <th>Quatz</th>\n",
       "      <th>Quat0</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>Roll</th>\n",
       "      <th>LinAccx</th>\n",
       "      <th>LinAccy</th>\n",
       "      <th>LinAccz</th>\n",
       "      <th>Vbat</th>\n",
       "      <th>Accx</th>\n",
       "      <th>Accy</th>\n",
       "      <th>Accz</th>\n",
       "      <th>Gyrox</th>\n",
       "      <th>Gyroy</th>\n",
       "      <th>Gyroz</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0362</td>\n",
       "      <td>2</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>306.93</td>\n",
       "      <td>25.57</td>\n",
       "      <td>-16.37</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>16.33</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0377</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.71</td>\n",
       "      <td>113.60</td>\n",
       "      <td>38.21</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-37.84</td>\n",
       "      <td>-4.27</td>\n",
       "      <td>53.71</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.77</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0961</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>192.97</td>\n",
       "      <td>-9.34</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>36.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0978</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>38.13</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.81</td>\n",
       "      <td>-18.92</td>\n",
       "      <td>-64.70</td>\n",
       "      <td>118.26</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.61</td>\n",
       "      <td>116.18</td>\n",
       "      <td>-5.60</td>\n",
       "      <td>82.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3.87</td>\n",
       "      <td>5.19</td>\n",
       "      <td>45.62</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.35</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>206.37</td>\n",
       "      <td>-9.81</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>18.31</td>\n",
       "      <td>61.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.84</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1069</td>\n",
       "      <td>4</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>300.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-90.39</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>3.79</td>\n",
       "      <td>8.85</td>\n",
       "      <td>-50.35</td>\n",
       "      <td>-32.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1086</td>\n",
       "      <td>7</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>42.78</td>\n",
       "      <td>15.05</td>\n",
       "      <td>16.65</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-14.34</td>\n",
       "      <td>-24.11</td>\n",
       "      <td>85.60</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1283</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>310.55</td>\n",
       "      <td>24.85</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-31.13</td>\n",
       "      <td>-19.23</td>\n",
       "      <td>33.87</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.77</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1301</td>\n",
       "      <td>3</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.74</td>\n",
       "      <td>119.48</td>\n",
       "      <td>37.14</td>\n",
       "      <td>5.69</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-56.00</td>\n",
       "      <td>1.37</td>\n",
       "      <td>29.91</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.68</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1662</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>199.05</td>\n",
       "      <td>-10.01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.92</td>\n",
       "      <td>10.53</td>\n",
       "      <td>-7.32</td>\n",
       "      <td>64.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1678</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>43.15</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.81</td>\n",
       "      <td>-13.58</td>\n",
       "      <td>-64.85</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1.05</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1982</td>\n",
       "      <td>5</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.64</td>\n",
       "      <td>121.61</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>81.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>7.48</td>\n",
       "      <td>67.75</td>\n",
       "      <td>37.99</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>211.95</td>\n",
       "      <td>-9.58</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>26.40</td>\n",
       "      <td>3.81</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2036</td>\n",
       "      <td>4</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>306.61</td>\n",
       "      <td>-7.64</td>\n",
       "      <td>-90.53</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.79</td>\n",
       "      <td>16.17</td>\n",
       "      <td>-65.77</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2050</td>\n",
       "      <td>7</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>50.00</td>\n",
       "      <td>12.38</td>\n",
       "      <td>15.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-39.06</td>\n",
       "      <td>18.62</td>\n",
       "      <td>69.12</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.2443</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.99</td>\n",
       "      <td>205.54</td>\n",
       "      <td>-10.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>14.04</td>\n",
       "      <td>7.93</td>\n",
       "      <td>55.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.2459</td>\n",
       "      <td>3</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>126.80</td>\n",
       "      <td>36.64</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-59.51</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>93.69</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.79</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2642</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>316.15</td>\n",
       "      <td>24.21</td>\n",
       "      <td>-17.51</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-36.01</td>\n",
       "      <td>-13.12</td>\n",
       "      <td>54.47</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.87</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.2659</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>47.90</td>\n",
       "      <td>-6.24</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.81</td>\n",
       "      <td>21.36</td>\n",
       "      <td>-20.60</td>\n",
       "      <td>82.86</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.02</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.3011</td>\n",
       "      <td>5</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.68</td>\n",
       "      <td>128.82</td>\n",
       "      <td>-11.79</td>\n",
       "      <td>80.05</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>17.85</td>\n",
       "      <td>71.72</td>\n",
       "      <td>24.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.38</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.3028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>220.57</td>\n",
       "      <td>-8.91</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.91</td>\n",
       "      <td>19.84</td>\n",
       "      <td>5.95</td>\n",
       "      <td>92.77</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.3065</td>\n",
       "      <td>4</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>315.36</td>\n",
       "      <td>-6.74</td>\n",
       "      <td>-90.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.02</td>\n",
       "      <td>-74.31</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.3080</td>\n",
       "      <td>7</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>60.57</td>\n",
       "      <td>12.96</td>\n",
       "      <td>14.21</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-22.28</td>\n",
       "      <td>32.50</td>\n",
       "      <td>104.06</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.01</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.3251</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.98</td>\n",
       "      <td>212.51</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.92</td>\n",
       "      <td>13.12</td>\n",
       "      <td>9.77</td>\n",
       "      <td>85.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.3266</td>\n",
       "      <td>3</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.83</td>\n",
       "      <td>137.50</td>\n",
       "      <td>36.95</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-35.86</td>\n",
       "      <td>18.31</td>\n",
       "      <td>78.58</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.3993</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>324.52</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-16.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-21.97</td>\n",
       "      <td>-21.51</td>\n",
       "      <td>90.33</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.4009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>229.17</td>\n",
       "      <td>-10.88</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>19.99</td>\n",
       "      <td>-8.39</td>\n",
       "      <td>61.65</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.4048</td>\n",
       "      <td>5</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.70</td>\n",
       "      <td>136.72</td>\n",
       "      <td>-13.27</td>\n",
       "      <td>79.93</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.87</td>\n",
       "      <td>17.70</td>\n",
       "      <td>78.58</td>\n",
       "      <td>24.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.4065</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>52.68</td>\n",
       "      <td>-9.29</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.81</td>\n",
       "      <td>12.66</td>\n",
       "      <td>-25.63</td>\n",
       "      <td>51.27</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>54.2570</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>133.41</td>\n",
       "      <td>3.77</td>\n",
       "      <td>-6.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.74</td>\n",
       "      <td>35.71</td>\n",
       "      <td>-16.94</td>\n",
       "      <td>-22.74</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4252</th>\n",
       "      <td>54.2600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>14.18</td>\n",
       "      <td>-19.14</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.88</td>\n",
       "      <td>-12.97</td>\n",
       "      <td>-44.71</td>\n",
       "      <td>21.82</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4253</th>\n",
       "      <td>54.2620</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>54.3000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>201.75</td>\n",
       "      <td>7.13</td>\n",
       "      <td>22.33</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.44</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>-109.86</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>54.3010</td>\n",
       "      <td>5</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>262.61</td>\n",
       "      <td>12.97</td>\n",
       "      <td>47.20</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>233.15</td>\n",
       "      <td>-25.94</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.07</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4256</th>\n",
       "      <td>54.3500</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>28.27</td>\n",
       "      <td>-27.88</td>\n",
       "      <td>-22.85</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.77</td>\n",
       "      <td>24.41</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-55.54</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>54.3510</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>29.15</td>\n",
       "      <td>-18.64</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>3.83</td>\n",
       "      <td>22.13</td>\n",
       "      <td>17.24</td>\n",
       "      <td>29.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.76</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>54.3550</td>\n",
       "      <td>3</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>349.90</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>-84.53</td>\n",
       "      <td>-65.00</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>54.3570</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>126.18</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-5.53</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.58</td>\n",
       "      <td>5.04</td>\n",
       "      <td>-91.09</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>54.4190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16.87</td>\n",
       "      <td>-20.28</td>\n",
       "      <td>-7.19</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.88</td>\n",
       "      <td>9.46</td>\n",
       "      <td>22.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>54.4210</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4262</th>\n",
       "      <td>54.4250</td>\n",
       "      <td>7</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>201.15</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>21.50</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-28.69</td>\n",
       "      <td>-94.60</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4263</th>\n",
       "      <td>54.4270</td>\n",
       "      <td>5</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>257.84</td>\n",
       "      <td>17.62</td>\n",
       "      <td>63.30</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.82</td>\n",
       "      <td>106.35</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-69.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.67</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>54.4470</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>27.62</td>\n",
       "      <td>-27.18</td>\n",
       "      <td>-18.53</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.77</td>\n",
       "      <td>44.40</td>\n",
       "      <td>5.65</td>\n",
       "      <td>38.76</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>54.4480</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>32.37</td>\n",
       "      <td>-18.29</td>\n",
       "      <td>-6.35</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.83</td>\n",
       "      <td>12.05</td>\n",
       "      <td>-4.58</td>\n",
       "      <td>30.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.03</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>54.4520</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>339.36</td>\n",
       "      <td>-9.22</td>\n",
       "      <td>6.64</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.76</td>\n",
       "      <td>35.71</td>\n",
       "      <td>-86.06</td>\n",
       "      <td>-163.88</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.19</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>54.4530</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>120.08</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-3.86</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.74</td>\n",
       "      <td>47.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1.08</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>54.4570</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17.70</td>\n",
       "      <td>-19.11</td>\n",
       "      <td>-6.88</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-5.95</td>\n",
       "      <td>3.66</td>\n",
       "      <td>-4.73</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.88</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>54.4590</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>54.5070</td>\n",
       "      <td>7</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>202.84</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>17.79</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-22.58</td>\n",
       "      <td>-64.24</td>\n",
       "      <td>83.92</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>54.5080</td>\n",
       "      <td>5</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>257.17</td>\n",
       "      <td>20.78</td>\n",
       "      <td>65.46</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>3.82</td>\n",
       "      <td>-65.46</td>\n",
       "      <td>5.49</td>\n",
       "      <td>14.65</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>54.5260</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>25.42</td>\n",
       "      <td>-24.87</td>\n",
       "      <td>-13.42</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3.77</td>\n",
       "      <td>32.50</td>\n",
       "      <td>50.96</td>\n",
       "      <td>-88.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>54.5270</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>32.49</td>\n",
       "      <td>-16.52</td>\n",
       "      <td>-6.97</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-27.77</td>\n",
       "      <td>30.52</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>54.5590</td>\n",
       "      <td>3</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>324.36</td>\n",
       "      <td>-12.77</td>\n",
       "      <td>13.73</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>3.76</td>\n",
       "      <td>42.72</td>\n",
       "      <td>-35.25</td>\n",
       "      <td>-87.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>54.5610</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>54.5640</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17.20</td>\n",
       "      <td>-19.03</td>\n",
       "      <td>-7.60</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-10.68</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-11.75</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>54.5660</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>122.58</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.74</td>\n",
       "      <td>41.81</td>\n",
       "      <td>5.04</td>\n",
       "      <td>32.81</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>54.6100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>210.16</td>\n",
       "      <td>-17.23</td>\n",
       "      <td>12.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-41.81</td>\n",
       "      <td>-44.40</td>\n",
       "      <td>69.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.98</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>54.6110</td>\n",
       "      <td>5</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>259.25</td>\n",
       "      <td>16.95</td>\n",
       "      <td>52.48</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.82</td>\n",
       "      <td>-240.02</td>\n",
       "      <td>-14.19</td>\n",
       "      <td>66.38</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.36</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>54.6620</td>\n",
       "      <td>3</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>315.13</td>\n",
       "      <td>-11.78</td>\n",
       "      <td>21.63</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>3.76</td>\n",
       "      <td>58.90</td>\n",
       "      <td>-7.48</td>\n",
       "      <td>-72.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.79</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986091 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TimeSec  Sensor  Quatx  Quaty  Quatz  Quat0  Heading  Pitch   Roll  \\\n",
       "0      0.0362       2  0.822   0.01   0.26  -0.51   306.93  25.57 -16.37   \n",
       "1      0.0377       3  0.625  -0.21   0.26   0.71   113.60  38.21   7.42   \n",
       "2      0.0961       0  0.008   0.08   0.02   1.00   192.97  -9.34   2.16   \n",
       "3      0.0978       6 -0.978   0.03  -0.00  -0.21    38.13   1.13  -2.73   \n",
       "4      0.1018       5  0.448   0.44   0.49   0.61   116.18  -5.60  82.09   \n",
       "5      0.1032       1  0.105  -0.09   0.01  -0.99   206.37  -9.81  -2.63   \n",
       "6      0.1069       4  0.540  -0.59   0.39  -0.45   300.68  -6.56 -90.39   \n",
       "7      0.1086       7  0.955   0.11   0.16   0.23    42.78  15.05  16.65   \n",
       "8      0.1283       2  0.838  -0.01   0.26  -0.48   310.55  24.85 -16.97   \n",
       "9      0.1301       3  0.586  -0.22   0.23   0.74   119.48  37.14   5.69   \n",
       "10     0.1662       0 -0.045   0.09   0.02   1.00   199.05 -10.01   1.58   \n",
       "11     0.1678       6 -0.968   0.01   0.03  -0.25    43.15  -3.44  -1.57   \n",
       "12     0.1982       5  0.411   0.43   0.49   0.64   121.61  -7.94  81.38   \n",
       "13     0.1998       1  0.154  -0.09   0.00  -0.98   211.95  -9.58  -2.03   \n",
       "14     0.2036       4  0.558  -0.62   0.35  -0.43   306.61  -7.64 -90.53   \n",
       "15     0.2050       7  0.942   0.09   0.14   0.29    50.00  12.38  15.05   \n",
       "16     0.2443       0 -0.101   0.09   0.03   0.99   205.54 -10.56   1.96   \n",
       "17     0.2459       3  0.535  -0.24   0.20   0.78   126.80  36.64   4.11   \n",
       "18     0.2642       2  0.861  -0.03   0.25  -0.44   316.15  24.21 -17.51   \n",
       "19     0.2659       6 -0.955  -0.00   0.06  -0.29    47.90  -6.24  -1.72   \n",
       "20     0.3011       5  0.355   0.41   0.50   0.68   128.82 -11.79  80.05   \n",
       "21     0.3028       1  0.228  -0.08  -0.01  -0.97   220.57  -8.91  -1.17   \n",
       "22     0.3065       4  0.592  -0.64   0.31  -0.38   315.36  -6.74 -90.54   \n",
       "23     0.3080       7  0.911   0.07   0.15   0.38    60.57  12.96  14.21   \n",
       "24     0.3251       0 -0.162   0.09   0.04   0.98   212.51 -10.29   2.50   \n",
       "25     0.3266       3  0.461  -0.26   0.19   0.83   137.50  36.95   5.13   \n",
       "26     0.3993       2  0.891  -0.04   0.25  -0.38   324.52  24.14 -16.14   \n",
       "27     0.4009       1  0.301  -0.09  -0.03  -0.95   229.17 -10.88   0.46   \n",
       "28     0.4048       5  0.300   0.38   0.52   0.70   136.72 -13.27  79.93   \n",
       "29     0.4065       6 -0.941  -0.02   0.08  -0.33    52.68  -9.29  -1.28   \n",
       "...       ...     ...    ...    ...    ...    ...      ...    ...    ...   \n",
       "4251  54.2570       4 -0.502   0.06   0.03  -0.86   133.41   3.77  -6.55   \n",
       "4252  54.2600       0  0.985  -0.05  -0.17  -0.01    14.18 -19.14  -5.33   \n",
       "4253  54.2620       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "4254  54.3000       7  0.054   0.07  -0.19  -0.98   201.75   7.13  22.33   \n",
       "4255  54.3010       5  0.475   0.31  -0.27  -0.78   262.61  12.97  47.20   \n",
       "4256  54.3500       6 -0.950   0.16   0.26  -0.07    28.27 -27.88 -22.85   \n",
       "4257  54.3510       1 -0.977   0.05   0.17  -0.12    29.15 -18.64  -8.00   \n",
       "4258  54.3550       3  0.977   0.04  -0.03  -0.21   349.90  -2.25   4.98   \n",
       "4259  54.3570       4 -0.557   0.05   0.03  -0.83   126.18   2.60  -5.53   \n",
       "4260  54.4190       0  0.982  -0.06  -0.18   0.01    16.87 -20.28  -7.19   \n",
       "4261  54.4210       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "4262  54.4250       7  0.063   0.00  -0.19  -0.98   201.15  -1.15  21.50   \n",
       "4263  54.4270       5  0.376   0.38  -0.37  -0.76   257.84  17.62  63.30   \n",
       "4264  54.4470       6 -0.957   0.13   0.25  -0.08    27.62 -27.18 -18.53   \n",
       "4265  54.4480       1 -0.975   0.03   0.17  -0.15    32.37 -18.29  -6.35   \n",
       "4266  54.4520       3  0.951   0.03  -0.09  -0.29   339.36  -9.22   6.64   \n",
       "4267  54.4530       4 -0.600   0.04   0.01  -0.80   120.08   2.60  -3.86   \n",
       "4268  54.4570       0  0.984  -0.05  -0.17   0.02    17.70 -19.11  -6.88   \n",
       "4269  54.4590       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "4270  54.5070       7  0.089  -0.07  -0.16  -0.98   202.84  -9.75  17.79   \n",
       "4271  54.5080       5  0.350   0.41  -0.37  -0.76   257.17  20.78  65.46   \n",
       "4272  54.5260       6 -0.968   0.09   0.22  -0.07    25.42 -24.87 -13.42   \n",
       "4273  54.5270       1 -0.976   0.04   0.15  -0.15    32.49 -16.52  -6.97   \n",
       "4274  54.5590       3  0.901   0.06  -0.15  -0.40   324.36 -12.77  13.73   \n",
       "4275  54.5610       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "4276  54.5640       0  0.984  -0.06  -0.17   0.02    17.20 -19.03  -7.60   \n",
       "4277  54.5660       4 -0.584   0.01  -0.03  -0.81   122.58   2.78   1.97   \n",
       "4278  54.6100       7  0.153  -0.13  -0.12  -0.97   210.16 -17.23  12.17   \n",
       "4279  54.6110       5  0.423   0.35  -0.30  -0.78   259.25  16.95  52.48   \n",
       "4280  54.6620       3  0.860   0.11  -0.18  -0.46   315.13 -11.78  21.63   \n",
       "\n",
       "      LinAccx  LinAccy  LinAccz  Vbat    Accx   Accy    Accz  Gyrox  Gyroy  \\\n",
       "0       -0.03    -0.03    -0.02  3.94  -19.84 -11.14   16.33  -0.47  -0.28   \n",
       "1        0.06    -0.08    -0.08  3.80  -37.84  -4.27   53.71  -0.51   0.03   \n",
       "2        0.11    -0.13    -0.02  3.92    2.75  -1.68   36.01   0.07  -0.10   \n",
       "3        0.08     0.36     0.33  3.81  -18.92 -64.70  118.26  -0.13   0.07   \n",
       "4        0.26    -0.13    -0.21  3.87    5.19  45.62   -2.90   0.15   0.67   \n",
       "5       -0.01     0.04    -0.05  3.91   -3.51  18.31   61.04   0.20  -0.02   \n",
       "6        0.10     0.06    -0.26  3.79    8.85 -50.35  -32.35   0.15  -1.09   \n",
       "7        0.11     0.29    -0.22  0.00  -14.34 -24.11   85.60  -0.28  -0.09   \n",
       "8       -0.12     0.06    -0.06  3.94  -31.13 -19.23   33.87  -0.37  -0.39   \n",
       "9       -0.01     0.01    -0.16  3.80  -56.00   1.37   29.91  -0.50   0.10   \n",
       "10       0.04    -0.03    -0.01  3.92   10.53  -7.32   64.24   0.13   0.00   \n",
       "11       0.21     0.33    -0.01  3.81  -13.58 -64.85    7.32   0.03  -0.42   \n",
       "12       0.43    -0.03    -0.02  3.87    7.48  67.75   37.99   0.04   0.72   \n",
       "13      -0.01    -0.02    -0.05  3.91   26.40   3.81   61.49   0.17  -0.07   \n",
       "14       0.21    -0.00    -0.03  3.79   16.17 -65.77   -1.83   0.20  -1.04   \n",
       "15      -0.00     0.19    -0.02  0.00  -39.06  18.62   69.12  -0.31   0.10   \n",
       "16       0.04     0.05     0.02  3.92   14.04   7.93   55.54   0.15   0.09   \n",
       "17       0.00     0.00    -0.02  3.80  -59.51  -0.31   93.69  -0.60   0.08   \n",
       "18      -0.01     0.08    -0.03  3.94  -36.01 -13.12   54.47  -0.35  -0.35   \n",
       "19      -0.12     0.09     0.01  3.81   21.36 -20.60   82.86  -0.04  -0.03   \n",
       "20       0.29     0.05    -0.02  3.87   17.85  71.72   24.41   0.10   0.77   \n",
       "21      -0.11     0.17    -0.01  3.91   19.84   5.95   92.77   0.34   0.10   \n",
       "22       0.27    -0.05    -0.26  3.79    7.02 -74.31    2.44   0.35  -0.95   \n",
       "23      -0.01     0.16     0.09  0.00  -22.28  32.50  104.06  -0.38   0.20   \n",
       "24       0.04     0.03    -0.11  3.92   13.12   9.77   85.14   0.14   0.10   \n",
       "25       0.00     0.09     0.11  3.80  -35.86  18.31   78.58  -0.71   0.12   \n",
       "26       0.09     0.13    -0.13  3.94  -21.97 -21.51   90.33  -0.22  -0.24   \n",
       "27       0.15     0.10    -0.05  3.91   19.99  -8.39   61.65   0.12   0.17   \n",
       "28       0.15     0.02    -0.04  3.87   17.70  78.58   24.87   0.12   0.95   \n",
       "29       0.20    -0.05    -0.04  3.81   12.66 -25.63   51.27   0.34  -0.08   \n",
       "...       ...      ...      ...   ...     ...    ...     ...    ...    ...   \n",
       "4251     0.15     0.16    -0.13  3.74   35.71 -16.94  -22.74  -0.27  -0.15   \n",
       "4252    -0.45    -0.07     0.21  3.88  -12.97 -44.71   21.82  -0.08  -0.07   \n",
       "4253     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "4254     0.72     0.15     0.66  0.00   49.44 -69.58 -109.86  -0.46   0.07   \n",
       "4255    -0.63    -0.20    -0.11  3.82  233.15 -25.94  -69.58  -0.16   0.19   \n",
       "4256    -0.30    -0.23     0.16  3.77   24.41   1.07  -55.54   0.33  -0.17   \n",
       "4257    -0.02    -0.23    -0.17  3.83   22.13  17.24   29.60   0.36   0.11   \n",
       "4258    -0.10    -0.11     0.14  3.76   -3.20 -84.53  -65.00  -0.08   0.14   \n",
       "4259     0.27     0.11    -0.12  3.74    4.58   5.04  -91.09  -0.25  -0.30   \n",
       "4260    -0.48    -0.12    -0.05  3.88    9.46  22.58   24.26  -0.15   0.04   \n",
       "4261     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "4262    -0.44     0.31     0.09  0.00  -28.69 -94.60   11.44   0.36   0.34   \n",
       "4263    -0.47    -0.28    -0.56  3.82  106.35  -0.61  -69.12  -0.18   0.11   \n",
       "4264    -0.36    -0.11    -0.08  3.77   44.40   5.65   38.76   0.13  -0.09   \n",
       "4265    -0.14    -0.16     0.03  3.83   12.05  -4.58   30.67   0.24   0.08   \n",
       "4266    -0.16    -0.07     0.11  3.76   35.71 -86.06 -163.88   0.03   0.10   \n",
       "4267     0.38     0.06     0.10  3.74   47.00   2.75    3.97  -0.22  -0.42   \n",
       "4268    -0.21    -0.09    -0.14  3.89   -5.95   3.66   -4.73   0.09   0.01   \n",
       "4269     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "4270    -0.05    -0.14    -0.10  0.00  -22.58 -64.24   83.92   0.17   0.14   \n",
       "4271    -0.50    -0.35    -0.53  3.82  -65.46   5.49   14.65  -0.26   0.12   \n",
       "4272    -0.33    -0.26    -0.21  3.77   32.50  50.96  -88.20   0.09   0.09   \n",
       "4273    -0.10    -0.14     0.16  3.83  -27.77  30.52   -3.51   0.27   0.03   \n",
       "4274    -0.10    -0.18    -0.10  3.76   42.72 -35.25  -87.28   0.00   0.28   \n",
       "4275     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "4276    -0.20    -0.10    -0.24  3.89  -10.68  -0.31  -11.75   0.07   0.00   \n",
       "4277     0.51     0.00     0.27  3.74   41.81   5.04   32.81  -0.22  -0.44   \n",
       "4278     0.20    -0.08    -0.01  0.00  -41.81 -44.40   69.12   0.08   0.19   \n",
       "4279    -0.78    -0.36     0.07  3.82 -240.02 -14.19   66.38  -0.33   0.32   \n",
       "4280    -0.13     0.11    -0.19  3.76   58.90  -7.48  -72.33   0.19   0.15   \n",
       "\n",
       "      Gyroz  activity  \n",
       "0      0.81         7  \n",
       "1      0.77         7  \n",
       "2      0.98         7  \n",
       "3      1.20         7  \n",
       "4      0.35         7  \n",
       "5      0.84         7  \n",
       "6      0.06         7  \n",
       "7      0.82         7  \n",
       "8      0.77         7  \n",
       "9      0.68         7  \n",
       "10     0.98         7  \n",
       "11     1.05         7  \n",
       "12     0.60         7  \n",
       "13     0.94         7  \n",
       "14     0.21         7  \n",
       "15     0.95         7  \n",
       "16     0.99         7  \n",
       "17     0.79         7  \n",
       "18     0.87         7  \n",
       "19     1.02         7  \n",
       "20     0.38         7  \n",
       "21     0.94         7  \n",
       "22     0.16         7  \n",
       "23     1.01         7  \n",
       "24     0.88         7  \n",
       "25     0.81         7  \n",
       "26     0.81         7  \n",
       "27     0.93         7  \n",
       "28     0.21         7  \n",
       "29     0.90         7  \n",
       "...     ...       ...  \n",
       "4251   0.83         8  \n",
       "4252   1.34         8  \n",
       "4253   0.00         8  \n",
       "4254   1.39         8  \n",
       "4255   1.07         8  \n",
       "4256   1.16         8  \n",
       "4257   0.76         8  \n",
       "4258   1.13         8  \n",
       "4259   0.84         8  \n",
       "4260   1.11         8  \n",
       "4261   0.00         8  \n",
       "4262   1.00         8  \n",
       "4263   0.67         8  \n",
       "4264   0.99         8  \n",
       "4265   1.03         8  \n",
       "4266   1.19         8  \n",
       "4267   1.08         8  \n",
       "4268   0.88         8  \n",
       "4269   0.00         8  \n",
       "4270   0.87         8  \n",
       "4271   0.74         8  \n",
       "4272   0.87         8  \n",
       "4273   1.12         8  \n",
       "4274   0.83         8  \n",
       "4275   0.00         8  \n",
       "4276   0.78         8  \n",
       "4277   1.28         8  \n",
       "4278   0.98         8  \n",
       "4279   1.36         8  \n",
       "4280   0.79         8  \n",
       "\n",
       "[986091 rows x 20 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the correct dataset should have 986091 rows and 20 columns\n",
    "data_df = pd.concat(frame)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = alex_data_df\n",
    "# result_df\n",
    "result_df = data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the sensor label index range from 0 to 7 \n",
    "# sensor_data = []\n",
    "# for i in range(0, 8):\n",
    "#     df = alex_data_df.where(alex_data_df['Sensor'] == i).dropna()\n",
    "#     sensor_data.append(df)\n",
    "# result_df = pd.concat(sensor_data).reset_index(drop=True)\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data processing and deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "window_size: The number of timesteps in one window (e.g. how many rows in one window).\n",
    "\n",
    "channel: The number of features in one window. Similar to image channels (RGB).\n",
    "\n",
    "batch_size: The numebr of windows in one batch.\n",
    "\n",
    "learning_rate: How fast the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60\n",
    "channel = 1\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process original dataset, create windows (window_size samples(rows), about 1 second)\n",
    "data = []\n",
    "window = 1\n",
    "while window*window_size < len(result_df):\n",
    "    data_window = result_df[(window - 1)*window_size:window*window_size]\n",
    "    data.append(data_window.values)\n",
    "    window += 1\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16434"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n"
     ]
    }
   ],
   "source": [
    "# delete window if multiple activities and sensors presents\n",
    "cleaned_data = []\n",
    "for i in data:\n",
    "    previous_activity = -1\n",
    "    previous_sensor = -1\n",
    "    for j in i:\n",
    "        current_activity = j[19]\n",
    "        current_sensor = j[1]\n",
    "        if (previous_activity != -1) and (current_activity != previous_activity):\n",
    "            print(\"data contains different activities! Window droped\")\n",
    "            break\n",
    "#         elif (previous_sensor != -1) and (current_sensor != previous_sensor):\n",
    "#             print(\"data contains different sensors! Window droped\")\n",
    "#             break\n",
    "        else:\n",
    "            previous_activity = current_activity\n",
    "            previous_sensor = current_sensor\n",
    "    else:\n",
    "        cleaned_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16326"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 20)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "seed(101)\n",
    "shuffle(cleaned_data)\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label from each window\n",
    "labels = []\n",
    "for i in cleaned_data:\n",
    "    label = i[0][19]\n",
    "    labels.append(label)\n",
    "labels = np.array(labels)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each window\n",
    "features = []\n",
    "for i in cleaned_data:\n",
    "    new = np.delete(i, 19, 1)\n",
    "    features.append(new)\n",
    "features = np.array(features)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 19)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[2.108, 5.0, 0.45, 0.76, 0.42, 0.21, 70.31, 4...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[41.887, 4.0, 0.42, -0.45, 0.79, -0.08, 266.1...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[19.691, 0.0, 0.387, 0.02, 0.05, 0.92, 148.52...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[49.842, 4.0, -0.499, 0.58, 0.42, -0.49, 93.7...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[28.145, 0.0, 0.037000000000000005, 0.19, -0....</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[0.442, 3.0, -0.768, 0.02, -0.38, -0.51, 91.0...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[16.147000000000002, 0.0, 0.326, 0.09, -0.03,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[50.397, 7.0, -0.955, 0.01, -0.28, 0.11, 359....</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[47.141999999999996, 1.0, 0.02, 0.05, 0.02, 1...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[10.713, 2.0, -0.474, 0.18, -0.82, 0.25, 241....</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[[1.709, 5.0, 0.157, 0.85, 0.42, 0.27, 71.7, -...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[2.109, 6.0, -0.977, -0.14, 0.04, 0.15, 356.0...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[[0.828, 5.0, 0.58, 0.58, 0.35, 0.46, 84.03, -...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[[29.484, 7.0, 0.201, 0.27, -0.15, -0.93, 224....</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[[44.903999999999996, 2.0, -0.083, 0.05, 0.04,...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[[31.274, 4.0, 0.42100000000000004, -0.45, 0.7...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[[6.362, 3.0, -0.94, 0.15, -0.27, -0.14, 27.15...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[[47.876999999999995, 3.0, 0.96, 0.11, -0.06, ...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[[43.851000000000006, 7.0, 0.54, -0.09, 0.32, ...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[[30.79, 2.0, 0.035, -0.07, 0.28, 0.96, 192.58...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[3.15, 0.0, 0.04, -0.43, 0.06, 0.9, 191.86, 5...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[0.313, 0.0, -0.10400000000000001, 0.29, -0.0...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[[1.242, 4.0, 0.306, -0.77, 0.54, -0.14, 306.4...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[0.7809999999999999, 3.0, 0.7140000000000001,...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[[21.974, 1.0, 0.703, 0.11, -0.1, 0.69, 103.28...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[[21.425, 5.0, 0.207, 0.24, 0.12, -0.94, 216.6...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[[25.476, 2.0, -0.215, -0.25, 0.21, 0.92, 226....</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[[1.89, 5.0, 0.882, 0.22, 0.22, 0.36, 62.26, 1...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[[25.113000000000003, 7.0, -0.586, -0.22, -0.7...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[[0.715, 0.0, 0.732, 0.01, -0.0, -0.68, 288.19...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16296</th>\n",
       "      <td>[[0.09300000000000001, 6.0, -0.932, 0.07, -0.3...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16297</th>\n",
       "      <td>[[2.108, 2.0, -0.6940000000000001, -0.01, -0.2...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16298</th>\n",
       "      <td>[[2.8121, 6.0, 0.14300000000000002, -0.15, 0.0...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16299</th>\n",
       "      <td>[[1.695, 7.0, -0.927, 0.02, -0.29, 0.23, 342.1...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16300</th>\n",
       "      <td>[[28.572, 7.0, 0.637, -0.16, 0.75, -0.11, 263....</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16301</th>\n",
       "      <td>[[3.3372, 4.0, -0.863, 0.11, -0.4, 0.29, 325.0...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16302</th>\n",
       "      <td>[[2.35, 1.0, 0.46, -0.15, -0.18, -0.86, 248.23...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16303</th>\n",
       "      <td>[[48.093, 3.0, 0.531, 0.25, 0.28, 0.76, 122.02...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16304</th>\n",
       "      <td>[[0.611, 2.0, 0.65, 0.4, -0.02, -0.65, 294.63,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16305</th>\n",
       "      <td>[[1.8229, 1.0, 0.08800000000000001, 0.19, 0.01...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16306</th>\n",
       "      <td>[[1.4980000000000002, 4.0, 0.981, -0.03, 0.08,...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16307</th>\n",
       "      <td>[[4.9146, 5.0, 0.971, -0.06, 0.2, 0.12, 27.3, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16308</th>\n",
       "      <td>[[1.41, 0.0, 0.013000000000000001, -0.26, 0.01...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16309</th>\n",
       "      <td>[[3.9571, 3.0, 0.836, 0.24, 0.05, -0.49, 316.6...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16310</th>\n",
       "      <td>[[1.788, 6.0, 0.321, 0.18, 0.04, -0.93, 232.73...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16311</th>\n",
       "      <td>[[55.604, 6.0, -0.893, -0.09, 0.06, 0.44, 321....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16312</th>\n",
       "      <td>[[26.169, 0.0, 0.37200000000000005, 0.08, -0.0...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16313</th>\n",
       "      <td>[[0.48100000000000004, 3.0, -0.741, -0.17, -0....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16314</th>\n",
       "      <td>[[3.83, 4.0, -0.946, 0.06, -0.08, 0.31, 337.6,...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16315</th>\n",
       "      <td>[[0.201, 1.0, -0.473, -0.01, 0.04, 0.88, 250.5...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16316</th>\n",
       "      <td>[[3.1839999999999997, 2.0, 0.815, 0.07, 0.11, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16317</th>\n",
       "      <td>[[1.801, 6.0, -0.995, 0.03, 0.01, 0.09, 3.32, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16318</th>\n",
       "      <td>[[5.396, 5.0, -0.132, 0.74, 0.61, -0.23, 95.85...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16319</th>\n",
       "      <td>[[0.893, 0.0, -0.063, -0.08, -0.07, 0.99, 200....</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16320</th>\n",
       "      <td>[[2.43, 1.0, -0.084, -0.17, 0.03, -0.98, 184.6...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16321</th>\n",
       "      <td>[[27.276, 4.0, -0.9159999999999999, 0.1, -0.39...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16322</th>\n",
       "      <td>[[2.218, 7.0, -0.9590000000000001, -0.01, -0.0...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16323</th>\n",
       "      <td>[[1.3969999999999998, 5.0, -0.591, -0.21, -0.2...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16324</th>\n",
       "      <td>[[24.39, 2.0, -0.49, 0.02, -0.87, 0.08, 205.8,...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16325</th>\n",
       "      <td>[[3.0974, 0.0, 0.67, 0.26, -0.21, 0.66, 101.51...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16326 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features  labels\n",
       "0      [[2.108, 5.0, 0.45, 0.76, 0.42, 0.21, 70.31, 4...     4.0\n",
       "1      [[41.887, 4.0, 0.42, -0.45, 0.79, -0.08, 266.1...    10.0\n",
       "2      [[19.691, 0.0, 0.387, 0.02, 0.05, 0.92, 148.52...    12.0\n",
       "3      [[49.842, 4.0, -0.499, 0.58, 0.42, -0.49, 93.7...     7.0\n",
       "4      [[28.145, 0.0, 0.037000000000000005, 0.19, -0....    10.0\n",
       "5      [[0.442, 3.0, -0.768, 0.02, -0.38, -0.51, 91.0...    13.0\n",
       "6      [[16.147000000000002, 0.0, 0.326, 0.09, -0.03,...     1.0\n",
       "7      [[50.397, 7.0, -0.955, 0.01, -0.28, 0.11, 359....     7.0\n",
       "8      [[47.141999999999996, 1.0, 0.02, 0.05, 0.02, 1...    10.0\n",
       "9      [[10.713, 2.0, -0.474, 0.18, -0.82, 0.25, 241....    10.0\n",
       "10     [[1.709, 5.0, 0.157, 0.85, 0.42, 0.27, 71.7, -...     4.0\n",
       "11     [[2.109, 6.0, -0.977, -0.14, 0.04, 0.15, 356.0...     4.0\n",
       "12     [[0.828, 5.0, 0.58, 0.58, 0.35, 0.46, 84.03, -...     4.0\n",
       "13     [[29.484, 7.0, 0.201, 0.27, -0.15, -0.93, 224....     9.0\n",
       "14     [[44.903999999999996, 2.0, -0.083, 0.05, 0.04,...     6.0\n",
       "15     [[31.274, 4.0, 0.42100000000000004, -0.45, 0.7...    10.0\n",
       "16     [[6.362, 3.0, -0.94, 0.15, -0.27, -0.14, 27.15...     9.0\n",
       "17     [[47.876999999999995, 3.0, 0.96, 0.11, -0.06, ...     7.0\n",
       "18     [[43.851000000000006, 7.0, 0.54, -0.09, 0.32, ...     9.0\n",
       "19     [[30.79, 2.0, 0.035, -0.07, 0.28, 0.96, 192.58...     0.0\n",
       "20     [[3.15, 0.0, 0.04, -0.43, 0.06, 0.9, 191.86, 5...     4.0\n",
       "21     [[0.313, 0.0, -0.10400000000000001, 0.29, -0.0...     4.0\n",
       "22     [[1.242, 4.0, 0.306, -0.77, 0.54, -0.14, 306.4...     4.0\n",
       "23     [[0.7809999999999999, 3.0, 0.7140000000000001,...    13.0\n",
       "24     [[21.974, 1.0, 0.703, 0.11, -0.1, 0.69, 103.28...     7.0\n",
       "25     [[21.425, 5.0, 0.207, 0.24, 0.12, -0.94, 216.6...     2.0\n",
       "26     [[25.476, 2.0, -0.215, -0.25, 0.21, 0.92, 226....     9.0\n",
       "27     [[1.89, 5.0, 0.882, 0.22, 0.22, 0.36, 62.26, 1...     4.0\n",
       "28     [[25.113000000000003, 7.0, -0.586, -0.22, -0.7...    10.0\n",
       "29     [[0.715, 0.0, 0.732, 0.01, -0.0, -0.68, 288.19...     4.0\n",
       "...                                                  ...     ...\n",
       "16296  [[0.09300000000000001, 6.0, -0.932, 0.07, -0.3...     4.0\n",
       "16297  [[2.108, 2.0, -0.6940000000000001, -0.01, -0.2...     4.0\n",
       "16298  [[2.8121, 6.0, 0.14300000000000002, -0.15, 0.0...     7.0\n",
       "16299  [[1.695, 7.0, -0.927, 0.02, -0.29, 0.23, 342.1...     4.0\n",
       "16300  [[28.572, 7.0, 0.637, -0.16, 0.75, -0.11, 263....    10.0\n",
       "16301  [[3.3372, 4.0, -0.863, 0.11, -0.4, 0.29, 325.0...    13.0\n",
       "16302  [[2.35, 1.0, 0.46, -0.15, -0.18, -0.86, 248.23...     4.0\n",
       "16303  [[48.093, 3.0, 0.531, 0.25, 0.28, 0.76, 122.02...    10.0\n",
       "16304  [[0.611, 2.0, 0.65, 0.4, -0.02, -0.65, 294.63,...     4.0\n",
       "16305  [[1.8229, 1.0, 0.08800000000000001, 0.19, 0.01...     4.0\n",
       "16306  [[1.4980000000000002, 4.0, 0.981, -0.03, 0.08,...    13.0\n",
       "16307  [[4.9146, 5.0, 0.971, -0.06, 0.2, 0.12, 27.3, ...     0.0\n",
       "16308  [[1.41, 0.0, 0.013000000000000001, -0.26, 0.01...     4.0\n",
       "16309  [[3.9571, 3.0, 0.836, 0.24, 0.05, -0.49, 316.6...     7.0\n",
       "16310  [[1.788, 6.0, 0.321, 0.18, 0.04, -0.93, 232.73...    13.0\n",
       "16311  [[55.604, 6.0, -0.893, -0.09, 0.06, 0.44, 321....     1.0\n",
       "16312  [[26.169, 0.0, 0.37200000000000005, 0.08, -0.0...     7.0\n",
       "16313  [[0.48100000000000004, 3.0, -0.741, -0.17, -0....     4.0\n",
       "16314  [[3.83, 4.0, -0.946, 0.06, -0.08, 0.31, 337.6,...    13.0\n",
       "16315  [[0.201, 1.0, -0.473, -0.01, 0.04, 0.88, 250.5...     4.0\n",
       "16316  [[3.1839999999999997, 2.0, 0.815, 0.07, 0.11, ...     4.0\n",
       "16317  [[1.801, 6.0, -0.995, 0.03, 0.01, 0.09, 3.32, ...     4.0\n",
       "16318  [[5.396, 5.0, -0.132, 0.74, 0.61, -0.23, 95.85...     1.0\n",
       "16319  [[0.893, 0.0, -0.063, -0.08, -0.07, 0.99, 200....    13.0\n",
       "16320  [[2.43, 1.0, -0.084, -0.17, 0.03, -0.98, 184.6...     4.0\n",
       "16321  [[27.276, 4.0, -0.9159999999999999, 0.1, -0.39...     9.0\n",
       "16322  [[2.218, 7.0, -0.9590000000000001, -0.01, -0.0...     4.0\n",
       "16323  [[1.3969999999999998, 5.0, -0.591, -0.21, -0.2...     4.0\n",
       "16324  [[24.39, 2.0, -0.49, 0.02, -0.87, 0.08, 205.8,...    10.0\n",
       "16325  [[3.0974, 0.0, 0.67, 0.26, -0.21, 0.66, 101.51...    10.0\n",
       "\n",
       "[16326 rows x 2 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the features and labels\n",
    "k = list(zip(features, labels))\n",
    "activity_data = pd.DataFrame(k)\n",
    "activity_data.columns = ['features', 'labels']\n",
    "activity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of activity. The final output of neural net \n",
    "# has to have max_index + 1 output\n",
    "max_index = activity_data['labels'].max()\n",
    "label_size = int(max_index + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Training on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"CUDA is available! Training on GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to test, validation, and train\n",
    "valid_size = 0.2\n",
    "test_size = 0.2\n",
    "activity_data.columns = [\"features\", \"labels\"]\n",
    "activity_data_train = activity_data[:int(len(activity_data)*(1-valid_size-test_size))]\n",
    "activity_data_valid = activity_data[int(len(activity_data)*(1-valid_size-test_size)):int(len(activity_data)*(1-test_size))]\n",
    "activity_data_test = activity_data[int(len(activity_data)*(1-test_size)):]\n",
    "# activity_data_train.to_csv(\"./activity_data_train.csv\", encoding='utf-8-sig')\n",
    "# activity_data_valid.to_csv(\"./activity_data_valid.csv\", encoding='utf-8-sig')\n",
    "# activity_data_train.to_csv(\"./activity_data_test.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset in pytorch\n",
    "class DatasetSpineTrack(Dataset):\n",
    "    \n",
    "    def __init__(self, file, transform=None):\n",
    "        #self.data = pd.read_csv(file_path)\n",
    "        self.data = file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        features = torch.tensor(self.data[\"features\"].iloc[index])\n",
    "        features = features.view(channel, window_size, 19) \n",
    "        labels = torch.tensor(self.data[\"labels\"].iloc[index], dtype=torch.long)\n",
    "        #print(labels.type())\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training and testing dataset in csv\n",
    "# train_dataset = DatasetSpineTrack(\"./activity_data_train.csv\")\n",
    "# valid_dataset = DatasetSpineTrack(\"./activity_data_valid.csv\")\n",
    "# test_dataset = DatasetSpineTrack(\"./activity_data_test.csv\")\n",
    "train_dataset = DatasetSpineTrack(activity_data_train)\n",
    "valid_dataset = DatasetSpineTrack(activity_data_valid)\n",
    "test_dataset = DatasetSpineTrack(activity_data_test)\n",
    "feature, label = train_dataset.__getitem__(0)\n",
    "#feature\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Network Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=15, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "# window_size channels\n",
    "# model.conv1 = torch.nn.Conv2d(window_size, batch_size, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(512, label_size, bias=True)\n",
    "model.add_module(\"dropout\", torch.nn.Dropout(p=0.5))\n",
    "model = model.double()\n",
    "\n",
    "# move tensors to GPU is CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Network\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.970308 \tValidation Loss: 1.757685\n",
      "Validation loss decreased (inf --> 1.757685).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.553499 \tValidation Loss: 0.659563\n",
      "Validation loss decreased (1.757685 --> 0.659563).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.443594 \tValidation Loss: 0.404271\n",
      "Validation loss decreased (0.659563 --> 0.404271).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.368439 \tValidation Loss: 0.366239\n",
      "Validation loss decreased (0.404271 --> 0.366239).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.287947 \tValidation Loss: 0.456951\n",
      "Epoch: 6 \tTraining Loss: 0.271860 \tValidation Loss: 0.826823\n",
      "Epoch: 7 \tTraining Loss: 0.243204 \tValidation Loss: 0.289843\n",
      "Validation loss decreased (0.366239 --> 0.289843).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.249601 \tValidation Loss: 0.334047\n",
      "Epoch: 9 \tTraining Loss: 0.175652 \tValidation Loss: 0.375451\n",
      "Epoch: 10 \tTraining Loss: 0.150563 \tValidation Loss: 1.388640\n",
      "Epoch: 11 \tTraining Loss: 0.146594 \tValidation Loss: 0.237634\n",
      "Validation loss decreased (0.289843 --> 0.237634).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.175684 \tValidation Loss: 0.228399\n",
      "Validation loss decreased (0.237634 --> 0.228399).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.153346 \tValidation Loss: 0.191799\n",
      "Validation loss decreased (0.228399 --> 0.191799).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.114815 \tValidation Loss: 0.205969\n",
      "Epoch: 15 \tTraining Loss: 0.090886 \tValidation Loss: 0.198761\n",
      "Epoch: 16 \tTraining Loss: 0.086383 \tValidation Loss: 0.268086\n",
      "Epoch: 17 \tTraining Loss: 0.098132 \tValidation Loss: 0.185623\n",
      "Validation loss decreased (0.191799 --> 0.185623).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.088986 \tValidation Loss: 0.891570\n",
      "Epoch: 19 \tTraining Loss: 0.090048 \tValidation Loss: 0.154859\n",
      "Validation loss decreased (0.185623 --> 0.154859).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.071028 \tValidation Loss: 0.476356\n",
      "Epoch: 21 \tTraining Loss: 0.123845 \tValidation Loss: 0.246384\n",
      "Epoch: 22 \tTraining Loss: 0.110677 \tValidation Loss: 0.209052\n",
      "Epoch: 23 \tTraining Loss: 0.073214 \tValidation Loss: 0.150680\n",
      "Validation loss decreased (0.154859 --> 0.150680).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.051092 \tValidation Loss: 0.175245\n",
      "Epoch: 25 \tTraining Loss: 0.066599 \tValidation Loss: 0.170929\n",
      "Epoch: 26 \tTraining Loss: 0.043694 \tValidation Loss: 0.154963\n",
      "Epoch: 27 \tTraining Loss: 0.050641 \tValidation Loss: 0.245754\n",
      "Epoch: 28 \tTraining Loss: 0.106326 \tValidation Loss: 0.535066\n",
      "Epoch: 29 \tTraining Loss: 0.081716 \tValidation Loss: 0.343659\n",
      "Epoch: 30 \tTraining Loss: 0.037214 \tValidation Loss: 0.501373\n",
      "Training time: 53 min 10 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for features, labels in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*features.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for features, labels in validloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*features.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_Spinetrack_data.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "# output running time\n",
    "running_time = time.time() - start_time\n",
    "sec = running_time % 60\n",
    "miniute = running_time / 60\n",
    "print(\"Training time: {} min {} sec\".format(int(miniute), int(sec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_Spinetrack_data.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Test your trained model on previously unseen data! A \"good\" result will be a result that gets more than 70% accuracy on these test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release all the GPU memory cache that can be freed\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy and each class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.139150\n",
      "\n",
      "Test Accuracy of Pulling_OneH: 93% (194/207)\n",
      "Test Accuracy of Overhead: 100% (194/194)\n",
      "Test Accuracy of Pulling: 85% (80/94)\n",
      "Test Accuracy of Sitting: 100% (97/97)\n",
      "Test Accuracy of Lifting: 96% (1028/1064)\n",
      "Test Accuracy of Crawling: 98% (78/79)\n",
      "Test Accuracy of Standing: 100% (103/103)\n",
      "Test Accuracy of Carrying: 100% (271/271)\n",
      "Test Accuracy of Walking: 95% (101/106)\n",
      "Test Accuracy of Pushing: 94% (170/180)\n",
      "Test Accuracy of Reaching: 100% (279/279)\n",
      "Test Accuracy of Static_Stoop: 100% (84/84)\n",
      "Test Accuracy of Kneeling: 98% (84/85)\n",
      "Test Accuracy of Lifting_OneH: 83% (273/327)\n",
      "Test Accuracy of Crouching: 88% (85/96)\n",
      "\n",
      "Test Accuracy (Overall): 95% (3121/3266)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(len(index_dict)))\n",
    "class_total = list(0. for i in range(len(index_dict)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "model.eval()\n",
    "torch.no_grad()\n",
    "# iterate over test data\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(features)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, labels)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*features.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(testloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(index_dict)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            list(index_dict.keys())[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (list(index_dict.keys())[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy (different calculation method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.92 GiB already allocated; 15.19 MiB free; 13.71 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-8f1997b81984>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain_on_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 338\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.92 GiB already allocated; 15.19 MiB free; 13.71 MiB cached)"
     ]
    }
   ],
   "source": [
    "# Turn off gradients for validation, saves memory and computations\n",
    "torch.no_grad()\n",
    "accuracy = 0\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    loss = model(features)\n",
    "    test_loss += criterion(loss, labels)\n",
    "\n",
    "#     ps = torch.exp(loss)\n",
    "    top_p, top_class = loss.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out pulling_OneH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
