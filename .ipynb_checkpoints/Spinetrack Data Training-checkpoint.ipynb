{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and reformating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\\\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from random import shuffle, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "window_size: The number of timesteps in one window (e.g. how many rows in one window).\n",
    "\n",
    "channel: The number of features in one window. Similar to image channels (RGB).\n",
    "\n",
    "batch_size: The numebr of windows in one batch.\n",
    "\n",
    "learning_rate: How fast the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 45\n",
    "channel = 1\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Go to \n",
    "<a href=#bookmark> Run all cell above</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {'Pulling_OneH': 0, 'Overhead': 1, 'Pulling': 2, 'Sitting': 3, 'Lifting': 4, \n",
    "              'Crawling': 5, 'Standing': 6, 'Carrying': 7, 'Walking': 8, 'Pushing': 9, \n",
    "              'Reaching': 10, 'Static_Stoop': 11, 'Kneeling': 12, 'Lifting_OneH': 13, 'Crouching': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pulling_OneH': 0, 'Overhead': 1, 'Pulling': 2, 'Sitting': 3, 'Lifting': 4, 'Crawling': 5, 'Standing': 6, 'Carrying': 7, 'Walking': 8, 'Pushing': 9, 'Reaching': 10, 'Static_Stoop': 11, 'Kneeling': 12, 'Lifting_OneH': 13, 'Crouching': 14}\n"
     ]
    }
   ],
   "source": [
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeSec</th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Quatx</th>\n",
       "      <th>Quaty</th>\n",
       "      <th>Quatz</th>\n",
       "      <th>Quat0</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>Roll</th>\n",
       "      <th>LinAccx</th>\n",
       "      <th>LinAccy</th>\n",
       "      <th>LinAccz</th>\n",
       "      <th>Vbat</th>\n",
       "      <th>Accx</th>\n",
       "      <th>Accy</th>\n",
       "      <th>Accz</th>\n",
       "      <th>Gyrox</th>\n",
       "      <th>Gyroy</th>\n",
       "      <th>Gyroz</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0362</td>\n",
       "      <td>2</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>306.93</td>\n",
       "      <td>25.57</td>\n",
       "      <td>-16.37</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>16.33</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0377</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.71</td>\n",
       "      <td>113.60</td>\n",
       "      <td>38.21</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-37.84</td>\n",
       "      <td>-4.27</td>\n",
       "      <td>53.71</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.77</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0961</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>192.97</td>\n",
       "      <td>-9.34</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>36.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0978</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>38.13</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.81</td>\n",
       "      <td>-18.92</td>\n",
       "      <td>-64.70</td>\n",
       "      <td>118.26</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.61</td>\n",
       "      <td>116.18</td>\n",
       "      <td>-5.60</td>\n",
       "      <td>82.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3.87</td>\n",
       "      <td>5.19</td>\n",
       "      <td>45.62</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.35</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>206.37</td>\n",
       "      <td>-9.81</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>18.31</td>\n",
       "      <td>61.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.84</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1069</td>\n",
       "      <td>4</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>300.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-90.39</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>3.79</td>\n",
       "      <td>8.85</td>\n",
       "      <td>-50.35</td>\n",
       "      <td>-32.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1086</td>\n",
       "      <td>7</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>42.78</td>\n",
       "      <td>15.05</td>\n",
       "      <td>16.65</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-14.34</td>\n",
       "      <td>-24.11</td>\n",
       "      <td>85.60</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1283</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>310.55</td>\n",
       "      <td>24.85</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-31.13</td>\n",
       "      <td>-19.23</td>\n",
       "      <td>33.87</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.77</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1301</td>\n",
       "      <td>3</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.74</td>\n",
       "      <td>119.48</td>\n",
       "      <td>37.14</td>\n",
       "      <td>5.69</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-56.00</td>\n",
       "      <td>1.37</td>\n",
       "      <td>29.91</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.68</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1662</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>199.05</td>\n",
       "      <td>-10.01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.92</td>\n",
       "      <td>10.53</td>\n",
       "      <td>-7.32</td>\n",
       "      <td>64.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1678</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>43.15</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.81</td>\n",
       "      <td>-13.58</td>\n",
       "      <td>-64.85</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1.05</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1982</td>\n",
       "      <td>5</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.64</td>\n",
       "      <td>121.61</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>81.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>7.48</td>\n",
       "      <td>67.75</td>\n",
       "      <td>37.99</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>211.95</td>\n",
       "      <td>-9.58</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>26.40</td>\n",
       "      <td>3.81</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2036</td>\n",
       "      <td>4</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>306.61</td>\n",
       "      <td>-7.64</td>\n",
       "      <td>-90.53</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.79</td>\n",
       "      <td>16.17</td>\n",
       "      <td>-65.77</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2050</td>\n",
       "      <td>7</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>50.00</td>\n",
       "      <td>12.38</td>\n",
       "      <td>15.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-39.06</td>\n",
       "      <td>18.62</td>\n",
       "      <td>69.12</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.2443</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.99</td>\n",
       "      <td>205.54</td>\n",
       "      <td>-10.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>14.04</td>\n",
       "      <td>7.93</td>\n",
       "      <td>55.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.2459</td>\n",
       "      <td>3</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>126.80</td>\n",
       "      <td>36.64</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-59.51</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>93.69</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.79</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2642</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>316.15</td>\n",
       "      <td>24.21</td>\n",
       "      <td>-17.51</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-36.01</td>\n",
       "      <td>-13.12</td>\n",
       "      <td>54.47</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.87</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.2659</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>47.90</td>\n",
       "      <td>-6.24</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.81</td>\n",
       "      <td>21.36</td>\n",
       "      <td>-20.60</td>\n",
       "      <td>82.86</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.02</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.3011</td>\n",
       "      <td>5</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.68</td>\n",
       "      <td>128.82</td>\n",
       "      <td>-11.79</td>\n",
       "      <td>80.05</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>17.85</td>\n",
       "      <td>71.72</td>\n",
       "      <td>24.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.38</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.3028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>220.57</td>\n",
       "      <td>-8.91</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.91</td>\n",
       "      <td>19.84</td>\n",
       "      <td>5.95</td>\n",
       "      <td>92.77</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.3065</td>\n",
       "      <td>4</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>315.36</td>\n",
       "      <td>-6.74</td>\n",
       "      <td>-90.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.02</td>\n",
       "      <td>-74.31</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.3080</td>\n",
       "      <td>7</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>60.57</td>\n",
       "      <td>12.96</td>\n",
       "      <td>14.21</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-22.28</td>\n",
       "      <td>32.50</td>\n",
       "      <td>104.06</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.01</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.3251</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.98</td>\n",
       "      <td>212.51</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.92</td>\n",
       "      <td>13.12</td>\n",
       "      <td>9.77</td>\n",
       "      <td>85.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.3266</td>\n",
       "      <td>3</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.83</td>\n",
       "      <td>137.50</td>\n",
       "      <td>36.95</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-35.86</td>\n",
       "      <td>18.31</td>\n",
       "      <td>78.58</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.3993</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>324.52</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-16.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-21.97</td>\n",
       "      <td>-21.51</td>\n",
       "      <td>90.33</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.4009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>229.17</td>\n",
       "      <td>-10.88</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>19.99</td>\n",
       "      <td>-8.39</td>\n",
       "      <td>61.65</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.4048</td>\n",
       "      <td>5</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.70</td>\n",
       "      <td>136.72</td>\n",
       "      <td>-13.27</td>\n",
       "      <td>79.93</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.87</td>\n",
       "      <td>17.70</td>\n",
       "      <td>78.58</td>\n",
       "      <td>24.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.4065</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>52.68</td>\n",
       "      <td>-9.29</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.81</td>\n",
       "      <td>12.66</td>\n",
       "      <td>-25.63</td>\n",
       "      <td>51.27</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986061</th>\n",
       "      <td>54.2570</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>133.41</td>\n",
       "      <td>3.77</td>\n",
       "      <td>-6.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.74</td>\n",
       "      <td>35.71</td>\n",
       "      <td>-16.94</td>\n",
       "      <td>-22.74</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986062</th>\n",
       "      <td>54.2600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>14.18</td>\n",
       "      <td>-19.14</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.88</td>\n",
       "      <td>-12.97</td>\n",
       "      <td>-44.71</td>\n",
       "      <td>21.82</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986063</th>\n",
       "      <td>54.2620</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986064</th>\n",
       "      <td>54.3000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>201.75</td>\n",
       "      <td>7.13</td>\n",
       "      <td>22.33</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.44</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>-109.86</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986065</th>\n",
       "      <td>54.3010</td>\n",
       "      <td>5</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>262.61</td>\n",
       "      <td>12.97</td>\n",
       "      <td>47.20</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>233.15</td>\n",
       "      <td>-25.94</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.07</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986066</th>\n",
       "      <td>54.3500</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>28.27</td>\n",
       "      <td>-27.88</td>\n",
       "      <td>-22.85</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.77</td>\n",
       "      <td>24.41</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-55.54</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986067</th>\n",
       "      <td>54.3510</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>29.15</td>\n",
       "      <td>-18.64</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>3.83</td>\n",
       "      <td>22.13</td>\n",
       "      <td>17.24</td>\n",
       "      <td>29.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.76</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986068</th>\n",
       "      <td>54.3550</td>\n",
       "      <td>3</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>349.90</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>-84.53</td>\n",
       "      <td>-65.00</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986069</th>\n",
       "      <td>54.3570</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>126.18</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-5.53</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.58</td>\n",
       "      <td>5.04</td>\n",
       "      <td>-91.09</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986070</th>\n",
       "      <td>54.4190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16.87</td>\n",
       "      <td>-20.28</td>\n",
       "      <td>-7.19</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.88</td>\n",
       "      <td>9.46</td>\n",
       "      <td>22.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986071</th>\n",
       "      <td>54.4210</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986072</th>\n",
       "      <td>54.4250</td>\n",
       "      <td>7</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>201.15</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>21.50</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-28.69</td>\n",
       "      <td>-94.60</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986073</th>\n",
       "      <td>54.4270</td>\n",
       "      <td>5</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>257.84</td>\n",
       "      <td>17.62</td>\n",
       "      <td>63.30</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.82</td>\n",
       "      <td>106.35</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-69.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.67</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986074</th>\n",
       "      <td>54.4470</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>27.62</td>\n",
       "      <td>-27.18</td>\n",
       "      <td>-18.53</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.77</td>\n",
       "      <td>44.40</td>\n",
       "      <td>5.65</td>\n",
       "      <td>38.76</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986075</th>\n",
       "      <td>54.4480</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>32.37</td>\n",
       "      <td>-18.29</td>\n",
       "      <td>-6.35</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.83</td>\n",
       "      <td>12.05</td>\n",
       "      <td>-4.58</td>\n",
       "      <td>30.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.03</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986076</th>\n",
       "      <td>54.4520</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>339.36</td>\n",
       "      <td>-9.22</td>\n",
       "      <td>6.64</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.76</td>\n",
       "      <td>35.71</td>\n",
       "      <td>-86.06</td>\n",
       "      <td>-163.88</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.19</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986077</th>\n",
       "      <td>54.4530</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>120.08</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-3.86</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.74</td>\n",
       "      <td>47.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1.08</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986078</th>\n",
       "      <td>54.4570</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17.70</td>\n",
       "      <td>-19.11</td>\n",
       "      <td>-6.88</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-5.95</td>\n",
       "      <td>3.66</td>\n",
       "      <td>-4.73</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.88</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986079</th>\n",
       "      <td>54.4590</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986080</th>\n",
       "      <td>54.5070</td>\n",
       "      <td>7</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>202.84</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>17.79</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-22.58</td>\n",
       "      <td>-64.24</td>\n",
       "      <td>83.92</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986081</th>\n",
       "      <td>54.5080</td>\n",
       "      <td>5</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>257.17</td>\n",
       "      <td>20.78</td>\n",
       "      <td>65.46</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>3.82</td>\n",
       "      <td>-65.46</td>\n",
       "      <td>5.49</td>\n",
       "      <td>14.65</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986082</th>\n",
       "      <td>54.5260</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>25.42</td>\n",
       "      <td>-24.87</td>\n",
       "      <td>-13.42</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3.77</td>\n",
       "      <td>32.50</td>\n",
       "      <td>50.96</td>\n",
       "      <td>-88.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986083</th>\n",
       "      <td>54.5270</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>32.49</td>\n",
       "      <td>-16.52</td>\n",
       "      <td>-6.97</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-27.77</td>\n",
       "      <td>30.52</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986084</th>\n",
       "      <td>54.5590</td>\n",
       "      <td>3</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>324.36</td>\n",
       "      <td>-12.77</td>\n",
       "      <td>13.73</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>3.76</td>\n",
       "      <td>42.72</td>\n",
       "      <td>-35.25</td>\n",
       "      <td>-87.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986085</th>\n",
       "      <td>54.5610</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986086</th>\n",
       "      <td>54.5640</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17.20</td>\n",
       "      <td>-19.03</td>\n",
       "      <td>-7.60</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-10.68</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-11.75</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986087</th>\n",
       "      <td>54.5660</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>122.58</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.74</td>\n",
       "      <td>41.81</td>\n",
       "      <td>5.04</td>\n",
       "      <td>32.81</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986088</th>\n",
       "      <td>54.6100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>210.16</td>\n",
       "      <td>-17.23</td>\n",
       "      <td>12.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-41.81</td>\n",
       "      <td>-44.40</td>\n",
       "      <td>69.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.98</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986089</th>\n",
       "      <td>54.6110</td>\n",
       "      <td>5</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>259.25</td>\n",
       "      <td>16.95</td>\n",
       "      <td>52.48</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.82</td>\n",
       "      <td>-240.02</td>\n",
       "      <td>-14.19</td>\n",
       "      <td>66.38</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.36</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986090</th>\n",
       "      <td>54.6620</td>\n",
       "      <td>3</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>315.13</td>\n",
       "      <td>-11.78</td>\n",
       "      <td>21.63</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>3.76</td>\n",
       "      <td>58.90</td>\n",
       "      <td>-7.48</td>\n",
       "      <td>-72.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.79</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986091 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TimeSec  Sensor  Quatx  Quaty  Quatz  Quat0  Heading  Pitch   Roll  \\\n",
       "0        0.0362       2  0.822   0.01   0.26  -0.51   306.93  25.57 -16.37   \n",
       "1        0.0377       3  0.625  -0.21   0.26   0.71   113.60  38.21   7.42   \n",
       "2        0.0961       0  0.008   0.08   0.02   1.00   192.97  -9.34   2.16   \n",
       "3        0.0978       6 -0.978   0.03  -0.00  -0.21    38.13   1.13  -2.73   \n",
       "4        0.1018       5  0.448   0.44   0.49   0.61   116.18  -5.60  82.09   \n",
       "5        0.1032       1  0.105  -0.09   0.01  -0.99   206.37  -9.81  -2.63   \n",
       "6        0.1069       4  0.540  -0.59   0.39  -0.45   300.68  -6.56 -90.39   \n",
       "7        0.1086       7  0.955   0.11   0.16   0.23    42.78  15.05  16.65   \n",
       "8        0.1283       2  0.838  -0.01   0.26  -0.48   310.55  24.85 -16.97   \n",
       "9        0.1301       3  0.586  -0.22   0.23   0.74   119.48  37.14   5.69   \n",
       "10       0.1662       0 -0.045   0.09   0.02   1.00   199.05 -10.01   1.58   \n",
       "11       0.1678       6 -0.968   0.01   0.03  -0.25    43.15  -3.44  -1.57   \n",
       "12       0.1982       5  0.411   0.43   0.49   0.64   121.61  -7.94  81.38   \n",
       "13       0.1998       1  0.154  -0.09   0.00  -0.98   211.95  -9.58  -2.03   \n",
       "14       0.2036       4  0.558  -0.62   0.35  -0.43   306.61  -7.64 -90.53   \n",
       "15       0.2050       7  0.942   0.09   0.14   0.29    50.00  12.38  15.05   \n",
       "16       0.2443       0 -0.101   0.09   0.03   0.99   205.54 -10.56   1.96   \n",
       "17       0.2459       3  0.535  -0.24   0.20   0.78   126.80  36.64   4.11   \n",
       "18       0.2642       2  0.861  -0.03   0.25  -0.44   316.15  24.21 -17.51   \n",
       "19       0.2659       6 -0.955  -0.00   0.06  -0.29    47.90  -6.24  -1.72   \n",
       "20       0.3011       5  0.355   0.41   0.50   0.68   128.82 -11.79  80.05   \n",
       "21       0.3028       1  0.228  -0.08  -0.01  -0.97   220.57  -8.91  -1.17   \n",
       "22       0.3065       4  0.592  -0.64   0.31  -0.38   315.36  -6.74 -90.54   \n",
       "23       0.3080       7  0.911   0.07   0.15   0.38    60.57  12.96  14.21   \n",
       "24       0.3251       0 -0.162   0.09   0.04   0.98   212.51 -10.29   2.50   \n",
       "25       0.3266       3  0.461  -0.26   0.19   0.83   137.50  36.95   5.13   \n",
       "26       0.3993       2  0.891  -0.04   0.25  -0.38   324.52  24.14 -16.14   \n",
       "27       0.4009       1  0.301  -0.09  -0.03  -0.95   229.17 -10.88   0.46   \n",
       "28       0.4048       5  0.300   0.38   0.52   0.70   136.72 -13.27  79.93   \n",
       "29       0.4065       6 -0.941  -0.02   0.08  -0.33    52.68  -9.29  -1.28   \n",
       "...         ...     ...    ...    ...    ...    ...      ...    ...    ...   \n",
       "986061  54.2570       4 -0.502   0.06   0.03  -0.86   133.41   3.77  -6.55   \n",
       "986062  54.2600       0  0.985  -0.05  -0.17  -0.01    14.18 -19.14  -5.33   \n",
       "986063  54.2620       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986064  54.3000       7  0.054   0.07  -0.19  -0.98   201.75   7.13  22.33   \n",
       "986065  54.3010       5  0.475   0.31  -0.27  -0.78   262.61  12.97  47.20   \n",
       "986066  54.3500       6 -0.950   0.16   0.26  -0.07    28.27 -27.88 -22.85   \n",
       "986067  54.3510       1 -0.977   0.05   0.17  -0.12    29.15 -18.64  -8.00   \n",
       "986068  54.3550       3  0.977   0.04  -0.03  -0.21   349.90  -2.25   4.98   \n",
       "986069  54.3570       4 -0.557   0.05   0.03  -0.83   126.18   2.60  -5.53   \n",
       "986070  54.4190       0  0.982  -0.06  -0.18   0.01    16.87 -20.28  -7.19   \n",
       "986071  54.4210       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986072  54.4250       7  0.063   0.00  -0.19  -0.98   201.15  -1.15  21.50   \n",
       "986073  54.4270       5  0.376   0.38  -0.37  -0.76   257.84  17.62  63.30   \n",
       "986074  54.4470       6 -0.957   0.13   0.25  -0.08    27.62 -27.18 -18.53   \n",
       "986075  54.4480       1 -0.975   0.03   0.17  -0.15    32.37 -18.29  -6.35   \n",
       "986076  54.4520       3  0.951   0.03  -0.09  -0.29   339.36  -9.22   6.64   \n",
       "986077  54.4530       4 -0.600   0.04   0.01  -0.80   120.08   2.60  -3.86   \n",
       "986078  54.4570       0  0.984  -0.05  -0.17   0.02    17.70 -19.11  -6.88   \n",
       "986079  54.4590       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986080  54.5070       7  0.089  -0.07  -0.16  -0.98   202.84  -9.75  17.79   \n",
       "986081  54.5080       5  0.350   0.41  -0.37  -0.76   257.17  20.78  65.46   \n",
       "986082  54.5260       6 -0.968   0.09   0.22  -0.07    25.42 -24.87 -13.42   \n",
       "986083  54.5270       1 -0.976   0.04   0.15  -0.15    32.49 -16.52  -6.97   \n",
       "986084  54.5590       3  0.901   0.06  -0.15  -0.40   324.36 -12.77  13.73   \n",
       "986085  54.5610       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986086  54.5640       0  0.984  -0.06  -0.17   0.02    17.20 -19.03  -7.60   \n",
       "986087  54.5660       4 -0.584   0.01  -0.03  -0.81   122.58   2.78   1.97   \n",
       "986088  54.6100       7  0.153  -0.13  -0.12  -0.97   210.16 -17.23  12.17   \n",
       "986089  54.6110       5  0.423   0.35  -0.30  -0.78   259.25  16.95  52.48   \n",
       "986090  54.6620       3  0.860   0.11  -0.18  -0.46   315.13 -11.78  21.63   \n",
       "\n",
       "        LinAccx  LinAccy  LinAccz  Vbat    Accx   Accy    Accz  Gyrox  Gyroy  \\\n",
       "0         -0.03    -0.03    -0.02  3.94  -19.84 -11.14   16.33  -0.47  -0.28   \n",
       "1          0.06    -0.08    -0.08  3.80  -37.84  -4.27   53.71  -0.51   0.03   \n",
       "2          0.11    -0.13    -0.02  3.92    2.75  -1.68   36.01   0.07  -0.10   \n",
       "3          0.08     0.36     0.33  3.81  -18.92 -64.70  118.26  -0.13   0.07   \n",
       "4          0.26    -0.13    -0.21  3.87    5.19  45.62   -2.90   0.15   0.67   \n",
       "5         -0.01     0.04    -0.05  3.91   -3.51  18.31   61.04   0.20  -0.02   \n",
       "6          0.10     0.06    -0.26  3.79    8.85 -50.35  -32.35   0.15  -1.09   \n",
       "7          0.11     0.29    -0.22  0.00  -14.34 -24.11   85.60  -0.28  -0.09   \n",
       "8         -0.12     0.06    -0.06  3.94  -31.13 -19.23   33.87  -0.37  -0.39   \n",
       "9         -0.01     0.01    -0.16  3.80  -56.00   1.37   29.91  -0.50   0.10   \n",
       "10         0.04    -0.03    -0.01  3.92   10.53  -7.32   64.24   0.13   0.00   \n",
       "11         0.21     0.33    -0.01  3.81  -13.58 -64.85    7.32   0.03  -0.42   \n",
       "12         0.43    -0.03    -0.02  3.87    7.48  67.75   37.99   0.04   0.72   \n",
       "13        -0.01    -0.02    -0.05  3.91   26.40   3.81   61.49   0.17  -0.07   \n",
       "14         0.21    -0.00    -0.03  3.79   16.17 -65.77   -1.83   0.20  -1.04   \n",
       "15        -0.00     0.19    -0.02  0.00  -39.06  18.62   69.12  -0.31   0.10   \n",
       "16         0.04     0.05     0.02  3.92   14.04   7.93   55.54   0.15   0.09   \n",
       "17         0.00     0.00    -0.02  3.80  -59.51  -0.31   93.69  -0.60   0.08   \n",
       "18        -0.01     0.08    -0.03  3.94  -36.01 -13.12   54.47  -0.35  -0.35   \n",
       "19        -0.12     0.09     0.01  3.81   21.36 -20.60   82.86  -0.04  -0.03   \n",
       "20         0.29     0.05    -0.02  3.87   17.85  71.72   24.41   0.10   0.77   \n",
       "21        -0.11     0.17    -0.01  3.91   19.84   5.95   92.77   0.34   0.10   \n",
       "22         0.27    -0.05    -0.26  3.79    7.02 -74.31    2.44   0.35  -0.95   \n",
       "23        -0.01     0.16     0.09  0.00  -22.28  32.50  104.06  -0.38   0.20   \n",
       "24         0.04     0.03    -0.11  3.92   13.12   9.77   85.14   0.14   0.10   \n",
       "25         0.00     0.09     0.11  3.80  -35.86  18.31   78.58  -0.71   0.12   \n",
       "26         0.09     0.13    -0.13  3.94  -21.97 -21.51   90.33  -0.22  -0.24   \n",
       "27         0.15     0.10    -0.05  3.91   19.99  -8.39   61.65   0.12   0.17   \n",
       "28         0.15     0.02    -0.04  3.87   17.70  78.58   24.87   0.12   0.95   \n",
       "29         0.20    -0.05    -0.04  3.81   12.66 -25.63   51.27   0.34  -0.08   \n",
       "...         ...      ...      ...   ...     ...    ...     ...    ...    ...   \n",
       "986061     0.15     0.16    -0.13  3.74   35.71 -16.94  -22.74  -0.27  -0.15   \n",
       "986062    -0.45    -0.07     0.21  3.88  -12.97 -44.71   21.82  -0.08  -0.07   \n",
       "986063     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986064     0.72     0.15     0.66  0.00   49.44 -69.58 -109.86  -0.46   0.07   \n",
       "986065    -0.63    -0.20    -0.11  3.82  233.15 -25.94  -69.58  -0.16   0.19   \n",
       "986066    -0.30    -0.23     0.16  3.77   24.41   1.07  -55.54   0.33  -0.17   \n",
       "986067    -0.02    -0.23    -0.17  3.83   22.13  17.24   29.60   0.36   0.11   \n",
       "986068    -0.10    -0.11     0.14  3.76   -3.20 -84.53  -65.00  -0.08   0.14   \n",
       "986069     0.27     0.11    -0.12  3.74    4.58   5.04  -91.09  -0.25  -0.30   \n",
       "986070    -0.48    -0.12    -0.05  3.88    9.46  22.58   24.26  -0.15   0.04   \n",
       "986071     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986072    -0.44     0.31     0.09  0.00  -28.69 -94.60   11.44   0.36   0.34   \n",
       "986073    -0.47    -0.28    -0.56  3.82  106.35  -0.61  -69.12  -0.18   0.11   \n",
       "986074    -0.36    -0.11    -0.08  3.77   44.40   5.65   38.76   0.13  -0.09   \n",
       "986075    -0.14    -0.16     0.03  3.83   12.05  -4.58   30.67   0.24   0.08   \n",
       "986076    -0.16    -0.07     0.11  3.76   35.71 -86.06 -163.88   0.03   0.10   \n",
       "986077     0.38     0.06     0.10  3.74   47.00   2.75    3.97  -0.22  -0.42   \n",
       "986078    -0.21    -0.09    -0.14  3.89   -5.95   3.66   -4.73   0.09   0.01   \n",
       "986079     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986080    -0.05    -0.14    -0.10  0.00  -22.58 -64.24   83.92   0.17   0.14   \n",
       "986081    -0.50    -0.35    -0.53  3.82  -65.46   5.49   14.65  -0.26   0.12   \n",
       "986082    -0.33    -0.26    -0.21  3.77   32.50  50.96  -88.20   0.09   0.09   \n",
       "986083    -0.10    -0.14     0.16  3.83  -27.77  30.52   -3.51   0.27   0.03   \n",
       "986084    -0.10    -0.18    -0.10  3.76   42.72 -35.25  -87.28   0.00   0.28   \n",
       "986085     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986086    -0.20    -0.10    -0.24  3.89  -10.68  -0.31  -11.75   0.07   0.00   \n",
       "986087     0.51     0.00     0.27  3.74   41.81   5.04   32.81  -0.22  -0.44   \n",
       "986088     0.20    -0.08    -0.01  0.00  -41.81 -44.40   69.12   0.08   0.19   \n",
       "986089    -0.78    -0.36     0.07  3.82 -240.02 -14.19   66.38  -0.33   0.32   \n",
       "986090    -0.13     0.11    -0.19  3.76   58.90  -7.48  -72.33   0.19   0.15   \n",
       "\n",
       "        Gyroz  activity  \n",
       "0        0.81         7  \n",
       "1        0.77         7  \n",
       "2        0.98         7  \n",
       "3        1.20         7  \n",
       "4        0.35         7  \n",
       "5        0.84         7  \n",
       "6        0.06         7  \n",
       "7        0.82         7  \n",
       "8        0.77         7  \n",
       "9        0.68         7  \n",
       "10       0.98         7  \n",
       "11       1.05         7  \n",
       "12       0.60         7  \n",
       "13       0.94         7  \n",
       "14       0.21         7  \n",
       "15       0.95         7  \n",
       "16       0.99         7  \n",
       "17       0.79         7  \n",
       "18       0.87         7  \n",
       "19       1.02         7  \n",
       "20       0.38         7  \n",
       "21       0.94         7  \n",
       "22       0.16         7  \n",
       "23       1.01         7  \n",
       "24       0.88         7  \n",
       "25       0.81         7  \n",
       "26       0.81         7  \n",
       "27       0.93         7  \n",
       "28       0.21         7  \n",
       "29       0.90         7  \n",
       "...       ...       ...  \n",
       "986061   0.83         8  \n",
       "986062   1.34         8  \n",
       "986063   0.00         8  \n",
       "986064   1.39         8  \n",
       "986065   1.07         8  \n",
       "986066   1.16         8  \n",
       "986067   0.76         8  \n",
       "986068   1.13         8  \n",
       "986069   0.84         8  \n",
       "986070   1.11         8  \n",
       "986071   0.00         8  \n",
       "986072   1.00         8  \n",
       "986073   0.67         8  \n",
       "986074   0.99         8  \n",
       "986075   1.03         8  \n",
       "986076   1.19         8  \n",
       "986077   1.08         8  \n",
       "986078   0.88         8  \n",
       "986079   0.00         8  \n",
       "986080   0.87         8  \n",
       "986081   0.74         8  \n",
       "986082   0.87         8  \n",
       "986083   1.12         8  \n",
       "986084   0.83         8  \n",
       "986085   0.00         8  \n",
       "986086   0.78         8  \n",
       "986087   1.28         8  \n",
       "986088   0.98         8  \n",
       "986089   1.36         8  \n",
       "986090   0.79         8  \n",
       "\n",
       "[986091 rows x 20 columns]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('./calibration_readone_data.csv', error_bad_lines=False)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = alex_data_df\n",
    "# result_df\n",
    "result_df = data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the sensor label index range from 0 to 7 \n",
    "# sensor_data = []\n",
    "# for i in range(0, 8):\n",
    "#     df = alex_data_df.where(alex_data_df['Sensor'] == i).dropna()\n",
    "#     sensor_data.append(df)\n",
    "# result_df = pd.concat(sensor_data).reset_index(drop=True)\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data processing and deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process original dataset, create windows (window_size samples(rows), about 1 second)\n",
    "data = []\n",
    "window = 1\n",
    "while window*window_size < len(result_df):\n",
    "    data_window = result_df[(window - 1)*window_size:window*window_size]\n",
    "    data.append(data_window.values)\n",
    "    window += 1\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21913"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n"
     ]
    }
   ],
   "source": [
    "# delete window if multiple activities and sensors presents\n",
    "cleaned_data = []\n",
    "for i in data:\n",
    "    previous_activity = -1\n",
    "    previous_sensor = -1\n",
    "    for j in i:\n",
    "        current_activity = j[19]\n",
    "        current_sensor = j[1]\n",
    "        if (previous_activity != -1) and (current_activity != previous_activity):\n",
    "            print(\"data contains different activities! Window droped\")\n",
    "            break\n",
    "#         elif (previous_sensor != -1) and (current_sensor != previous_sensor):\n",
    "#             print(\"data contains different sensors! Window droped\")\n",
    "#             break\n",
    "        else:\n",
    "            previous_activity = current_activity\n",
    "            previous_sensor = current_sensor\n",
    "    else:\n",
    "        cleaned_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21808"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 20)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "seed(101)\n",
    "shuffle(cleaned_data)\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label from each window\n",
    "labels = []\n",
    "for i in cleaned_data:\n",
    "    label = i[0][19]\n",
    "    labels.append(label)\n",
    "labels = np.array(labels)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each window\n",
    "features = []\n",
    "for i in cleaned_data:\n",
    "    new = np.delete(i, 19, 1)\n",
    "    features.append(new)\n",
    "features = np.array(features)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 19)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1.62, 4.0, -0.387, 0.62, -0.51, 0.46, 287.66...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[23.134, 3.0, 0.816, 0.02, -0.0, 0.58, 84.54,...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[1.301, 0.0, -0.08, -0.67, -0.06, 0.73, 216.8...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[1.614, 7.0, 0.861, -0.05, 0.01, 0.51, 74.79,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[32.284, 3.0, 0.7709999999999999, -0.02, -0.0...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[42.202, 0.0, -0.6940000000000001, -0.11, 0.1...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[35.964, 1.0, -0.26, -0.07, -0.01, -0.96, 163...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[11.457, 3.0, 0.299, 0.07, -0.07, -0.95, 229....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[0.703, 2.0, -0.883, 0.03, -0.23, 0.41, 321.3...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[45.265, 6.0, -0.83, 0.03, 0.14, 0.54, 307.18...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[[20.207, 4.0, 0.421, -0.31, 0.85, 0.07, 239.5...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[0.2773, 6.0, -0.981, -0.05, -0.19, -0.0, 15....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[[2.905, 3.0, 0.18600000000000005, -0.34, 0.16...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[[50.815, 2.0, 0.1, 0.09, 0.0, -0.99, 205.65, ...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[[47.312, 5.0, -0.939, -0.12, -0.27, 0.18, 354...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[[2.24, 2.0, 0.625, 0.16, 0.39, -0.66, 270.39,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[[35.764, 5.0, 0.049, -0.15, -0.98, 0.11, 176....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[[37.276, 5.0, 0.044, 0.67, 0.7, -0.24, 109.92...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[[1.622, 2.0, -0.768, -0.29, -0.05, 0.57, 306....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[[34.965, 1.0, -0.623, -0.14, 0.12, -0.76, 115...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[0.889, 1.0, 0.191, 0.11, 0.0, -0.98, 216.39,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[13.044, 0.0, -0.077, 0.04, 0.04, 1.0, 202.76...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[[22.19400000000001, 7.0, -0.948, -0.19, -0.05...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[1.474, 0.0, 0.26, -0.12, -0.07, -0.95, 223.8...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[[15.627, 2.0, -0.947, 0.27, -0.05, -0.17, 31....</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[[0.574, 2.0, -0.8859999999999999, -0.06, 0.01...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[[19.416, 6.0, -0.916, -0.08, -0.22, -0.32, 56...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[[3.024, 7.0, -0.875, 0.05, -0.0, 0.48, 316.44...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[[46.804, 6.0, 0.8140000000000001, -0.06, 0.15...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[[34.775, 7.0, 0.203, -0.06, -0.29, 0.93, 169....</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21778</th>\n",
       "      <td>[[4.7225, 6.0, 0.018000000000000002, -0.02, 0....</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21779</th>\n",
       "      <td>[[49.787, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21780</th>\n",
       "      <td>[[0.8490000000000001, 1.0, 0.045, -0.18, 0.03,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21781</th>\n",
       "      <td>[[2.064, 5.0, 0.527, 0.1, 0.13, 0.83, 129.08, ...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21782</th>\n",
       "      <td>[[30.143, 6.0, 0.722, -0.0, 0.69, -0.06, 309.5...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21783</th>\n",
       "      <td>[[37.021, 2.0, 0.447, -0.05, 0.12, -0.88, 247....</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21784</th>\n",
       "      <td>[[20.353, 0.0, 0.078, 0.27, -0.01, 0.96, 184.2...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21785</th>\n",
       "      <td>[[1.702, 0.0, 0.145, -0.15, -0.05, -0.98, 210....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21786</th>\n",
       "      <td>[[1.494, 5.0, -0.054000000000000006, -0.13, -0...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21787</th>\n",
       "      <td>[[54.316, 4.0, -0.298, 0.15, -0.94, -0.05, 211...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21788</th>\n",
       "      <td>[[18.906, 3.0, 0.145, -0.1, -0.11, 0.98, 176.0...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21789</th>\n",
       "      <td>[[14.963, 2.0, 0.99, -0.14, 0.02, 0.04, 18.37,...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21790</th>\n",
       "      <td>[[0.06, 3.0, -0.397, 0.08, -0.19, -0.9, 148.6,...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21791</th>\n",
       "      <td>[[21.738000000000003, 2.0, -0.863, -0.13, -0.2...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21792</th>\n",
       "      <td>[[2.317, 1.0, 0.113, 0.07, 0.0, -0.99, 207.1, ...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21793</th>\n",
       "      <td>[[2.37, 6.0, 0.959, -0.06, -0.04, -0.28, 342.1...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21794</th>\n",
       "      <td>[[2.189, 0.0, 0.172, -0.14, -0.05, -0.97, 213....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21795</th>\n",
       "      <td>[[9.079, 4.0, 0.985, -0.14, -0.05, -0.09, 5.06...</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21796</th>\n",
       "      <td>[[0.055, 7.0, 0.929, -0.01, -0.06, -0.36, 331....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21797</th>\n",
       "      <td>[[0.909, 4.0, 0.65, -0.47, 0.45, -0.39, 301.08...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21798</th>\n",
       "      <td>[[1.515, 1.0, 0.246, -0.14, -0.07, -0.96, 222....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21799</th>\n",
       "      <td>[[0.598, 6.0, 0.982, 0.01, 0.06, 0.18, 34.45, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21800</th>\n",
       "      <td>[[39.918, 4.0, -0.923, -0.08, -0.13, 0.35, 332...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21801</th>\n",
       "      <td>[[15.663, 5.0, -0.342, 0.07, -0.37, -0.86, 158...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21802</th>\n",
       "      <td>[[1.287, 6.0, 0.994, -0.03, -0.02, -0.11, 1.87...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21803</th>\n",
       "      <td>[[43.784, 5.0, -0.923, 0.04, -0.22, -0.31, 52....</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21804</th>\n",
       "      <td>[[1.22, 0.0, 0.092, 0.35, -0.07, 0.93, 184.76,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21805</th>\n",
       "      <td>[[15.2, 7.0, 0.998, 0.03, 0.02, 0.05, 19.64, 2...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21806</th>\n",
       "      <td>[[42.778, 2.0, 0.945, -0.03, -0.05, -0.32, 336...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21807</th>\n",
       "      <td>[[22.855, 1.0, -0.636, -0.1, 0.07, -0.76, 114....</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21808 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features  labels\n",
       "0      [[1.62, 4.0, -0.387, 0.62, -0.51, 0.46, 287.66...    13.0\n",
       "1      [[23.134, 3.0, 0.816, 0.02, -0.0, 0.58, 84.54,...     7.0\n",
       "2      [[1.301, 0.0, -0.08, -0.67, -0.06, 0.73, 216.8...     4.0\n",
       "3      [[1.614, 7.0, 0.861, -0.05, 0.01, 0.51, 74.79,...     4.0\n",
       "4      [[32.284, 3.0, 0.7709999999999999, -0.02, -0.0...     5.0\n",
       "5      [[42.202, 0.0, -0.6940000000000001, -0.11, 0.1...     8.0\n",
       "6      [[35.964, 1.0, -0.26, -0.07, -0.01, -0.96, 163...    12.0\n",
       "7      [[11.457, 3.0, 0.299, 0.07, -0.07, -0.95, 229....     0.0\n",
       "8      [[0.703, 2.0, -0.883, 0.03, -0.23, 0.41, 321.3...     4.0\n",
       "9      [[45.265, 6.0, -0.83, 0.03, 0.14, 0.54, 307.18...     1.0\n",
       "10     [[20.207, 4.0, 0.421, -0.31, 0.85, 0.07, 239.5...    10.0\n",
       "11     [[0.2773, 6.0, -0.981, -0.05, -0.19, -0.0, 15....     4.0\n",
       "12     [[2.905, 3.0, 0.18600000000000005, -0.34, 0.16...     4.0\n",
       "13     [[50.815, 2.0, 0.1, 0.09, 0.0, -0.99, 205.65, ...     8.0\n",
       "14     [[47.312, 5.0, -0.939, -0.12, -0.27, 0.18, 354...     5.0\n",
       "15     [[2.24, 2.0, 0.625, 0.16, 0.39, -0.66, 270.39,...     4.0\n",
       "16     [[35.764, 5.0, 0.049, -0.15, -0.98, 0.11, 176....     1.0\n",
       "17     [[37.276, 5.0, 0.044, 0.67, 0.7, -0.24, 109.92...     1.0\n",
       "18     [[1.622, 2.0, -0.768, -0.29, -0.05, 0.57, 306....     4.0\n",
       "19     [[34.965, 1.0, -0.623, -0.14, 0.12, -0.76, 115...     1.0\n",
       "20     [[0.889, 1.0, 0.191, 0.11, 0.0, -0.98, 216.39,...     4.0\n",
       "21     [[13.044, 0.0, -0.077, 0.04, 0.04, 1.0, 202.76...    10.0\n",
       "22     [[22.19400000000001, 7.0, -0.948, -0.19, -0.05...     0.0\n",
       "23     [[1.474, 0.0, 0.26, -0.12, -0.07, -0.95, 223.8...     4.0\n",
       "24     [[15.627, 2.0, -0.947, 0.27, -0.05, -0.17, 31....     2.0\n",
       "25     [[0.574, 2.0, -0.8859999999999999, -0.06, 0.01...     4.0\n",
       "26     [[19.416, 6.0, -0.916, -0.08, -0.22, -0.32, 56...    14.0\n",
       "27     [[3.024, 7.0, -0.875, 0.05, -0.0, 0.48, 316.44...     4.0\n",
       "28     [[46.804, 6.0, 0.8140000000000001, -0.06, 0.15...     9.0\n",
       "29     [[34.775, 7.0, 0.203, -0.06, -0.29, 0.93, 169....     8.0\n",
       "...                                                  ...     ...\n",
       "21778  [[4.7225, 6.0, 0.018000000000000002, -0.02, 0....     7.0\n",
       "21779  [[49.787, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...     0.0\n",
       "21780  [[0.8490000000000001, 1.0, 0.045, -0.18, 0.03,...     4.0\n",
       "21781  [[2.064, 5.0, 0.527, 0.1, 0.13, 0.83, 129.08, ...    13.0\n",
       "21782  [[30.143, 6.0, 0.722, -0.0, 0.69, -0.06, 309.5...    10.0\n",
       "21783  [[37.021, 2.0, 0.447, -0.05, 0.12, -0.88, 247....     7.0\n",
       "21784  [[20.353, 0.0, 0.078, 0.27, -0.01, 0.96, 184.2...     1.0\n",
       "21785  [[1.702, 0.0, 0.145, -0.15, -0.05, -0.98, 210....     4.0\n",
       "21786  [[1.494, 5.0, -0.054000000000000006, -0.13, -0...     4.0\n",
       "21787  [[54.316, 4.0, -0.298, 0.15, -0.94, -0.05, 211...    10.0\n",
       "21788  [[18.906, 3.0, 0.145, -0.1, -0.11, 0.98, 176.0...     9.0\n",
       "21789  [[14.963, 2.0, 0.99, -0.14, 0.02, 0.04, 18.37,...     7.0\n",
       "21790  [[0.06, 3.0, -0.397, 0.08, -0.19, -0.9, 148.6,...     9.0\n",
       "21791  [[21.738000000000003, 2.0, -0.863, -0.13, -0.2...     0.0\n",
       "21792  [[2.317, 1.0, 0.113, 0.07, 0.0, -0.99, 207.1, ...    13.0\n",
       "21793  [[2.37, 6.0, 0.959, -0.06, -0.04, -0.28, 342.1...    13.0\n",
       "21794  [[2.189, 0.0, 0.172, -0.14, -0.05, -0.97, 213....     4.0\n",
       "21795  [[9.079, 4.0, 0.985, -0.14, -0.05, -0.09, 5.06...    11.0\n",
       "21796  [[0.055, 7.0, 0.929, -0.01, -0.06, -0.36, 331....     4.0\n",
       "21797  [[0.909, 4.0, 0.65, -0.47, 0.45, -0.39, 301.08...     4.0\n",
       "21798  [[1.515, 1.0, 0.246, -0.14, -0.07, -0.96, 222....     4.0\n",
       "21799  [[0.598, 6.0, 0.982, 0.01, 0.06, 0.18, 34.45, ...     4.0\n",
       "21800  [[39.918, 4.0, -0.923, -0.08, -0.13, 0.35, 332...    14.0\n",
       "21801  [[15.663, 5.0, -0.342, 0.07, -0.37, -0.86, 158...     8.0\n",
       "21802  [[1.287, 6.0, 0.994, -0.03, -0.02, -0.11, 1.87...    13.0\n",
       "21803  [[43.784, 5.0, -0.923, 0.04, -0.22, -0.31, 52....     9.0\n",
       "21804  [[1.22, 0.0, 0.092, 0.35, -0.07, 0.93, 184.76,...     4.0\n",
       "21805  [[15.2, 7.0, 0.998, 0.03, 0.02, 0.05, 19.64, 2...     1.0\n",
       "21806  [[42.778, 2.0, 0.945, -0.03, -0.05, -0.32, 336...     7.0\n",
       "21807  [[22.855, 1.0, -0.636, -0.1, 0.07, -0.76, 114....     7.0\n",
       "\n",
       "[21808 rows x 2 columns]"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the features and labels\n",
    "k = list(zip(features, labels))\n",
    "activity_data = pd.DataFrame(k)\n",
    "activity_data.columns = ['features', 'labels']\n",
    "activity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of activity. The final output of neural net \n",
    "# has to have max_index + 1 output\n",
    "max_index = activity_data['labels'].max()\n",
    "label_size = int(max_index + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Training on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"CUDA is available! Training on GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to test, validation, and train\n",
    "valid_size = 0.2\n",
    "test_size = 0.2\n",
    "activity_data.columns = [\"features\", \"labels\"]\n",
    "activity_data_train = activity_data[:int(len(activity_data)*(1-valid_size-test_size))]\n",
    "activity_data_valid = activity_data[int(len(activity_data)*(1-valid_size-test_size)):int(len(activity_data)*(1-test_size))]\n",
    "activity_data_test = activity_data[int(len(activity_data)*(1-test_size)):]\n",
    "# activity_data_train.to_csv(\"./activity_data_train.csv\", encoding='utf-8-sig')\n",
    "# activity_data_valid.to_csv(\"./activity_data_valid.csv\", encoding='utf-8-sig')\n",
    "# activity_data_train.to_csv(\"./activity_data_test.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset in pytorch\n",
    "class DatasetSpineTrack(Dataset):\n",
    "    \n",
    "    def __init__(self, file, transform=None):\n",
    "        #self.data = pd.read_csv(file_path)\n",
    "        self.data = file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        features = torch.tensor(self.data[\"features\"].iloc[index])\n",
    "        features = features.view(channel, window_size, 19) \n",
    "        labels = torch.tensor(self.data[\"labels\"].iloc[index], dtype=torch.long)\n",
    "        #print(labels.type())\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training and testing dataset in csv\n",
    "# train_dataset = DatasetSpineTrack(\"./activity_data_train.csv\")\n",
    "# valid_dataset = DatasetSpineTrack(\"./activity_data_valid.csv\")\n",
    "# test_dataset = DatasetSpineTrack(\"./activity_data_test.csv\")\n",
    "train_dataset = DatasetSpineTrack(activity_data_train)\n",
    "valid_dataset = DatasetSpineTrack(activity_data_valid)\n",
    "test_dataset = DatasetSpineTrack(activity_data_test)\n",
    "feature, label = train_dataset.__getitem__(0)\n",
    "#feature\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Network Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = False\n",
    "# train_on_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTM(nn.Module):\n",
    "    def __init__(self, channel, label_size):\n",
    "        super(CNNLSTM, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.conv2 = torch.nn.Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        \n",
    "        # bn1 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.max_pool = torch.nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 256, bias=True)\n",
    "        self.fc2 = nn.Linear(256, label_size, bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.lstm = nn.LSTM(256, 128, 2, dropout=0.25)\n",
    "        \n",
    "    def forward(self, hidden)\n",
    "        \n",
    "\n",
    "\n",
    "CNNLSTM = nn.Sequential()\n",
    "CNNLSTM.add_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=15, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=False)\n",
    "# window_size channels\n",
    "# model.conv1 = torch.nn.Conv2d(window_size, batch_size, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(2048, label_size, bias=True)\n",
    "model.add_module(\"dropout\", torch.nn.Dropout(p=0.5))\n",
    "model = model.double()\n",
    "\n",
    "# move tensors to GPU is CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Run all above \n",
    "<a name='bookmark' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Network\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.438185 \tValidation Loss: 1.375088\n",
      "Validation loss decreased (inf --> 1.375088).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.872650 \tValidation Loss: 0.647232\n",
      "Validation loss decreased (1.375088 --> 0.647232).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.771780 \tValidation Loss: 0.860717\n",
      "Epoch: 4 \tTraining Loss: 0.637491 \tValidation Loss: 4.911091\n",
      "Epoch: 5 \tTraining Loss: 0.595495 \tValidation Loss: 0.562561\n",
      "Validation loss decreased (0.647232 --> 0.562561).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 0.475186 \tValidation Loss: 0.378704\n",
      "Validation loss decreased (0.562561 --> 0.378704).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.426539 \tValidation Loss: 0.445898\n",
      "Epoch: 8 \tTraining Loss: 0.438716 \tValidation Loss: 3.131602\n",
      "Epoch: 9 \tTraining Loss: 0.384222 \tValidation Loss: 0.434618\n",
      "Epoch: 10 \tTraining Loss: 0.350891 \tValidation Loss: 0.718135\n",
      "Epoch: 11 \tTraining Loss: 0.371245 \tValidation Loss: 0.516086\n",
      "Epoch: 12 \tTraining Loss: 0.321571 \tValidation Loss: 0.429405\n",
      "Epoch: 13 \tTraining Loss: 0.397637 \tValidation Loss: 0.309141\n",
      "Validation loss decreased (0.378704 --> 0.309141).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.264895 \tValidation Loss: 0.323614\n",
      "Epoch: 15 \tTraining Loss: 0.257729 \tValidation Loss: 1.584170\n",
      "Epoch: 16 \tTraining Loss: 0.258166 \tValidation Loss: 0.519573\n",
      "Epoch: 17 \tTraining Loss: 0.241312 \tValidation Loss: 0.286064\n",
      "Validation loss decreased (0.309141 --> 0.286064).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.184488 \tValidation Loss: 0.688867\n",
      "Epoch: 19 \tTraining Loss: 0.166981 \tValidation Loss: 4.598111\n",
      "Epoch: 20 \tTraining Loss: 0.154185 \tValidation Loss: 0.242316\n",
      "Validation loss decreased (0.286064 --> 0.242316).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.182883 \tValidation Loss: 0.247960\n",
      "Epoch: 22 \tTraining Loss: 0.136947 \tValidation Loss: 2.222412\n",
      "Epoch: 23 \tTraining Loss: 0.151198 \tValidation Loss: 0.260932\n",
      "Epoch: 24 \tTraining Loss: 0.198096 \tValidation Loss: 0.287587\n",
      "Epoch: 25 \tTraining Loss: 0.150419 \tValidation Loss: 0.309402\n",
      "Epoch: 26 \tTraining Loss: 0.110778 \tValidation Loss: 7.121719\n",
      "Epoch: 27 \tTraining Loss: 0.104459 \tValidation Loss: 0.281432\n",
      "Epoch: 28 \tTraining Loss: 0.102457 \tValidation Loss: 0.462236\n",
      "Epoch: 29 \tTraining Loss: 0.128384 \tValidation Loss: 0.614708\n",
      "Epoch: 30 \tTraining Loss: 0.116258 \tValidation Loss: 0.311385\n",
      "Training time: 144 min 31 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for features, labels in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*features.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for features, labels in validloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*features.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_Spinetrack_data.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "# output running time\n",
    "running_time = time.time() - start_time\n",
    "sec = running_time % 60\n",
    "miniute = running_time / 60\n",
    "print(\"Training time: {} min {} sec\".format(int(miniute), int(sec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load on gpu\n",
    "# model.load_state_dict(torch.load('model_Spinetrack_3.pt'))\n",
    "\n",
    "# load on cpu\n",
    "model.load_state_dict(torch.load('model_Spinetrack_data.pt', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Test your trained model on previously unseen data! A \"good\" result will be a result that gets more than 70% accuracy on these test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release all the GPU memory cache that can be freed\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy and each class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.317470\n",
      "\n",
      "Test Accuracy of Pulling_OneH: 88% (243/275)\n",
      "Test Accuracy of Overhead: 99% (279/281)\n",
      "Test Accuracy of Pulling: 88% (118/133)\n",
      "Test Accuracy of Sitting: 100% (125/125)\n",
      "Test Accuracy of Lifting: 90% (1253/1384)\n",
      "Test Accuracy of Crawling: 88% (117/132)\n",
      "Test Accuracy of Standing: 96% (129/133)\n",
      "Test Accuracy of Carrying: 96% (377/390)\n",
      "Test Accuracy of Walking: 91% (117/128)\n",
      "Test Accuracy of Pushing: 75% (165/220)\n",
      "Test Accuracy of Reaching: 99% (351/352)\n",
      "Test Accuracy of Static_Stoop: 98% (96/97)\n",
      "Test Accuracy of Kneeling: 100% (119/119)\n",
      "Test Accuracy of Lifting_OneH: 90% (437/481)\n",
      "Test Accuracy of Crouching: 67% (76/112)\n",
      "\n",
      "Test Accuracy (Overall): 91% (4002/4362)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(len(index_dict)))\n",
    "class_total = list(0. for i in range(len(index_dict)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "model.eval()\n",
    "torch.no_grad()\n",
    "# iterate over test data\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(features)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, labels)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*features.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(testloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(index_dict)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            list(index_dict.keys())[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (list(index_dict.keys())[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy (different calculation method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off gradients for validation, saves memory and computations\n",
    "torch.no_grad()\n",
    "accuracy = 0\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    loss = model(features)\n",
    "    test_loss += criterion(loss, labels)\n",
    "\n",
    "#     ps = torch.exp(loss)\n",
    "    top_p, top_class = loss.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out pulling_OneH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
