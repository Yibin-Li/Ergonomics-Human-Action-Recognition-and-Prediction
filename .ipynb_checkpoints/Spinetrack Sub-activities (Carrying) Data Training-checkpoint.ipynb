{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and reformating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\\\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from random import shuffle, seed\n",
    "\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "window_size: The number of timesteps in one window (e.g. how many rows in one window).\n",
    "\n",
    "channel: The number of features in one window. Similar to image channels (RGB).\n",
    "\n",
    "batch_size: The numebr of windows in one batch.\n",
    "\n",
    "learning_rate: How fast the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "channel = 1\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {'Carrying_5':0, 'Carrying_10':1, 'Carrying_20':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Carrying_5': 0, 'Carrying_10': 1, 'Carrying_20': 2}\n"
     ]
    }
   ],
   "source": [
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['TimeSec', 'Sensor', 'Quatx', 'Quaty', 'Quatz', 'Quat0', 'Heading',\n",
    "       'Pitch', 'Roll', 'LinAccx', 'LinAccy', 'LinAccz', 'Vbat', 'Accx',\n",
    "       'Accy', 'Accz', 'Gyrox', 'Gyroy', 'Gyroz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Go to \n",
    "<a href=#bookmark> Run all cell above</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alex_data number of activity: 3\n",
      "Alexander_data number of activity: 3\n",
      "Charlotte_data number of activity: 3\n",
      "Christian_data number of activity: 0\n",
      "Elias_data number of activity: 3\n",
      "Jesse_data number of activity: 3\n",
      "Jiyoo_data number of activity: 3\n"
     ]
    }
   ],
   "source": [
    "directory = \"./Spinetrack Data/data/\"\n",
    "file = []\n",
    "\n",
    "for d in sorted(os.listdir(directory)):\n",
    "    if d != '.DS_Store':\n",
    "        files = directory + d + \"/Processed_redone\"\n",
    "        name = []\n",
    "        try:\n",
    "            for f in sorted(os.listdir(files)):\n",
    "                if f.startswith(\"Carrying_\") and f.endswith(\".csv\"): \n",
    "                    name.append(files + \"/\" + f)\n",
    "            file.append(name)\n",
    "            print(\"%s number of activity: %s\" %(d, len(name)))\n",
    "        except:\n",
    "            pass\n",
    "# file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrying_10\n",
      "processing:  ./Spinetrack Data/data/Alex_data/Processed_redone/Carrying_10.csv\n",
      "Carrying_20\n",
      "processing:  ./Spinetrack Data/data/Alex_data/Processed_redone/Carrying_20.csv\n",
      "Carrying_5\n",
      "processing:  ./Spinetrack Data/data/Alex_data/Processed_redone/Carrying_5.csv\n",
      "Carrying_10\n",
      "processing:  ./Spinetrack Data/data/Alexander_data/Processed_redone/Carrying_10.csv\n",
      "Carrying_20\n",
      "processing:  ./Spinetrack Data/data/Alexander_data/Processed_redone/Carrying_20.csv\n",
      "Carrying_5\n",
      "processing:  ./Spinetrack Data/data/Alexander_data/Processed_redone/Carrying_5.csv\n",
      "Carrying_10\n",
      "processing:  ./Spinetrack Data/data/Charlotte_data/Processed_redone/Carrying_10.csv\n",
      "Carrying_20\n",
      "processing:  ./Spinetrack Data/data/Charlotte_data/Processed_redone/Carrying_20.csv\n",
      "Carrying_5\n",
      "processing:  ./Spinetrack Data/data/Charlotte_data/Processed_redone/Carrying_5.csv\n",
      "Carrying_10\n",
      "processing:  ./Spinetrack Data/data/Elias_data/Processed_redone/Carrying_10.csv\n",
      "Carrying_20\n",
      "processing:  ./Spinetrack Data/data/Elias_data/Processed_redone/Carrying_20.csv\n",
      "Carrying_5\n",
      "processing:  ./Spinetrack Data/data/Elias_data/Processed_redone/Carrying_5.csv\n",
      "Carrying_10\n",
      "processing:  ./Spinetrack Data/data/Jesse_data/Processed_redone/Carrying_10.csv\n",
      "Carrying_20\n",
      "processing:  ./Spinetrack Data/data/Jesse_data/Processed_redone/Carrying_20.csv\n",
      "Carrying_5\n",
      "processing:  ./Spinetrack Data/data/Jesse_data/Processed_redone/Carrying_5.csv\n",
      "Carrying_10\n",
      "processing:  ./Spinetrack Data/data/Jiyoo_data/Processed_redone/Carrying_10.csv\n",
      "Carrying_20\n",
      "processing:  ./Spinetrack Data/data/Jiyoo_data/Processed_redone/Carrying_20.csv\n",
      "Carrying_5\n",
      "processing:  ./Spinetrack Data/data/Jiyoo_data/Processed_redone/Carrying_5.csv\n"
     ]
    }
   ],
   "source": [
    "frame = []\n",
    "for person in file:\n",
    "    for file_path in person:\n",
    "        activity_lst = file_path.split(\"/\")[-1].split('.')[0]\n",
    "        \n",
    "        activity_name = activity_lst\n",
    "        print(activity_name)\n",
    "        print(\"processing: \", file_path)\n",
    "        if activity_name in index_dict.keys():\n",
    "            df = pd.read_csv(file_path, error_bad_lines=False)\n",
    "            df.columns = header\n",
    "            df[\"activity\"] = [index_dict[activity_name] for i in range(len(df))]\n",
    "            frame.append(df)\n",
    "            \n",
    "            # mkdir if not exist. Save to local csv file\n",
    "#             if not (os.path.exists('./Spinetrack Data/Yibin_Processed/' + category + folder_name)):\n",
    "#                 os.makedirs('./Spinetrack Data/Yibin_Processed/' + category + folder_name)\n",
    "#             csv_name = '/' + activity_name + '.csv'\n",
    "#             folder_name = file_path.split(\"/\")[-3] # person's name \n",
    "#             category = 'data/' # data or task\n",
    "#             file_name = './Spinetrack Data/Yibin_Processed/' + category + folder_name + csv_name\n",
    "#             df.to_csv(file_name) # save csv processed file to local\n",
    "            \n",
    "            #print(file_path)\n",
    "data_df = pd.concat(frame)\n",
    "\n",
    "result_df = data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data processing and deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process original dataset, create windows (window_size samples(rows), about 1 second)\n",
    "data = []\n",
    "window = 1\n",
    "while window*window_size < len(result_df):\n",
    "    data_window = result_df[(window - 1)*window_size:window*window_size]\n",
    "    data.append(data_window.values)\n",
    "    window += 1\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8061"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n"
     ]
    }
   ],
   "source": [
    "# delete window if multiple activities and sensors presents\n",
    "cleaned_data = []\n",
    "for i in data:\n",
    "    previous_activity = -1\n",
    "    previous_sensor = -1\n",
    "    for j in i:\n",
    "        current_activity = j[19]\n",
    "        current_sensor = j[1]\n",
    "        if (previous_activity != -1) and (current_activity != previous_activity):\n",
    "            print(\"data contains different activities! Window droped\")\n",
    "            break\n",
    "#         elif (previous_sensor != -1) and (current_sensor != previous_sensor):\n",
    "#             print(\"data contains different sensors! Window droped\")\n",
    "#             break\n",
    "        else:\n",
    "            previous_activity = current_activity\n",
    "            previous_sensor = current_sensor\n",
    "    else:\n",
    "        cleaned_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8044"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "seed(101)\n",
    "shuffle(cleaned_data)\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label from each window\n",
    "labels = []\n",
    "for i in cleaned_data:\n",
    "    label = i[0][19]\n",
    "    labels.append(label)\n",
    "labels = np.array(labels)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each window\n",
    "features = []\n",
    "for i in cleaned_data:\n",
    "    new = np.delete(i, 19, 1)\n",
    "    features.append(new)\n",
    "features = np.array(features)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8044, 10, 19)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.resize(features[:int(len(cleaned_data)*0.8)], (int(len(cleaned_data)*0.8), -1))\n",
    "# Y_train = labels[:int(len(cleaned_data)*0.8)]\n",
    "# X_test = np.resize(features[int(len(cleaned_data)*0.8):], ((features.shape[0] - int(len(cleaned_data)*0.8)), -1))\n",
    "# Y_test = labels[int(len(cleaned_data)*0.8):]\n",
    "\n",
    "# clf = svm.SVC()\n",
    "# clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import metrics\n",
    "# Y_predict = clf.predict(X_test)\n",
    "# metrics.accuracy_score(Y_test, Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[30.502, 3.0, -0.18, 0.03, 0.02, -0.98, 173.2...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[51.61, 6.0, 0.161, 0.03, 0.02, -0.99, 212.54...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[36.451, 3.0, -0.9940000000000001, 0.03, -0.1...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[4.9510000000000005, 7.0, 0.431, 0.19, -0.14,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[42.03, 3.0, -0.7509999999999999, 0.06, -0.05...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[14.633, 3.0, 0.289, -0.1, -0.08, -0.95, 227....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[50.248999999999995, 2.0, 0.513, -0.11, -0.04...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[25.961, 1.0, 0.5710000000000001, 0.03, -0.05...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[10.77, 6.0, 0.873, 0.08, 0.1, -0.47, 317.47,...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[11.544, 3.0, -0.7390000000000001, -0.07, 0.0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[[13.58, 3.0, -0.5489999999999999, -0.07, -0.0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[26.096, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[[19.042, 6.0, 0.82, -0.04, 0.02, 0.57, 83.57,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[[4.5407, 1.0, 0.983, -0.04, -0.17, 0.05, 20.8...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[[21.81, 6.0, 0.7929999999999999, 0.05, -0.08,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[[42.876000000000005, 4.0, 0.685, -0.63, 0.36,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[[9.707, 5.0, -0.45399999999999996, -0.53, -0....</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[[43.118, 3.0, -0.284, 0.09, 0.01, -0.95, 160....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[[50.409, 6.0, -0.379, 0.12, -0.09, -0.91, 149...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[[12.537, 6.0, -0.985, -0.05, -0.14, 0.1, 3.58...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[52.006, 3.0, -0.66, 0.07, -0.13, -0.74, 111....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[49.801, 6.0, -0.696, 0.04, 0.12, -0.71, 105....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[[23.685, 3.0, 0.997, 0.01, -0.01, -0.07, 5.89...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[31.965999999999998, 2.0, -0.563, 0.02, -0.03...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[[19.309, 6.0, -0.11, -0.14, -0.06, -0.98, 180...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[[0.1418, 2.0, -0.987, 0.13, -0.1, 0.02, 10.03...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[[37.201, 3.0, -0.069, -0.09, -0.01, 0.99, 201...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[[23.869, 7.0, -0.972, -0.12, 0.04, -0.2, 36.0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[[36.38, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[[45.24, 1.0, -0.6920000000000001, 0.13, 0.09,...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>[[48.957, 3.0, -0.953, -0.04, -0.19, -0.23, 43...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8015</th>\n",
       "      <td>[[25.421999999999997, 2.0, 0.732, -0.22, 0.04,...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8016</th>\n",
       "      <td>[[48.537, 1.0, 0.6709999999999999, 0.04, -0.12...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8017</th>\n",
       "      <td>[[26.783, 5.0, 0.773, 0.59, 0.19, -0.14, 14.29...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8018</th>\n",
       "      <td>[[25.294, 3.0, 0.8809999999999999, -0.06, 0.08...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>[[5.9031, 4.0, -0.47200000000000003, 0.33, 0.5...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8020</th>\n",
       "      <td>[[41.177, 3.0, -0.28800000000000003, -0.14, 0....</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021</th>\n",
       "      <td>[[40.759, 3.0, -0.217, -0.17, -0.19, -0.94, 16...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8022</th>\n",
       "      <td>[[37.967, 3.0, 0.8809999999999999, -0.08, 0.07...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8023</th>\n",
       "      <td>[[46.598, 5.0, -0.737, -0.45, -0.44, -0.25, 71...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8024</th>\n",
       "      <td>[[13.142000000000001, 3.0, -0.235, -0.06, -0.0...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8025</th>\n",
       "      <td>[[30.581, 3.0, -0.523, 0.05, 0.04, 0.85, 257.2...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8026</th>\n",
       "      <td>[[37.616, 7.0, 0.992, 0.08, -0.05, 0.08, 22.28...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8027</th>\n",
       "      <td>[[60.047, 6.0, -0.41100000000000003, -0.09, -0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8028</th>\n",
       "      <td>[[38.817, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8029</th>\n",
       "      <td>[[56.403999999999996, 1.0, 0.986, -0.01, 0.16,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8030</th>\n",
       "      <td>[[49.706, 6.0, -0.34600000000000003, -0.15, -0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8031</th>\n",
       "      <td>[[48.92, 2.0, 0.639, 0.02, 0.05, -0.77, 273.53...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8032</th>\n",
       "      <td>[[2.0303, 5.0, 0.303, 0.25, -0.64, -0.65, 240....</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8033</th>\n",
       "      <td>[[0.6191, 1.0, -0.9890000000000001, 0.04, 0.1,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8034</th>\n",
       "      <td>[[51.156000000000006, 5.0, -0.7070000000000001...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8035</th>\n",
       "      <td>[[18.194000000000003, 7.0, -0.337, 0.26, 0.1, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8036</th>\n",
       "      <td>[[12.93, 3.0, 0.312, -0.04, 0.14, 0.94, 158.44...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8037</th>\n",
       "      <td>[[45.635, 4.0, -0.821, 0.55, -0.15, -0.05, 9.6...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8038</th>\n",
       "      <td>[[40.830999999999996, 5.0, 0.627, 0.67, 0.34, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8039</th>\n",
       "      <td>[[34.527, 7.0, -0.637, -0.08, 0.07, 0.76, 273....</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8040</th>\n",
       "      <td>[[29.275, 4.0, 0.7240000000000001, -0.63, 0.21...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8041</th>\n",
       "      <td>[[28.3, 5.0, 0.785, 0.53, 0.31, 0.04, 40.38, 2...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8042</th>\n",
       "      <td>[[25.543000000000003, 4.0, 0.187, 0.22, 0.57, ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8043</th>\n",
       "      <td>[[27.08, 1.0, 0.18100000000000002, -0.08, -0.0...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8044 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               features  labels\n",
       "0     [[30.502, 3.0, -0.18, 0.03, 0.02, -0.98, 173.2...     0.0\n",
       "1     [[51.61, 6.0, 0.161, 0.03, 0.02, -0.99, 212.54...     2.0\n",
       "2     [[36.451, 3.0, -0.9940000000000001, 0.03, -0.1...     0.0\n",
       "3     [[4.9510000000000005, 7.0, 0.431, 0.19, -0.14,...     0.0\n",
       "4     [[42.03, 3.0, -0.7509999999999999, 0.06, -0.05...     1.0\n",
       "5     [[14.633, 3.0, 0.289, -0.1, -0.08, -0.95, 227....     1.0\n",
       "6     [[50.248999999999995, 2.0, 0.513, -0.11, -0.04...     2.0\n",
       "7     [[25.961, 1.0, 0.5710000000000001, 0.03, -0.05...     2.0\n",
       "8     [[10.77, 6.0, 0.873, 0.08, 0.1, -0.47, 317.47,...     2.0\n",
       "9     [[11.544, 3.0, -0.7390000000000001, -0.07, 0.0...     2.0\n",
       "10    [[13.58, 3.0, -0.5489999999999999, -0.07, -0.0...     2.0\n",
       "11    [[26.096, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...     2.0\n",
       "12    [[19.042, 6.0, 0.82, -0.04, 0.02, 0.57, 83.57,...     0.0\n",
       "13    [[4.5407, 1.0, 0.983, -0.04, -0.17, 0.05, 20.8...     0.0\n",
       "14    [[21.81, 6.0, 0.7929999999999999, 0.05, -0.08,...     0.0\n",
       "15    [[42.876000000000005, 4.0, 0.685, -0.63, 0.36,...     0.0\n",
       "16    [[9.707, 5.0, -0.45399999999999996, -0.53, -0....     2.0\n",
       "17    [[43.118, 3.0, -0.284, 0.09, 0.01, -0.95, 160....     0.0\n",
       "18    [[50.409, 6.0, -0.379, 0.12, -0.09, -0.91, 149...     0.0\n",
       "19    [[12.537, 6.0, -0.985, -0.05, -0.14, 0.1, 3.58...     2.0\n",
       "20    [[52.006, 3.0, -0.66, 0.07, -0.13, -0.74, 111....     1.0\n",
       "21    [[49.801, 6.0, -0.696, 0.04, 0.12, -0.71, 105....     0.0\n",
       "22    [[23.685, 3.0, 0.997, 0.01, -0.01, -0.07, 5.89...     2.0\n",
       "23    [[31.965999999999998, 2.0, -0.563, 0.02, -0.03...     0.0\n",
       "24    [[19.309, 6.0, -0.11, -0.14, -0.06, -0.98, 180...     2.0\n",
       "25    [[0.1418, 2.0, -0.987, 0.13, -0.1, 0.02, 10.03...     2.0\n",
       "26    [[37.201, 3.0, -0.069, -0.09, -0.01, 0.99, 201...     2.0\n",
       "27    [[23.869, 7.0, -0.972, -0.12, 0.04, -0.2, 36.0...     0.0\n",
       "28    [[36.38, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....     2.0\n",
       "29    [[45.24, 1.0, -0.6920000000000001, 0.13, 0.09,...     2.0\n",
       "...                                                 ...     ...\n",
       "8014  [[48.957, 3.0, -0.953, -0.04, -0.19, -0.23, 43...     2.0\n",
       "8015  [[25.421999999999997, 2.0, 0.732, -0.22, 0.04,...     2.0\n",
       "8016  [[48.537, 1.0, 0.6709999999999999, 0.04, -0.12...     0.0\n",
       "8017  [[26.783, 5.0, 0.773, 0.59, 0.19, -0.14, 14.29...     1.0\n",
       "8018  [[25.294, 3.0, 0.8809999999999999, -0.06, 0.08...     2.0\n",
       "8019  [[5.9031, 4.0, -0.47200000000000003, 0.33, 0.5...     2.0\n",
       "8020  [[41.177, 3.0, -0.28800000000000003, -0.14, 0....     2.0\n",
       "8021  [[40.759, 3.0, -0.217, -0.17, -0.19, -0.94, 16...     0.0\n",
       "8022  [[37.967, 3.0, 0.8809999999999999, -0.08, 0.07...     0.0\n",
       "8023  [[46.598, 5.0, -0.737, -0.45, -0.44, -0.25, 71...     1.0\n",
       "8024  [[13.142000000000001, 3.0, -0.235, -0.06, -0.0...     0.0\n",
       "8025  [[30.581, 3.0, -0.523, 0.05, 0.04, 0.85, 257.2...     1.0\n",
       "8026  [[37.616, 7.0, 0.992, 0.08, -0.05, 0.08, 22.28...     0.0\n",
       "8027  [[60.047, 6.0, -0.41100000000000003, -0.09, -0...     2.0\n",
       "8028  [[38.817, 2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0...     2.0\n",
       "8029  [[56.403999999999996, 1.0, 0.986, -0.01, 0.16,...     1.0\n",
       "8030  [[49.706, 6.0, -0.34600000000000003, -0.15, -0...     2.0\n",
       "8031  [[48.92, 2.0, 0.639, 0.02, 0.05, -0.77, 273.53...     1.0\n",
       "8032  [[2.0303, 5.0, 0.303, 0.25, -0.64, -0.65, 240....     1.0\n",
       "8033  [[0.6191, 1.0, -0.9890000000000001, 0.04, 0.1,...     1.0\n",
       "8034  [[51.156000000000006, 5.0, -0.7070000000000001...     1.0\n",
       "8035  [[18.194000000000003, 7.0, -0.337, 0.26, 0.1, ...     0.0\n",
       "8036  [[12.93, 3.0, 0.312, -0.04, 0.14, 0.94, 158.44...     1.0\n",
       "8037  [[45.635, 4.0, -0.821, 0.55, -0.15, -0.05, 9.6...     2.0\n",
       "8038  [[40.830999999999996, 5.0, 0.627, 0.67, 0.34, ...     1.0\n",
       "8039  [[34.527, 7.0, -0.637, -0.08, 0.07, 0.76, 273....     0.0\n",
       "8040  [[29.275, 4.0, 0.7240000000000001, -0.63, 0.21...     2.0\n",
       "8041  [[28.3, 5.0, 0.785, 0.53, 0.31, 0.04, 40.38, 2...     1.0\n",
       "8042  [[25.543000000000003, 4.0, 0.187, 0.22, 0.57, ...     1.0\n",
       "8043  [[27.08, 1.0, 0.18100000000000002, -0.08, -0.0...     2.0\n",
       "\n",
       "[8044 rows x 2 columns]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the features and labels\n",
    "k = list(zip(features, labels))\n",
    "activity_data = pd.DataFrame(k)\n",
    "activity_data.columns = ['features', 'labels']\n",
    "activity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 19)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_data['features'][8043].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of activity. The final output of neural net \n",
    "# has to have max_index + 1 output\n",
    "max_index = activity_data['labels'].max()\n",
    "label_size = int(max_index + 1)\n",
    "# label_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Training on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"CUDA is available! Training on GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to test, validation, and train\n",
    "valid_size = 0.1\n",
    "test_size = 0.1\n",
    "activity_data.columns = [\"features\", \"labels\"]\n",
    "activity_data_train = activity_data[:int(len(activity_data)*(1-valid_size-test_size))]\n",
    "activity_data_valid = activity_data[int(len(activity_data)*(1-valid_size-test_size)):int(len(activity_data)*(1-test_size))]\n",
    "activity_data_test = activity_data[int(len(activity_data)*(1-test_size)):]\n",
    "# activity_data_train.to_csv(\"./activity_data_train.csv\", encoding='utf-8-sig')\n",
    "# activity_data_valid.to_csv(\"./activity_data_valid.csv\", encoding='utf-8-sig')\n",
    "# activity_data_train.to_csv(\"./activity_data_test.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6435, 2)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset in pytorch\n",
    "class DatasetSpineTrack(Dataset):\n",
    "    \n",
    "    def __init__(self, file, transform=None):\n",
    "        #self.data = pd.read_csv(file_path)\n",
    "        self.data = file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        features = torch.tensor(self.data[\"features\"].iloc[index])\n",
    "        features = features.view(channel, window_size, -1) \n",
    "        labels = torch.tensor(self.data[\"labels\"].iloc[index], dtype=torch.long)\n",
    "        #print(labels.type())\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training and testing dataset in csv\n",
    "# train_dataset = DatasetSpineTrack(\"./activity_data_train.csv\")\n",
    "# valid_dataset = DatasetSpineTrack(\"./activity_data_valid.csv\")\n",
    "# test_dataset = DatasetSpineTrack(\"./activity_data_test.csv\")\n",
    "train_dataset = DatasetSpineTrack(activity_data_train)\n",
    "valid_dataset = DatasetSpineTrack(activity_data_valid)\n",
    "test_dataset = DatasetSpineTrack(activity_data_test)\n",
    "feature, label = train_dataset.__getitem__(0)\n",
    "#feature\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Network Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_gpu = False\n",
    "# train_on_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.25)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TimeCNN(nn.Module):\n",
    "    def __init__(self, channel, label_size):\n",
    "        super(TimeCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channel, 32, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        \n",
    "        # bn1 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 64, bias=True)\n",
    "        self.fc2 = nn.Linear(64, label_size, bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "#         print(\"forward shape 1: \", x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(batch_size, -1)\n",
    "#         print(\"forward shape 2: \", x.shape)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = TimeCNN(channel, label_size)\n",
    "model = model.double()\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Run all above \n",
    "<a name='bookmark' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Network\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.283499 \tValidation Loss: 0.412117\n",
      "Validation loss decreased (inf --> 0.412117).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.276125 \tValidation Loss: 0.418696\n",
      "Epoch: 3 \tTraining Loss: 0.252173 \tValidation Loss: 0.413387\n",
      "Epoch: 4 \tTraining Loss: 0.263621 \tValidation Loss: 0.400917\n",
      "Validation loss decreased (0.412117 --> 0.400917).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.261279 \tValidation Loss: 0.413148\n",
      "Epoch: 6 \tTraining Loss: 0.249301 \tValidation Loss: 0.424072\n",
      "Epoch: 7 \tTraining Loss: 0.238239 \tValidation Loss: 0.417327\n",
      "Epoch: 8 \tTraining Loss: 0.236918 \tValidation Loss: 0.412120\n",
      "Epoch: 9 \tTraining Loss: 0.246845 \tValidation Loss: 0.386809\n",
      "Validation loss decreased (0.400917 --> 0.386809).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.248971 \tValidation Loss: 0.433604\n",
      "Epoch: 11 \tTraining Loss: 0.236616 \tValidation Loss: 0.420678\n",
      "Epoch: 12 \tTraining Loss: 0.222927 \tValidation Loss: 0.444847\n",
      "Epoch: 13 \tTraining Loss: 0.226512 \tValidation Loss: 0.399440\n",
      "Epoch: 14 \tTraining Loss: 0.215506 \tValidation Loss: 0.419127\n",
      "Epoch: 15 \tTraining Loss: 0.224402 \tValidation Loss: 0.423132\n",
      "Epoch: 16 \tTraining Loss: 0.213566 \tValidation Loss: 0.449935\n",
      "Epoch: 17 \tTraining Loss: 0.212270 \tValidation Loss: 0.452434\n",
      "Epoch: 18 \tTraining Loss: 0.217553 \tValidation Loss: 0.470572\n",
      "Epoch: 19 \tTraining Loss: 0.213411 \tValidation Loss: 0.455396\n",
      "Epoch: 20 \tTraining Loss: 0.203659 \tValidation Loss: 0.449032\n",
      "Epoch: 21 \tTraining Loss: 0.191420 \tValidation Loss: 0.445078\n",
      "Epoch: 22 \tTraining Loss: 0.186591 \tValidation Loss: 0.544947\n",
      "Epoch: 23 \tTraining Loss: 0.212156 \tValidation Loss: 0.485471\n",
      "Epoch: 24 \tTraining Loss: 0.201741 \tValidation Loss: 0.466028\n",
      "Epoch: 25 \tTraining Loss: 0.181088 \tValidation Loss: 0.421002\n",
      "Epoch: 26 \tTraining Loss: 0.192435 \tValidation Loss: 0.570220\n",
      "Epoch: 27 \tTraining Loss: 0.190821 \tValidation Loss: 0.474520\n",
      "Epoch: 28 \tTraining Loss: 0.179359 \tValidation Loss: 0.434876\n",
      "Epoch: 29 \tTraining Loss: 0.191265 \tValidation Loss: 0.475888\n",
      "Epoch: 30 \tTraining Loss: 0.186401 \tValidation Loss: 0.439600\n",
      "Epoch: 31 \tTraining Loss: 0.173937 \tValidation Loss: 0.475611\n",
      "Epoch: 32 \tTraining Loss: 0.174867 \tValidation Loss: 0.496539\n",
      "Epoch: 33 \tTraining Loss: 0.179057 \tValidation Loss: 0.432985\n",
      "Epoch: 34 \tTraining Loss: 0.181459 \tValidation Loss: 0.477044\n",
      "Epoch: 35 \tTraining Loss: 0.164216 \tValidation Loss: 0.491912\n",
      "Epoch: 36 \tTraining Loss: 0.176097 \tValidation Loss: 0.446931\n",
      "Epoch: 37 \tTraining Loss: 0.172471 \tValidation Loss: 0.465834\n",
      "Epoch: 38 \tTraining Loss: 0.165659 \tValidation Loss: 0.461837\n",
      "Epoch: 39 \tTraining Loss: 0.176835 \tValidation Loss: 0.483471\n",
      "Epoch: 40 \tTraining Loss: 0.160782 \tValidation Loss: 0.458555\n",
      "Epoch: 41 \tTraining Loss: 0.166784 \tValidation Loss: 0.507786\n",
      "Epoch: 42 \tTraining Loss: 0.175571 \tValidation Loss: 0.461363\n",
      "Epoch: 43 \tTraining Loss: 0.149678 \tValidation Loss: 0.501158\n",
      "Epoch: 44 \tTraining Loss: 0.160430 \tValidation Loss: 0.503910\n",
      "Epoch: 45 \tTraining Loss: 0.164302 \tValidation Loss: 0.467038\n",
      "Epoch: 46 \tTraining Loss: 0.155321 \tValidation Loss: 0.469410\n",
      "Epoch: 47 \tTraining Loss: 0.158705 \tValidation Loss: 0.468544\n",
      "Epoch: 48 \tTraining Loss: 0.187406 \tValidation Loss: 0.406825\n",
      "Epoch: 49 \tTraining Loss: 0.156680 \tValidation Loss: 0.484557\n",
      "Epoch: 50 \tTraining Loss: 0.147009 \tValidation Loss: 0.503961\n",
      "Epoch: 51 \tTraining Loss: 0.156315 \tValidation Loss: 0.479217\n",
      "Epoch: 52 \tTraining Loss: 0.147237 \tValidation Loss: 0.483671\n",
      "Epoch: 53 \tTraining Loss: 0.153980 \tValidation Loss: 0.429747\n",
      "Epoch: 54 \tTraining Loss: 0.140017 \tValidation Loss: 0.449039\n",
      "Epoch: 55 \tTraining Loss: 0.145501 \tValidation Loss: 0.508325\n",
      "Epoch: 56 \tTraining Loss: 0.165320 \tValidation Loss: 0.456215\n",
      "Epoch: 57 \tTraining Loss: 0.156018 \tValidation Loss: 0.416866\n",
      "Epoch: 58 \tTraining Loss: 0.139520 \tValidation Loss: 0.430218\n",
      "Epoch: 59 \tTraining Loss: 0.145966 \tValidation Loss: 0.482668\n",
      "Epoch: 60 \tTraining Loss: 0.157093 \tValidation Loss: 0.500956\n",
      "Epoch: 61 \tTraining Loss: 0.133091 \tValidation Loss: 0.465235\n",
      "Epoch: 62 \tTraining Loss: 0.142922 \tValidation Loss: 0.470726\n",
      "Epoch: 63 \tTraining Loss: 0.151259 \tValidation Loss: 0.426574\n",
      "Epoch: 64 \tTraining Loss: 0.141099 \tValidation Loss: 0.469918\n",
      "Epoch: 65 \tTraining Loss: 0.133592 \tValidation Loss: 0.510518\n",
      "Epoch: 66 \tTraining Loss: 0.132235 \tValidation Loss: 0.505450\n",
      "Epoch: 67 \tTraining Loss: 0.143836 \tValidation Loss: 0.420382\n",
      "Epoch: 68 \tTraining Loss: 0.138821 \tValidation Loss: 0.563175\n",
      "Epoch: 69 \tTraining Loss: 0.137704 \tValidation Loss: 0.492782\n",
      "Epoch: 70 \tTraining Loss: 0.131522 \tValidation Loss: 0.456813\n",
      "Epoch: 71 \tTraining Loss: 0.120824 \tValidation Loss: 0.535447\n",
      "Epoch: 72 \tTraining Loss: 0.128309 \tValidation Loss: 0.537508\n",
      "Epoch: 73 \tTraining Loss: 0.132420 \tValidation Loss: 0.519304\n",
      "Epoch: 74 \tTraining Loss: 0.119497 \tValidation Loss: 0.484711\n",
      "Epoch: 75 \tTraining Loss: 0.129272 \tValidation Loss: 0.503382\n",
      "Epoch: 76 \tTraining Loss: 0.129848 \tValidation Loss: 0.545738\n",
      "Epoch: 77 \tTraining Loss: 0.124196 \tValidation Loss: 0.526055\n",
      "Epoch: 78 \tTraining Loss: 0.127560 \tValidation Loss: 0.547180\n",
      "Epoch: 79 \tTraining Loss: 0.139767 \tValidation Loss: 0.525763\n",
      "Epoch: 80 \tTraining Loss: 0.121789 \tValidation Loss: 0.554047\n",
      "Epoch: 81 \tTraining Loss: 0.128372 \tValidation Loss: 0.621316\n",
      "Epoch: 82 \tTraining Loss: 0.125586 \tValidation Loss: 0.460845\n",
      "Epoch: 83 \tTraining Loss: 0.121497 \tValidation Loss: 0.478945\n",
      "Epoch: 84 \tTraining Loss: 0.122061 \tValidation Loss: 0.521504\n",
      "Epoch: 85 \tTraining Loss: 0.130495 \tValidation Loss: 0.509328\n",
      "Epoch: 86 \tTraining Loss: 0.134519 \tValidation Loss: 0.427455\n",
      "Epoch: 87 \tTraining Loss: 0.119935 \tValidation Loss: 0.557365\n",
      "Epoch: 88 \tTraining Loss: 0.111079 \tValidation Loss: 0.522358\n",
      "Epoch: 89 \tTraining Loss: 0.131421 \tValidation Loss: 0.488758\n",
      "Epoch: 90 \tTraining Loss: 0.107142 \tValidation Loss: 0.547425\n",
      "Epoch: 91 \tTraining Loss: 0.110117 \tValidation Loss: 0.539031\n",
      "Epoch: 92 \tTraining Loss: 0.124526 \tValidation Loss: 0.501448\n",
      "Epoch: 93 \tTraining Loss: 0.108017 \tValidation Loss: 0.495191\n",
      "Epoch: 94 \tTraining Loss: 0.113695 \tValidation Loss: 0.509863\n",
      "Epoch: 95 \tTraining Loss: 0.127778 \tValidation Loss: 0.470377\n",
      "Epoch: 96 \tTraining Loss: 0.106796 \tValidation Loss: 0.523399\n",
      "Epoch: 97 \tTraining Loss: 0.122814 \tValidation Loss: 0.537437\n",
      "Epoch: 98 \tTraining Loss: 0.098191 \tValidation Loss: 0.517167\n",
      "Epoch: 99 \tTraining Loss: 0.125689 \tValidation Loss: 0.520956\n",
      "Epoch: 100 \tTraining Loss: 0.112810 \tValidation Loss: 0.602691\n",
      "Training time: 2 min 25 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train() \n",
    "    for features, labels in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*features.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for features, labels in validloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*features.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_Spinetrack_data2.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "# output running time\n",
    "running_time = time.time() - start_time\n",
    "sec = running_time % 60\n",
    "miniute = running_time / 60\n",
    "print(\"Training time: {} min {} sec\".format(int(miniute), int(sec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeCNN(nn.Module):\n",
    "    def __init__(self, channel, label_size):\n",
    "        super(TimeCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(channel, 32, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        \n",
    "        # bn1 = BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 64, bias=True)\n",
    "        self.fc2 = nn.Linear(64, label_size, bias=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "#         print(\"forward shape 1: \", x.shape)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(batch_size, -1)\n",
    "#         print(\"forward shape 2: \", x.shape)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add 2nd hidden layer, with relu activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = TimeCNN(channel, label_size)\n",
    "model = model.double()\n",
    "if train_on_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_Spinetrack_data2.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-267-6d446f65236a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# load on cpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_Spinetrack_data2.pt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    380\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_Spinetrack_data2.pt'"
     ]
    }
   ],
   "source": [
    "# load on gpu\n",
    "# model.load_state_dict(torch.load('model_Spinetrack_3.pt'))\n",
    "\n",
    "# load on cpu\n",
    "model.load_state_dict(torch.load('model_Spinetrack_data2.pt', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Test your trained model on previously unseen data! A \"good\" result will be a result that gets more than 70% accuracy on these test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release all the GPU memory cache that can be freed\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy and each class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.352170\n",
      "\n",
      "Test Accuracy of Carrying_5: 88% (233/262)\n",
      "Test Accuracy of Carrying_10: 89% (262/293)\n",
      "Test Accuracy of Carrying_20: 78% (192/245)\n",
      "\n",
      "Test Accuracy (Overall): 85% (687/800)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(len(index_dict)))\n",
    "class_total = list(0. for i in range(len(index_dict)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "model.eval()\n",
    "torch.no_grad()\n",
    "# iterate over test data\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(features)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, labels)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*features.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(testloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(index_dict)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            list(index_dict.keys())[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (list(index_dict.keys())[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy (different calculation method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off gradients for validation, saves memory and computations\n",
    "torch.no_grad()\n",
    "accuracy = 0\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    loss = model(features)\n",
    "    test_loss += criterion(loss, labels)\n",
    "\n",
    "#     ps = torch.exp(loss)\n",
    "    top_p, top_class = loss.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out pulling_OneH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
