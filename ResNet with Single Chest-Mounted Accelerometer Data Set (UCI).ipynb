{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from random import shuffle, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1704"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process original dataset, create windows (60 samples(rows), about 1 second)\n",
    "first_df = pd.read_csv(\"./Activity Recognition from Single Chest-Mounted Accelerometer/3.csv\")\n",
    "first_df.columns = [\"time\", \"x_acce\", \"y_acce\", \"z_acce\", \"label\"]\n",
    "first_df = first_df[59:]\n",
    "data = []\n",
    "# first_df\n",
    "window = 1\n",
    "while window*60 < len(first_df.index):\n",
    "    data60 = first_df[(window - 1)*60:window*60]\n",
    "    data.append(data60.values)\n",
    "    window += 1\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data contains different activities!\n",
      "data contains different activities!\n",
      "data contains different activities!\n",
      "data contains different activities!\n",
      "data contains different activities!\n",
      "data contains different activities!\n",
      "data contains different activities!\n",
      "data contains different activities!\n"
     ]
    }
   ],
   "source": [
    "# delete window if multiple activities presents\n",
    "cleaned_data = []\n",
    "for i in data:\n",
    "    previous_label = -1\n",
    "    for j in i:\n",
    "        current_label = j[4]\n",
    "        if (previous_label != -1) and (current_label != previous_label):\n",
    "            print(\"data contains different activities!\")\n",
    "            break\n",
    "        else:\n",
    "            previous_label = current_label\n",
    "    else:\n",
    "        cleaned_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1696"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "seed(101)\n",
    "shuffle(cleaned_data)\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label from each window\n",
    "labels = []\n",
    "for i in cleaned_data:\n",
    "    label = i[0][4]\n",
    "    labels.append(label)\n",
    "labels = np.array(labels)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each window\n",
    "features = []\n",
    "for i in cleaned_data:\n",
    "    new = np.delete(i, 4, 1)\n",
    "    features.append(new)\n",
    "features = np.array(features)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the features and labels, and convert it to a csv\n",
    "k = list(zip(features, labels))\n",
    "activity_data = pd.DataFrame(k)\n",
    "#activity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to test and train\n",
    "activity_data.columns = [\"features\", \"labels\"]\n",
    "activity_data_train = activity_data[:int(len(activity_data)*0.7)]\n",
    "activity_data_test = activity_data[int(len(activity_data)*0.7):]\n",
    "activity_data_train.to_csv(\"./activity_data_train.csv\", encoding='utf-8-sig')\n",
    "activity_data_train.to_csv(\"./activity_data_test.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset in pytorch\n",
    "class DatasetHAR(Dataset):\n",
    "    \n",
    "    def __init__(self, file, transform=None):\n",
    "        #self.data = pd.read_csv(file_path)\n",
    "        self.data = file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        features = torch.tensor(self.data[\"features\"].iloc[index])\n",
    "        features = features.view(60, 1, 4)\n",
    "        labels = torch.tensor(self.data[\"labels\"].iloc[index], dtype=torch.long)\n",
    "        #print(labels.type())\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training and testing dataset in csv\n",
    "# train_dataset = DatasetHAR(\"./activity_data_train.csv\")\n",
    "# test_dataset = DatasetHAR(\"./activity_data_test.csv\")\n",
    "train_dataset = DatasetHAR(activity_data_train)\n",
    "test_dataset = DatasetHAR(activity_data_test)\n",
    "feature, label = train_dataset.__getitem__(0)\n",
    "#feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainloader = DataLoader(train_dataset, batch_size=64)\n",
    "testloader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(60, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "model.conv1 = torch.nn.Conv2d(60, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(512, 8, bias=True)\n",
    "model = model.double()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30..  Training Loss: 0.859..  Test Loss: 0.371..  Test Accuracy: 0.866\n",
      "Epoch: 2/30..  Training Loss: 0.461..  Test Loss: 0.373..  Test Accuracy: 0.873\n",
      "Epoch: 3/30..  Training Loss: 0.421..  Test Loss: 0.386..  Test Accuracy: 0.847\n",
      "Epoch: 4/30..  Training Loss: 0.400..  Test Loss: 0.312..  Test Accuracy: 0.882\n",
      "Epoch: 5/30..  Training Loss: 0.371..  Test Loss: 0.340..  Test Accuracy: 0.869\n",
      "Epoch: 6/30..  Training Loss: 0.350..  Test Loss: 0.332..  Test Accuracy: 0.861\n",
      "Epoch: 7/30..  Training Loss: 0.361..  Test Loss: 0.356..  Test Accuracy: 0.853\n",
      "Epoch: 8/30..  Training Loss: 0.344..  Test Loss: 0.278..  Test Accuracy: 0.898\n",
      "Epoch: 9/30..  Training Loss: 0.335..  Test Loss: 0.331..  Test Accuracy: 0.855\n",
      "Epoch: 10/30..  Training Loss: 0.323..  Test Loss: 0.306..  Test Accuracy: 0.869\n",
      "Epoch: 11/30..  Training Loss: 0.330..  Test Loss: 0.279..  Test Accuracy: 0.894\n",
      "Epoch: 12/30..  Training Loss: 0.328..  Test Loss: 0.303..  Test Accuracy: 0.865\n",
      "Epoch: 13/30..  Training Loss: 0.293..  Test Loss: 0.296..  Test Accuracy: 0.886\n",
      "Epoch: 14/30..  Training Loss: 0.317..  Test Loss: 0.279..  Test Accuracy: 0.880\n",
      "Epoch: 15/30..  Training Loss: 0.311..  Test Loss: 0.308..  Test Accuracy: 0.882\n",
      "Epoch: 16/30..  Training Loss: 0.309..  Test Loss: 0.268..  Test Accuracy: 0.896\n",
      "Epoch: 17/30..  Training Loss: 0.338..  Test Loss: 0.356..  Test Accuracy: 0.859\n",
      "Epoch: 18/30..  Training Loss: 0.345..  Test Loss: 0.290..  Test Accuracy: 0.888\n",
      "Epoch: 19/30..  Training Loss: 0.339..  Test Loss: 0.347..  Test Accuracy: 0.859\n",
      "Epoch: 20/30..  Training Loss: 0.325..  Test Loss: 0.302..  Test Accuracy: 0.888\n",
      "Epoch: 21/30..  Training Loss: 0.320..  Test Loss: 0.292..  Test Accuracy: 0.888\n",
      "Epoch: 22/30..  Training Loss: 0.277..  Test Loss: 0.304..  Test Accuracy: 0.886\n",
      "Epoch: 23/30..  Training Loss: 0.262..  Test Loss: 0.364..  Test Accuracy: 0.841\n",
      "Epoch: 24/30..  Training Loss: 0.273..  Test Loss: 0.274..  Test Accuracy: 0.906\n",
      "Epoch: 25/30..  Training Loss: 0.310..  Test Loss: 0.279..  Test Accuracy: 0.882\n",
      "Epoch: 26/30..  Training Loss: 0.261..  Test Loss: 0.281..  Test Accuracy: 0.890\n",
      "Epoch: 27/30..  Training Loss: 0.256..  Test Loss: 0.277..  Test Accuracy: 0.888\n",
      "Epoch: 28/30..  Training Loss: 0.247..  Test Loss: 0.254..  Test Accuracy: 0.910\n",
      "Epoch: 29/30..  Training Loss: 0.286..  Test Loss: 0.263..  Test Accuracy: 0.896\n",
      "Epoch: 30/30..  Training Loss: 0.293..  Test Loss: 0.284..  Test Accuracy: 0.904\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for feature, label in trainloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model(feature)\n",
    "        loss = criterion(log_ps, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        #print(running_loss)\n",
    "        \n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for feature, label in testloader:\n",
    "                log_ps = model(feature)\n",
    "                test_loss += criterion(log_ps, label)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == label.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(trainloader))\n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The rest is using a simple linear NN to compare with the ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset in pytorch\n",
    "class DatasetH(Dataset):\n",
    "    \n",
    "    def __init__(self, file, transform=None):\n",
    "        #self.data = pd.read_csv(file_path)\n",
    "        self.data = file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        features = torch.tensor(self.data[\"features\"].iloc[index])\n",
    "        labels = torch.tensor(self.data[\"labels\"].iloc[index], dtype=torch.long)\n",
    "        #print(labels.type())\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1, 4])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset2 = DatasetH(activity_data_train)\n",
    "test_dataset2 = DatasetH(activity_data_test)\n",
    "feature, label = train_dataset.__getitem__(0)\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainloader2 = DataLoader(train_dataset2, batch_size=64)\n",
    "testloader2 = DataLoader(test_dataset2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(nn.Linear(240, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 8))\n",
    "model2 = model2.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30..  Training Loss: 2183.701..  Test Loss: 457.619..  Test Accuracy: 0.240\n",
      "Epoch: 2/30..  Training Loss: 248.914..  Test Loss: 164.841..  Test Accuracy: 0.240\n",
      "Epoch: 3/30..  Training Loss: 112.447..  Test Loss: 102.499..  Test Accuracy: 0.240\n",
      "Epoch: 4/30..  Training Loss: 69.074..  Test Loss: 81.733..  Test Accuracy: 0.240\n",
      "Epoch: 5/30..  Training Loss: 53.706..  Test Loss: 69.147..  Test Accuracy: 0.265\n",
      "Epoch: 6/30..  Training Loss: 29.002..  Test Loss: 91.389..  Test Accuracy: 0.309\n",
      "Epoch: 7/30..  Training Loss: 49.801..  Test Loss: 36.282..  Test Accuracy: 0.455\n",
      "Epoch: 8/30..  Training Loss: 22.436..  Test Loss: 13.556..  Test Accuracy: 0.547\n",
      "Epoch: 9/30..  Training Loss: 9.847..  Test Loss: 11.244..  Test Accuracy: 0.552\n",
      "Epoch: 10/30..  Training Loss: 6.211..  Test Loss: 7.810..  Test Accuracy: 0.643\n",
      "Epoch: 11/30..  Training Loss: 4.657..  Test Loss: 9.455..  Test Accuracy: 0.702\n",
      "Epoch: 12/30..  Training Loss: 5.076..  Test Loss: 3.149..  Test Accuracy: 0.762\n",
      "Epoch: 13/30..  Training Loss: 2.829..  Test Loss: 5.940..  Test Accuracy: 0.718\n",
      "Epoch: 14/30..  Training Loss: 3.801..  Test Loss: 4.003..  Test Accuracy: 0.801\n",
      "Epoch: 15/30..  Training Loss: 3.382..  Test Loss: 3.887..  Test Accuracy: 0.771\n",
      "Epoch: 16/30..  Training Loss: 4.439..  Test Loss: 5.642..  Test Accuracy: 0.715\n",
      "Epoch: 17/30..  Training Loss: 5.422..  Test Loss: 6.331..  Test Accuracy: 0.651\n",
      "Epoch: 18/30..  Training Loss: 4.596..  Test Loss: 7.424..  Test Accuracy: 0.745\n",
      "Epoch: 19/30..  Training Loss: 3.163..  Test Loss: 3.788..  Test Accuracy: 0.705\n",
      "Epoch: 20/30..  Training Loss: 2.830..  Test Loss: 4.321..  Test Accuracy: 0.725\n",
      "Epoch: 21/30..  Training Loss: 3.343..  Test Loss: 2.771..  Test Accuracy: 0.784\n",
      "Epoch: 22/30..  Training Loss: 2.719..  Test Loss: 4.222..  Test Accuracy: 0.777\n",
      "Epoch: 23/30..  Training Loss: 3.367..  Test Loss: 4.619..  Test Accuracy: 0.748\n",
      "Epoch: 24/30..  Training Loss: 2.155..  Test Loss: 2.752..  Test Accuracy: 0.724\n",
      "Epoch: 25/30..  Training Loss: 1.776..  Test Loss: 2.461..  Test Accuracy: 0.702\n",
      "Epoch: 26/30..  Training Loss: 1.474..  Test Loss: 2.472..  Test Accuracy: 0.736\n",
      "Epoch: 27/30..  Training Loss: 1.367..  Test Loss: 2.301..  Test Accuracy: 0.734\n",
      "Epoch: 28/30..  Training Loss: 1.759..  Test Loss: 6.830..  Test Accuracy: 0.570\n",
      "Epoch: 29/30..  Training Loss: 2.898..  Test Loss: 3.132..  Test Accuracy: 0.688\n",
      "Epoch: 30/30..  Training Loss: 1.929..  Test Loss: 1.556..  Test Accuracy: 0.789\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for feature, label in trainloader2:\n",
    "        feature = feature.view(feature.shape[0], -1)\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        log_ps = model2(feature)\n",
    "        loss = criterion2(log_ps, label)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        #print(running_loss)\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            for feature, label in testloader2:\n",
    "                feature = feature.view(feature.shape[0], -1)\n",
    "                #print(feature.shape)\n",
    "                #print(feature.shape)\n",
    "                #print(label.shape)\n",
    "                log_ps = model2(feature)\n",
    "                test_loss += criterion2(log_ps, label)\n",
    "\n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == label.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "        train_losses.append(running_loss/len(trainloader2))\n",
    "        test_losses.append(test_loss/len(testloader2))\n",
    "\n",
    "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss/len(trainloader)),\n",
    "              \"Test Loss: {:.3f}.. \".format(test_loss/len(testloader)),\n",
    "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
