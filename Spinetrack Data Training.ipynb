{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and reformating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\\\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from random import shuffle, seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter\n",
    "window_size: The number of timesteps in one window (e.g. how many rows in one window).\n",
    "\n",
    "channel: The number of features in one window. Similar to image channels (RGB).\n",
    "\n",
    "batch_size: The numebr of windows in one batch.\n",
    "\n",
    "learning_rate: How fast the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 180\n",
    "channel = 1\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Go to \n",
    "<a href=#bookmark> Run all cell above</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {'Pulling_OneH': 0, 'Overhead': 1, 'Pulling': 2, 'Sitting': 3, 'Lifting': 4, \n",
    "              'Crawling': 5, 'Standing': 6, 'Carrying': 7, 'Walking': 8, 'Pushing': 9, \n",
    "              'Reaching': 10, 'Static_Stoop': 11, 'Kneeling': 12, 'Lifting_OneH': 13, 'Crouching': 14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pulling_OneH': 0, 'Overhead': 1, 'Pulling': 2, 'Sitting': 3, 'Lifting': 4, 'Crawling': 5, 'Standing': 6, 'Carrying': 7, 'Walking': 8, 'Pushing': 9, 'Reaching': 10, 'Static_Stoop': 11, 'Kneeling': 12, 'Lifting_OneH': 13, 'Crouching': 14}\n"
     ]
    }
   ],
   "source": [
    "print(index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TimeSec</th>\n",
       "      <th>Sensor</th>\n",
       "      <th>Quatx</th>\n",
       "      <th>Quaty</th>\n",
       "      <th>Quatz</th>\n",
       "      <th>Quat0</th>\n",
       "      <th>Heading</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>Roll</th>\n",
       "      <th>LinAccx</th>\n",
       "      <th>LinAccy</th>\n",
       "      <th>LinAccz</th>\n",
       "      <th>Vbat</th>\n",
       "      <th>Accx</th>\n",
       "      <th>Accy</th>\n",
       "      <th>Accz</th>\n",
       "      <th>Gyrox</th>\n",
       "      <th>Gyroy</th>\n",
       "      <th>Gyroz</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0362</td>\n",
       "      <td>2</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>306.93</td>\n",
       "      <td>25.57</td>\n",
       "      <td>-16.37</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-19.84</td>\n",
       "      <td>-11.14</td>\n",
       "      <td>16.33</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0377</td>\n",
       "      <td>3</td>\n",
       "      <td>0.625</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.71</td>\n",
       "      <td>113.60</td>\n",
       "      <td>38.21</td>\n",
       "      <td>7.42</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-37.84</td>\n",
       "      <td>-4.27</td>\n",
       "      <td>53.71</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.77</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0961</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>192.97</td>\n",
       "      <td>-9.34</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>36.01</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0978</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.978</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>38.13</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.33</td>\n",
       "      <td>3.81</td>\n",
       "      <td>-18.92</td>\n",
       "      <td>-64.70</td>\n",
       "      <td>118.26</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.61</td>\n",
       "      <td>116.18</td>\n",
       "      <td>-5.60</td>\n",
       "      <td>82.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3.87</td>\n",
       "      <td>5.19</td>\n",
       "      <td>45.62</td>\n",
       "      <td>-2.90</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.35</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1032</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.99</td>\n",
       "      <td>206.37</td>\n",
       "      <td>-9.81</td>\n",
       "      <td>-2.63</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>18.31</td>\n",
       "      <td>61.04</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.84</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1069</td>\n",
       "      <td>4</td>\n",
       "      <td>0.540</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>300.68</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>-90.39</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>3.79</td>\n",
       "      <td>8.85</td>\n",
       "      <td>-50.35</td>\n",
       "      <td>-32.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-1.09</td>\n",
       "      <td>0.06</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1086</td>\n",
       "      <td>7</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.23</td>\n",
       "      <td>42.78</td>\n",
       "      <td>15.05</td>\n",
       "      <td>16.65</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-14.34</td>\n",
       "      <td>-24.11</td>\n",
       "      <td>85.60</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.82</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1283</td>\n",
       "      <td>2</td>\n",
       "      <td>0.838</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>310.55</td>\n",
       "      <td>24.85</td>\n",
       "      <td>-16.97</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-31.13</td>\n",
       "      <td>-19.23</td>\n",
       "      <td>33.87</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>0.77</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1301</td>\n",
       "      <td>3</td>\n",
       "      <td>0.586</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.74</td>\n",
       "      <td>119.48</td>\n",
       "      <td>37.14</td>\n",
       "      <td>5.69</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-56.00</td>\n",
       "      <td>1.37</td>\n",
       "      <td>29.91</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.68</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1662</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>199.05</td>\n",
       "      <td>-10.01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.92</td>\n",
       "      <td>10.53</td>\n",
       "      <td>-7.32</td>\n",
       "      <td>64.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1678</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>43.15</td>\n",
       "      <td>-3.44</td>\n",
       "      <td>-1.57</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.81</td>\n",
       "      <td>-13.58</td>\n",
       "      <td>-64.85</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1.05</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1982</td>\n",
       "      <td>5</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.64</td>\n",
       "      <td>121.61</td>\n",
       "      <td>-7.94</td>\n",
       "      <td>81.38</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>7.48</td>\n",
       "      <td>67.75</td>\n",
       "      <td>37.99</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>211.95</td>\n",
       "      <td>-9.58</td>\n",
       "      <td>-2.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>26.40</td>\n",
       "      <td>3.81</td>\n",
       "      <td>61.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2036</td>\n",
       "      <td>4</td>\n",
       "      <td>0.558</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>306.61</td>\n",
       "      <td>-7.64</td>\n",
       "      <td>-90.53</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.79</td>\n",
       "      <td>16.17</td>\n",
       "      <td>-65.77</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-1.04</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.2050</td>\n",
       "      <td>7</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>50.00</td>\n",
       "      <td>12.38</td>\n",
       "      <td>15.05</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-39.06</td>\n",
       "      <td>18.62</td>\n",
       "      <td>69.12</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.95</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.2443</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.99</td>\n",
       "      <td>205.54</td>\n",
       "      <td>-10.56</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.92</td>\n",
       "      <td>14.04</td>\n",
       "      <td>7.93</td>\n",
       "      <td>55.54</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.2459</td>\n",
       "      <td>3</td>\n",
       "      <td>0.535</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.78</td>\n",
       "      <td>126.80</td>\n",
       "      <td>36.64</td>\n",
       "      <td>4.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-59.51</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>93.69</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.79</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2642</td>\n",
       "      <td>2</td>\n",
       "      <td>0.861</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>316.15</td>\n",
       "      <td>24.21</td>\n",
       "      <td>-17.51</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-36.01</td>\n",
       "      <td>-13.12</td>\n",
       "      <td>54.47</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.87</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.2659</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.955</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>47.90</td>\n",
       "      <td>-6.24</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.81</td>\n",
       "      <td>21.36</td>\n",
       "      <td>-20.60</td>\n",
       "      <td>82.86</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.02</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.3011</td>\n",
       "      <td>5</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.68</td>\n",
       "      <td>128.82</td>\n",
       "      <td>-11.79</td>\n",
       "      <td>80.05</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>3.87</td>\n",
       "      <td>17.85</td>\n",
       "      <td>71.72</td>\n",
       "      <td>24.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.38</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.3028</td>\n",
       "      <td>1</td>\n",
       "      <td>0.228</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>220.57</td>\n",
       "      <td>-8.91</td>\n",
       "      <td>-1.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>3.91</td>\n",
       "      <td>19.84</td>\n",
       "      <td>5.95</td>\n",
       "      <td>92.77</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.94</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.3065</td>\n",
       "      <td>4</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>315.36</td>\n",
       "      <td>-6.74</td>\n",
       "      <td>-90.54</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>3.79</td>\n",
       "      <td>7.02</td>\n",
       "      <td>-74.31</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.3080</td>\n",
       "      <td>7</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.38</td>\n",
       "      <td>60.57</td>\n",
       "      <td>12.96</td>\n",
       "      <td>14.21</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-22.28</td>\n",
       "      <td>32.50</td>\n",
       "      <td>104.06</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.01</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.3251</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.98</td>\n",
       "      <td>212.51</td>\n",
       "      <td>-10.29</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.92</td>\n",
       "      <td>13.12</td>\n",
       "      <td>9.77</td>\n",
       "      <td>85.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.3266</td>\n",
       "      <td>3</td>\n",
       "      <td>0.461</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.83</td>\n",
       "      <td>137.50</td>\n",
       "      <td>36.95</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.80</td>\n",
       "      <td>-35.86</td>\n",
       "      <td>18.31</td>\n",
       "      <td>78.58</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.3993</td>\n",
       "      <td>2</td>\n",
       "      <td>0.891</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>324.52</td>\n",
       "      <td>24.14</td>\n",
       "      <td>-16.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-21.97</td>\n",
       "      <td>-21.51</td>\n",
       "      <td>90.33</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.4009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.301</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>229.17</td>\n",
       "      <td>-10.88</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.91</td>\n",
       "      <td>19.99</td>\n",
       "      <td>-8.39</td>\n",
       "      <td>61.65</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.93</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.4048</td>\n",
       "      <td>5</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.70</td>\n",
       "      <td>136.72</td>\n",
       "      <td>-13.27</td>\n",
       "      <td>79.93</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.87</td>\n",
       "      <td>17.70</td>\n",
       "      <td>78.58</td>\n",
       "      <td>24.87</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.21</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.4065</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>52.68</td>\n",
       "      <td>-9.29</td>\n",
       "      <td>-1.28</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.81</td>\n",
       "      <td>12.66</td>\n",
       "      <td>-25.63</td>\n",
       "      <td>51.27</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.90</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986061</th>\n",
       "      <td>54.2570</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.502</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>133.41</td>\n",
       "      <td>3.77</td>\n",
       "      <td>-6.55</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>3.74</td>\n",
       "      <td>35.71</td>\n",
       "      <td>-16.94</td>\n",
       "      <td>-22.74</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986062</th>\n",
       "      <td>54.2600</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>14.18</td>\n",
       "      <td>-19.14</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.88</td>\n",
       "      <td>-12.97</td>\n",
       "      <td>-44.71</td>\n",
       "      <td>21.82</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986063</th>\n",
       "      <td>54.2620</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986064</th>\n",
       "      <td>54.3000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>201.75</td>\n",
       "      <td>7.13</td>\n",
       "      <td>22.33</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>49.44</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>-109.86</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.39</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986065</th>\n",
       "      <td>54.3010</td>\n",
       "      <td>5</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>262.61</td>\n",
       "      <td>12.97</td>\n",
       "      <td>47.20</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>3.82</td>\n",
       "      <td>233.15</td>\n",
       "      <td>-25.94</td>\n",
       "      <td>-69.58</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.07</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986066</th>\n",
       "      <td>54.3500</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.950</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>28.27</td>\n",
       "      <td>-27.88</td>\n",
       "      <td>-22.85</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.77</td>\n",
       "      <td>24.41</td>\n",
       "      <td>1.07</td>\n",
       "      <td>-55.54</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986067</th>\n",
       "      <td>54.3510</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.977</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>29.15</td>\n",
       "      <td>-18.64</td>\n",
       "      <td>-8.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>3.83</td>\n",
       "      <td>22.13</td>\n",
       "      <td>17.24</td>\n",
       "      <td>29.60</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.76</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986068</th>\n",
       "      <td>54.3550</td>\n",
       "      <td>3</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>349.90</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>4.98</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.76</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>-84.53</td>\n",
       "      <td>-65.00</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986069</th>\n",
       "      <td>54.3570</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>126.18</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-5.53</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4.58</td>\n",
       "      <td>5.04</td>\n",
       "      <td>-91.09</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986070</th>\n",
       "      <td>54.4190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.982</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.01</td>\n",
       "      <td>16.87</td>\n",
       "      <td>-20.28</td>\n",
       "      <td>-7.19</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>3.88</td>\n",
       "      <td>9.46</td>\n",
       "      <td>22.58</td>\n",
       "      <td>24.26</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.11</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986071</th>\n",
       "      <td>54.4210</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986072</th>\n",
       "      <td>54.4250</td>\n",
       "      <td>7</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>201.15</td>\n",
       "      <td>-1.15</td>\n",
       "      <td>21.50</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-28.69</td>\n",
       "      <td>-94.60</td>\n",
       "      <td>11.44</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986073</th>\n",
       "      <td>54.4270</td>\n",
       "      <td>5</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>257.84</td>\n",
       "      <td>17.62</td>\n",
       "      <td>63.30</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.82</td>\n",
       "      <td>106.35</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>-69.12</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.67</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986074</th>\n",
       "      <td>54.4470</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.957</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>27.62</td>\n",
       "      <td>-27.18</td>\n",
       "      <td>-18.53</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>3.77</td>\n",
       "      <td>44.40</td>\n",
       "      <td>5.65</td>\n",
       "      <td>38.76</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.99</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986075</th>\n",
       "      <td>54.4480</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.975</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>32.37</td>\n",
       "      <td>-18.29</td>\n",
       "      <td>-6.35</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.03</td>\n",
       "      <td>3.83</td>\n",
       "      <td>12.05</td>\n",
       "      <td>-4.58</td>\n",
       "      <td>30.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1.03</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986076</th>\n",
       "      <td>54.4520</td>\n",
       "      <td>3</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>339.36</td>\n",
       "      <td>-9.22</td>\n",
       "      <td>6.64</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.76</td>\n",
       "      <td>35.71</td>\n",
       "      <td>-86.06</td>\n",
       "      <td>-163.88</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.19</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986077</th>\n",
       "      <td>54.4530</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>120.08</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-3.86</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.74</td>\n",
       "      <td>47.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3.97</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>1.08</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986078</th>\n",
       "      <td>54.4570</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17.70</td>\n",
       "      <td>-19.11</td>\n",
       "      <td>-6.88</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-5.95</td>\n",
       "      <td>3.66</td>\n",
       "      <td>-4.73</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.88</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986079</th>\n",
       "      <td>54.4590</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986080</th>\n",
       "      <td>54.5070</td>\n",
       "      <td>7</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>202.84</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>17.79</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-22.58</td>\n",
       "      <td>-64.24</td>\n",
       "      <td>83.92</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986081</th>\n",
       "      <td>54.5080</td>\n",
       "      <td>5</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-0.76</td>\n",
       "      <td>257.17</td>\n",
       "      <td>20.78</td>\n",
       "      <td>65.46</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.53</td>\n",
       "      <td>3.82</td>\n",
       "      <td>-65.46</td>\n",
       "      <td>5.49</td>\n",
       "      <td>14.65</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986082</th>\n",
       "      <td>54.5260</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.968</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>25.42</td>\n",
       "      <td>-24.87</td>\n",
       "      <td>-13.42</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>3.77</td>\n",
       "      <td>32.50</td>\n",
       "      <td>50.96</td>\n",
       "      <td>-88.20</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.87</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986083</th>\n",
       "      <td>54.5270</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.976</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>32.49</td>\n",
       "      <td>-16.52</td>\n",
       "      <td>-6.97</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-27.77</td>\n",
       "      <td>30.52</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986084</th>\n",
       "      <td>54.5590</td>\n",
       "      <td>3</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>324.36</td>\n",
       "      <td>-12.77</td>\n",
       "      <td>13.73</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>3.76</td>\n",
       "      <td>42.72</td>\n",
       "      <td>-35.25</td>\n",
       "      <td>-87.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986085</th>\n",
       "      <td>54.5610</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986086</th>\n",
       "      <td>54.5640</td>\n",
       "      <td>0</td>\n",
       "      <td>0.984</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>17.20</td>\n",
       "      <td>-19.03</td>\n",
       "      <td>-7.60</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>3.89</td>\n",
       "      <td>-10.68</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-11.75</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986087</th>\n",
       "      <td>54.5660</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.584</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>122.58</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>3.74</td>\n",
       "      <td>41.81</td>\n",
       "      <td>5.04</td>\n",
       "      <td>32.81</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986088</th>\n",
       "      <td>54.6100</td>\n",
       "      <td>7</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>210.16</td>\n",
       "      <td>-17.23</td>\n",
       "      <td>12.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-41.81</td>\n",
       "      <td>-44.40</td>\n",
       "      <td>69.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.98</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986089</th>\n",
       "      <td>54.6110</td>\n",
       "      <td>5</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>259.25</td>\n",
       "      <td>16.95</td>\n",
       "      <td>52.48</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.07</td>\n",
       "      <td>3.82</td>\n",
       "      <td>-240.02</td>\n",
       "      <td>-14.19</td>\n",
       "      <td>66.38</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.36</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986090</th>\n",
       "      <td>54.6620</td>\n",
       "      <td>3</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>315.13</td>\n",
       "      <td>-11.78</td>\n",
       "      <td>21.63</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>3.76</td>\n",
       "      <td>58.90</td>\n",
       "      <td>-7.48</td>\n",
       "      <td>-72.33</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.79</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986091 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TimeSec  Sensor  Quatx  Quaty  Quatz  Quat0  Heading  Pitch   Roll  \\\n",
       "0        0.0362       2  0.822   0.01   0.26  -0.51   306.93  25.57 -16.37   \n",
       "1        0.0377       3  0.625  -0.21   0.26   0.71   113.60  38.21   7.42   \n",
       "2        0.0961       0  0.008   0.08   0.02   1.00   192.97  -9.34   2.16   \n",
       "3        0.0978       6 -0.978   0.03  -0.00  -0.21    38.13   1.13  -2.73   \n",
       "4        0.1018       5  0.448   0.44   0.49   0.61   116.18  -5.60  82.09   \n",
       "5        0.1032       1  0.105  -0.09   0.01  -0.99   206.37  -9.81  -2.63   \n",
       "6        0.1069       4  0.540  -0.59   0.39  -0.45   300.68  -6.56 -90.39   \n",
       "7        0.1086       7  0.955   0.11   0.16   0.23    42.78  15.05  16.65   \n",
       "8        0.1283       2  0.838  -0.01   0.26  -0.48   310.55  24.85 -16.97   \n",
       "9        0.1301       3  0.586  -0.22   0.23   0.74   119.48  37.14   5.69   \n",
       "10       0.1662       0 -0.045   0.09   0.02   1.00   199.05 -10.01   1.58   \n",
       "11       0.1678       6 -0.968   0.01   0.03  -0.25    43.15  -3.44  -1.57   \n",
       "12       0.1982       5  0.411   0.43   0.49   0.64   121.61  -7.94  81.38   \n",
       "13       0.1998       1  0.154  -0.09   0.00  -0.98   211.95  -9.58  -2.03   \n",
       "14       0.2036       4  0.558  -0.62   0.35  -0.43   306.61  -7.64 -90.53   \n",
       "15       0.2050       7  0.942   0.09   0.14   0.29    50.00  12.38  15.05   \n",
       "16       0.2443       0 -0.101   0.09   0.03   0.99   205.54 -10.56   1.96   \n",
       "17       0.2459       3  0.535  -0.24   0.20   0.78   126.80  36.64   4.11   \n",
       "18       0.2642       2  0.861  -0.03   0.25  -0.44   316.15  24.21 -17.51   \n",
       "19       0.2659       6 -0.955  -0.00   0.06  -0.29    47.90  -6.24  -1.72   \n",
       "20       0.3011       5  0.355   0.41   0.50   0.68   128.82 -11.79  80.05   \n",
       "21       0.3028       1  0.228  -0.08  -0.01  -0.97   220.57  -8.91  -1.17   \n",
       "22       0.3065       4  0.592  -0.64   0.31  -0.38   315.36  -6.74 -90.54   \n",
       "23       0.3080       7  0.911   0.07   0.15   0.38    60.57  12.96  14.21   \n",
       "24       0.3251       0 -0.162   0.09   0.04   0.98   212.51 -10.29   2.50   \n",
       "25       0.3266       3  0.461  -0.26   0.19   0.83   137.50  36.95   5.13   \n",
       "26       0.3993       2  0.891  -0.04   0.25  -0.38   324.52  24.14 -16.14   \n",
       "27       0.4009       1  0.301  -0.09  -0.03  -0.95   229.17 -10.88   0.46   \n",
       "28       0.4048       5  0.300   0.38   0.52   0.70   136.72 -13.27  79.93   \n",
       "29       0.4065       6 -0.941  -0.02   0.08  -0.33    52.68  -9.29  -1.28   \n",
       "...         ...     ...    ...    ...    ...    ...      ...    ...    ...   \n",
       "986061  54.2570       4 -0.502   0.06   0.03  -0.86   133.41   3.77  -6.55   \n",
       "986062  54.2600       0  0.985  -0.05  -0.17  -0.01    14.18 -19.14  -5.33   \n",
       "986063  54.2620       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986064  54.3000       7  0.054   0.07  -0.19  -0.98   201.75   7.13  22.33   \n",
       "986065  54.3010       5  0.475   0.31  -0.27  -0.78   262.61  12.97  47.20   \n",
       "986066  54.3500       6 -0.950   0.16   0.26  -0.07    28.27 -27.88 -22.85   \n",
       "986067  54.3510       1 -0.977   0.05   0.17  -0.12    29.15 -18.64  -8.00   \n",
       "986068  54.3550       3  0.977   0.04  -0.03  -0.21   349.90  -2.25   4.98   \n",
       "986069  54.3570       4 -0.557   0.05   0.03  -0.83   126.18   2.60  -5.53   \n",
       "986070  54.4190       0  0.982  -0.06  -0.18   0.01    16.87 -20.28  -7.19   \n",
       "986071  54.4210       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986072  54.4250       7  0.063   0.00  -0.19  -0.98   201.15  -1.15  21.50   \n",
       "986073  54.4270       5  0.376   0.38  -0.37  -0.76   257.84  17.62  63.30   \n",
       "986074  54.4470       6 -0.957   0.13   0.25  -0.08    27.62 -27.18 -18.53   \n",
       "986075  54.4480       1 -0.975   0.03   0.17  -0.15    32.37 -18.29  -6.35   \n",
       "986076  54.4520       3  0.951   0.03  -0.09  -0.29   339.36  -9.22   6.64   \n",
       "986077  54.4530       4 -0.600   0.04   0.01  -0.80   120.08   2.60  -3.86   \n",
       "986078  54.4570       0  0.984  -0.05  -0.17   0.02    17.70 -19.11  -6.88   \n",
       "986079  54.4590       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986080  54.5070       7  0.089  -0.07  -0.16  -0.98   202.84  -9.75  17.79   \n",
       "986081  54.5080       5  0.350   0.41  -0.37  -0.76   257.17  20.78  65.46   \n",
       "986082  54.5260       6 -0.968   0.09   0.22  -0.07    25.42 -24.87 -13.42   \n",
       "986083  54.5270       1 -0.976   0.04   0.15  -0.15    32.49 -16.52  -6.97   \n",
       "986084  54.5590       3  0.901   0.06  -0.15  -0.40   324.36 -12.77  13.73   \n",
       "986085  54.5610       2  1.000   0.00   0.00   0.00     0.00   0.00   0.00   \n",
       "986086  54.5640       0  0.984  -0.06  -0.17   0.02    17.20 -19.03  -7.60   \n",
       "986087  54.5660       4 -0.584   0.01  -0.03  -0.81   122.58   2.78   1.97   \n",
       "986088  54.6100       7  0.153  -0.13  -0.12  -0.97   210.16 -17.23  12.17   \n",
       "986089  54.6110       5  0.423   0.35  -0.30  -0.78   259.25  16.95  52.48   \n",
       "986090  54.6620       3  0.860   0.11  -0.18  -0.46   315.13 -11.78  21.63   \n",
       "\n",
       "        LinAccx  LinAccy  LinAccz  Vbat    Accx   Accy    Accz  Gyrox  Gyroy  \\\n",
       "0         -0.03    -0.03    -0.02  3.94  -19.84 -11.14   16.33  -0.47  -0.28   \n",
       "1          0.06    -0.08    -0.08  3.80  -37.84  -4.27   53.71  -0.51   0.03   \n",
       "2          0.11    -0.13    -0.02  3.92    2.75  -1.68   36.01   0.07  -0.10   \n",
       "3          0.08     0.36     0.33  3.81  -18.92 -64.70  118.26  -0.13   0.07   \n",
       "4          0.26    -0.13    -0.21  3.87    5.19  45.62   -2.90   0.15   0.67   \n",
       "5         -0.01     0.04    -0.05  3.91   -3.51  18.31   61.04   0.20  -0.02   \n",
       "6          0.10     0.06    -0.26  3.79    8.85 -50.35  -32.35   0.15  -1.09   \n",
       "7          0.11     0.29    -0.22  0.00  -14.34 -24.11   85.60  -0.28  -0.09   \n",
       "8         -0.12     0.06    -0.06  3.94  -31.13 -19.23   33.87  -0.37  -0.39   \n",
       "9         -0.01     0.01    -0.16  3.80  -56.00   1.37   29.91  -0.50   0.10   \n",
       "10         0.04    -0.03    -0.01  3.92   10.53  -7.32   64.24   0.13   0.00   \n",
       "11         0.21     0.33    -0.01  3.81  -13.58 -64.85    7.32   0.03  -0.42   \n",
       "12         0.43    -0.03    -0.02  3.87    7.48  67.75   37.99   0.04   0.72   \n",
       "13        -0.01    -0.02    -0.05  3.91   26.40   3.81   61.49   0.17  -0.07   \n",
       "14         0.21    -0.00    -0.03  3.79   16.17 -65.77   -1.83   0.20  -1.04   \n",
       "15        -0.00     0.19    -0.02  0.00  -39.06  18.62   69.12  -0.31   0.10   \n",
       "16         0.04     0.05     0.02  3.92   14.04   7.93   55.54   0.15   0.09   \n",
       "17         0.00     0.00    -0.02  3.80  -59.51  -0.31   93.69  -0.60   0.08   \n",
       "18        -0.01     0.08    -0.03  3.94  -36.01 -13.12   54.47  -0.35  -0.35   \n",
       "19        -0.12     0.09     0.01  3.81   21.36 -20.60   82.86  -0.04  -0.03   \n",
       "20         0.29     0.05    -0.02  3.87   17.85  71.72   24.41   0.10   0.77   \n",
       "21        -0.11     0.17    -0.01  3.91   19.84   5.95   92.77   0.34   0.10   \n",
       "22         0.27    -0.05    -0.26  3.79    7.02 -74.31    2.44   0.35  -0.95   \n",
       "23        -0.01     0.16     0.09  0.00  -22.28  32.50  104.06  -0.38   0.20   \n",
       "24         0.04     0.03    -0.11  3.92   13.12   9.77   85.14   0.14   0.10   \n",
       "25         0.00     0.09     0.11  3.80  -35.86  18.31   78.58  -0.71   0.12   \n",
       "26         0.09     0.13    -0.13  3.94  -21.97 -21.51   90.33  -0.22  -0.24   \n",
       "27         0.15     0.10    -0.05  3.91   19.99  -8.39   61.65   0.12   0.17   \n",
       "28         0.15     0.02    -0.04  3.87   17.70  78.58   24.87   0.12   0.95   \n",
       "29         0.20    -0.05    -0.04  3.81   12.66 -25.63   51.27   0.34  -0.08   \n",
       "...         ...      ...      ...   ...     ...    ...     ...    ...    ...   \n",
       "986061     0.15     0.16    -0.13  3.74   35.71 -16.94  -22.74  -0.27  -0.15   \n",
       "986062    -0.45    -0.07     0.21  3.88  -12.97 -44.71   21.82  -0.08  -0.07   \n",
       "986063     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986064     0.72     0.15     0.66  0.00   49.44 -69.58 -109.86  -0.46   0.07   \n",
       "986065    -0.63    -0.20    -0.11  3.82  233.15 -25.94  -69.58  -0.16   0.19   \n",
       "986066    -0.30    -0.23     0.16  3.77   24.41   1.07  -55.54   0.33  -0.17   \n",
       "986067    -0.02    -0.23    -0.17  3.83   22.13  17.24   29.60   0.36   0.11   \n",
       "986068    -0.10    -0.11     0.14  3.76   -3.20 -84.53  -65.00  -0.08   0.14   \n",
       "986069     0.27     0.11    -0.12  3.74    4.58   5.04  -91.09  -0.25  -0.30   \n",
       "986070    -0.48    -0.12    -0.05  3.88    9.46  22.58   24.26  -0.15   0.04   \n",
       "986071     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986072    -0.44     0.31     0.09  0.00  -28.69 -94.60   11.44   0.36   0.34   \n",
       "986073    -0.47    -0.28    -0.56  3.82  106.35  -0.61  -69.12  -0.18   0.11   \n",
       "986074    -0.36    -0.11    -0.08  3.77   44.40   5.65   38.76   0.13  -0.09   \n",
       "986075    -0.14    -0.16     0.03  3.83   12.05  -4.58   30.67   0.24   0.08   \n",
       "986076    -0.16    -0.07     0.11  3.76   35.71 -86.06 -163.88   0.03   0.10   \n",
       "986077     0.38     0.06     0.10  3.74   47.00   2.75    3.97  -0.22  -0.42   \n",
       "986078    -0.21    -0.09    -0.14  3.89   -5.95   3.66   -4.73   0.09   0.01   \n",
       "986079     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986080    -0.05    -0.14    -0.10  0.00  -22.58 -64.24   83.92   0.17   0.14   \n",
       "986081    -0.50    -0.35    -0.53  3.82  -65.46   5.49   14.65  -0.26   0.12   \n",
       "986082    -0.33    -0.26    -0.21  3.77   32.50  50.96  -88.20   0.09   0.09   \n",
       "986083    -0.10    -0.14     0.16  3.83  -27.77  30.52   -3.51   0.27   0.03   \n",
       "986084    -0.10    -0.18    -0.10  3.76   42.72 -35.25  -87.28   0.00   0.28   \n",
       "986085     0.00     0.00     0.00  3.94    0.00   0.00    0.00   0.00   0.00   \n",
       "986086    -0.20    -0.10    -0.24  3.89  -10.68  -0.31  -11.75   0.07   0.00   \n",
       "986087     0.51     0.00     0.27  3.74   41.81   5.04   32.81  -0.22  -0.44   \n",
       "986088     0.20    -0.08    -0.01  0.00  -41.81 -44.40   69.12   0.08   0.19   \n",
       "986089    -0.78    -0.36     0.07  3.82 -240.02 -14.19   66.38  -0.33   0.32   \n",
       "986090    -0.13     0.11    -0.19  3.76   58.90  -7.48  -72.33   0.19   0.15   \n",
       "\n",
       "        Gyroz  activity  \n",
       "0        0.81         7  \n",
       "1        0.77         7  \n",
       "2        0.98         7  \n",
       "3        1.20         7  \n",
       "4        0.35         7  \n",
       "5        0.84         7  \n",
       "6        0.06         7  \n",
       "7        0.82         7  \n",
       "8        0.77         7  \n",
       "9        0.68         7  \n",
       "10       0.98         7  \n",
       "11       1.05         7  \n",
       "12       0.60         7  \n",
       "13       0.94         7  \n",
       "14       0.21         7  \n",
       "15       0.95         7  \n",
       "16       0.99         7  \n",
       "17       0.79         7  \n",
       "18       0.87         7  \n",
       "19       1.02         7  \n",
       "20       0.38         7  \n",
       "21       0.94         7  \n",
       "22       0.16         7  \n",
       "23       1.01         7  \n",
       "24       0.88         7  \n",
       "25       0.81         7  \n",
       "26       0.81         7  \n",
       "27       0.93         7  \n",
       "28       0.21         7  \n",
       "29       0.90         7  \n",
       "...       ...       ...  \n",
       "986061   0.83         8  \n",
       "986062   1.34         8  \n",
       "986063   0.00         8  \n",
       "986064   1.39         8  \n",
       "986065   1.07         8  \n",
       "986066   1.16         8  \n",
       "986067   0.76         8  \n",
       "986068   1.13         8  \n",
       "986069   0.84         8  \n",
       "986070   1.11         8  \n",
       "986071   0.00         8  \n",
       "986072   1.00         8  \n",
       "986073   0.67         8  \n",
       "986074   0.99         8  \n",
       "986075   1.03         8  \n",
       "986076   1.19         8  \n",
       "986077   1.08         8  \n",
       "986078   0.88         8  \n",
       "986079   0.00         8  \n",
       "986080   0.87         8  \n",
       "986081   0.74         8  \n",
       "986082   0.87         8  \n",
       "986083   1.12         8  \n",
       "986084   0.83         8  \n",
       "986085   0.00         8  \n",
       "986086   0.78         8  \n",
       "986087   1.28         8  \n",
       "986088   0.98         8  \n",
       "986089   1.36         8  \n",
       "986090   0.79         8  \n",
       "\n",
       "[986091 rows x 20 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('./calibration_readone_data.csv', error_bad_lines=False)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df = alex_data_df\n",
    "# result_df\n",
    "result_df = data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the sensor label index range from 0 to 7 \n",
    "# sensor_data = []\n",
    "# for i in range(0, 8):\n",
    "#     df = alex_data_df.where(alex_data_df['Sensor'] == i).dropna()\n",
    "#     sensor_data.append(df)\n",
    "# result_df = pd.concat(sensor_data).reset_index(drop=True)\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data processing and deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process original dataset, create windows (window_size samples(rows), about 1 second)\n",
    "data = []\n",
    "window = 1\n",
    "while window*window_size < len(result_df):\n",
    "    data_window = result_df[(window - 1)*window_size:window*window_size]\n",
    "    data.append(data_window.values)\n",
    "    window += 1\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5478"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n",
      "data contains different activities! Window droped\n"
     ]
    }
   ],
   "source": [
    "# delete window if multiple activities and sensors presents\n",
    "cleaned_data = []\n",
    "for i in data:\n",
    "    previous_activity = -1\n",
    "    previous_sensor = -1\n",
    "    for j in i:\n",
    "        current_activity = j[19]\n",
    "        current_sensor = j[1]\n",
    "        if (previous_activity != -1) and (current_activity != previous_activity):\n",
    "            print(\"data contains different activities! Window droped\")\n",
    "            break\n",
    "#         elif (previous_sensor != -1) and (current_sensor != previous_sensor):\n",
    "#             print(\"data contains different sensors! Window droped\")\n",
    "#             break\n",
    "        else:\n",
    "            previous_activity = current_activity\n",
    "            previous_sensor = current_sensor\n",
    "    else:\n",
    "        cleaned_data.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5370"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 20)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "seed(101)\n",
    "shuffle(cleaned_data)\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract label from each window\n",
    "labels = []\n",
    "for i in cleaned_data:\n",
    "    label = i[0][19]\n",
    "    labels.append(label)\n",
    "labels = np.array(labels)\n",
    "#labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each window\n",
    "features = []\n",
    "for i in cleaned_data:\n",
    "    new = np.delete(i, 19, 1)\n",
    "    features.append(new)\n",
    "features = np.array(features)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 19)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1.689, 0.0, -0.189, -0.14, -0.18, -0.96, 169...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[22.592, 6.0, 0.14400000000000002, 0.14, -0.0...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[33.924, 2.0, 0.209, 0.38, 0.78, -0.44, 160.5...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[2.4442, 5.0, 0.644, 0.19, 0.3, 0.68, 109.8, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[19.282, 6.0, 0.8759999999999999, -0.05, -0.0...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[[1.013, 0.0, 0.125, 0.48, 0.12, -0.86, 205.11...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[[1.26, 5.0, 0.931, 0.02, 0.08, 0.36, 56.3, 7....</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[[3.2235, 2.0, 0.18600000000000005, 0.19, 0.2,...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[[39.567, 7.0, 0.269, -0.26, 0.1, 0.92, 162.54...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[[1.14, 5.0, -0.121, -0.78, -0.57, -0.23, 89.2...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[[0.718, 2.0, -0.7170000000000001, -0.24, 0.17...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[[0.307, 0.0, 0.032, 0.44, 0.29, -0.85, 176.04...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[[9.1971, 3.0, 0.617, -0.04, 0.07, 0.78, 117.7...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[[30.299, 2.0, -0.846, -0.24, -0.31, -0.36, 68...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[[1.242, 4.0, 0.306, -0.77, 0.54, -0.14, 306.4...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[[1.002, 3.0, -0.82, 0.01, -0.1, -0.56, 83.5, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[[37.087, 1.0, 0.931, 0.09, -0.07, 0.35, 54.35...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[[1.0162, 3.0, 0.452, -0.4, 0.02, 0.8, 125.35,...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[[0.49, 1.0, -0.118, 0.07, 0.03, 0.99, 207.46,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[[1.999, 5.0, 0.527, 0.5, 0.61, 0.31, 100.59, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[[28.965, 0.0, 0.688, 0.05, -0.07, 0.72, 106.8...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[[1.72, 2.0, -0.723, -0.02, 0.13, 0.68, 286.67...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[[0.59, 6.0, -0.968, 0.02, 0.12, 0.22, 348.75,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[[22.549, 7.0, 0.999, 0.04, 0.02, 0.03, 17.07,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[[0.11, 4.0, -0.484, 0.56, -0.67, 0.08, 291.0,...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[[48.4, 3.0, 0.049, -0.24, 0.14, -0.96, 204.55...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[[14.119000000000002, 1.0, 0.946, 0.07, -0.02,...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[[2.901, 7.0, 0.921, 0.12, 0.12, 0.35, 56.82, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[[2.65, 5.0, 0.466, 0.78, 0.34, 0.24, 62.64, -...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[[3.3, 6.0, -0.5710000000000001, 0.42, -0.32, ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5340</th>\n",
       "      <td>[[3.237, 3.0, -0.586, 0.05, 0.01, -0.81, 121.9...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5341</th>\n",
       "      <td>[[11.201, 5.0, 0.8740000000000001, 0.01, 0.02,...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5342</th>\n",
       "      <td>[[18.517, 7.0, 0.779, 0.29, 0.56, 0.04, 59.04,...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5343</th>\n",
       "      <td>[[4.3538, 3.0, 0.166, 0.81, 0.55, -0.13, 80.28...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5344</th>\n",
       "      <td>[[2.122, 2.0, 0.8270000000000001, 0.15, 0.13, ...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5345</th>\n",
       "      <td>[[1.194, 6.0, 0.61, 0.29, 0.67, -0.3, 180.42, ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5346</th>\n",
       "      <td>[[34.435, 3.0, 0.982, -0.01, 0.15, -0.12, 359....</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5347</th>\n",
       "      <td>[[23.459, 6.0, -0.684, 0.28, -0.15, -0.66, 97....</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5348</th>\n",
       "      <td>[[14.479, 3.0, 0.325, -0.01, 0.15, 0.93, 156.6...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5349</th>\n",
       "      <td>[[3.3480000000000003, 0.0, 0.358, 0.08, -0.04,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>[[17.883, 3.0, -0.978, -0.05, -0.14, -0.14, 31...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351</th>\n",
       "      <td>[[5.8886, 6.0, 0.958, -0.08, -0.13, 0.24, 43.9...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5352</th>\n",
       "      <td>[[15.144, 1.0, 0.026, -0.21, -0.06, 0.97, 189....</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>[[13.802, 7.0, 0.82, 0.25, 0.41, 0.31, 70.51, ...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5354</th>\n",
       "      <td>[[36.62, 0.0, 0.008, 0.01, 0.04, 1.0, 193.14, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>[[2.931, 3.0, -0.085, -0.03, -0.06, 0.99, 203....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>[[3.09, 7.0, 0.805, 0.04, 0.01, -0.59, 301.55,...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>[[3.531, 5.0, -0.7959999999999999, -0.05, -0.1...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>[[4.9597, 6.0, -0.047, -0.23, -0.07, 0.97, 197...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>[[0.275, 6.0, -0.833, 0.05, -0.54, 0.11, 343.0...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>[[2.055, 2.0, 0.425, 0.2, 0.64, -0.61, 218.7, ...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>[[1.931, 1.0, 0.006, 0.0, -0.04, -1.0, 194.74,...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5362</th>\n",
       "      <td>[[2.296, 1.0, -0.085, 0.53, -0.02, 0.84, 214.9...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>[[0.912, 0.0, -0.114, 0.06, -0.06, -0.99, 181....</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>[[0.138, 5.0, -0.731, 0.02, -0.19, -0.65, 99.8...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5365</th>\n",
       "      <td>[[10.424, 1.0, 0.054000000000000006, 0.05, 0.0...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5366</th>\n",
       "      <td>[[1.3, 4.0, -0.387, 0.66, -0.54, 0.35, 294.23,...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5367</th>\n",
       "      <td>[[0.215, 0.0, 0.736, 0.14, 0.08, -0.66, 291.59...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5368</th>\n",
       "      <td>[[44.11, 2.0, 0.8590000000000001, -0.2, -0.43,...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5369</th>\n",
       "      <td>[[28.82, 4.0, -0.245, -0.02, -0.3, 0.92, 220.5...</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5370 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               features  labels\n",
       "0     [[1.689, 0.0, -0.189, -0.14, -0.18, -0.96, 169...     4.0\n",
       "1     [[22.592, 6.0, 0.14400000000000002, 0.14, -0.0...     7.0\n",
       "2     [[33.924, 2.0, 0.209, 0.38, 0.78, -0.44, 160.5...     1.0\n",
       "3     [[2.4442, 5.0, 0.644, 0.19, 0.3, 0.68, 109.8, ...     4.0\n",
       "4     [[19.282, 6.0, 0.8759999999999999, -0.05, -0.0...     1.0\n",
       "5     [[1.013, 0.0, 0.125, 0.48, 0.12, -0.86, 205.11...     4.0\n",
       "6     [[1.26, 5.0, 0.931, 0.02, 0.08, 0.36, 56.3, 7....    13.0\n",
       "7     [[3.2235, 2.0, 0.18600000000000005, 0.19, 0.2,...    14.0\n",
       "8     [[39.567, 7.0, 0.269, -0.26, 0.1, 0.92, 162.54...     0.0\n",
       "9     [[1.14, 5.0, -0.121, -0.78, -0.57, -0.23, 89.2...     4.0\n",
       "10    [[0.718, 2.0, -0.7170000000000001, -0.24, 0.17...    13.0\n",
       "11    [[0.307, 0.0, 0.032, 0.44, 0.29, -0.85, 176.04...    13.0\n",
       "12    [[9.1971, 3.0, 0.617, -0.04, 0.07, 0.78, 117.7...     7.0\n",
       "13    [[30.299, 2.0, -0.846, -0.24, -0.31, -0.36, 68...     9.0\n",
       "14    [[1.242, 4.0, 0.306, -0.77, 0.54, -0.14, 306.4...     4.0\n",
       "15    [[1.002, 3.0, -0.82, 0.01, -0.1, -0.56, 83.5, ...     4.0\n",
       "16    [[37.087, 1.0, 0.931, 0.09, -0.07, 0.35, 54.35...     9.0\n",
       "17    [[1.0162, 3.0, 0.452, -0.4, 0.02, 0.8, 125.35,...    13.0\n",
       "18    [[0.49, 1.0, -0.118, 0.07, 0.03, 0.99, 207.46,...     4.0\n",
       "19    [[1.999, 5.0, 0.527, 0.5, 0.61, 0.31, 100.59, ...     4.0\n",
       "20    [[28.965, 0.0, 0.688, 0.05, -0.07, 0.72, 106.8...     7.0\n",
       "21    [[1.72, 2.0, -0.723, -0.02, 0.13, 0.68, 286.67...    13.0\n",
       "22    [[0.59, 6.0, -0.968, 0.02, 0.12, 0.22, 348.75,...     4.0\n",
       "23    [[22.549, 7.0, 0.999, 0.04, 0.02, 0.03, 17.07,...     1.0\n",
       "24    [[0.11, 4.0, -0.484, 0.56, -0.67, 0.08, 291.0,...    13.0\n",
       "25    [[48.4, 3.0, 0.049, -0.24, 0.14, -0.96, 204.55...     5.0\n",
       "26    [[14.119000000000002, 1.0, 0.946, 0.07, -0.02,...     8.0\n",
       "27    [[2.901, 7.0, 0.921, 0.12, 0.12, 0.35, 56.82, ...     4.0\n",
       "28    [[2.65, 5.0, 0.466, 0.78, 0.34, 0.24, 62.64, -...     4.0\n",
       "29    [[3.3, 6.0, -0.5710000000000001, 0.42, -0.32, ...     4.0\n",
       "...                                                 ...     ...\n",
       "5340  [[3.237, 3.0, -0.586, 0.05, 0.01, -0.81, 121.9...     4.0\n",
       "5341  [[11.201, 5.0, 0.8740000000000001, 0.01, 0.02,...    14.0\n",
       "5342  [[18.517, 7.0, 0.779, 0.29, 0.56, 0.04, 59.04,...    10.0\n",
       "5343  [[4.3538, 3.0, 0.166, 0.81, 0.55, -0.13, 80.28...     1.0\n",
       "5344  [[2.122, 2.0, 0.8270000000000001, 0.15, 0.13, ...    13.0\n",
       "5345  [[1.194, 6.0, 0.61, 0.29, 0.67, -0.3, 180.42, ...     3.0\n",
       "5346  [[34.435, 3.0, 0.982, -0.01, 0.15, -0.12, 359....     7.0\n",
       "5347  [[23.459, 6.0, -0.684, 0.28, -0.15, -0.66, 97....     2.0\n",
       "5348  [[14.479, 3.0, 0.325, -0.01, 0.15, 0.93, 156.6...     2.0\n",
       "5349  [[3.3480000000000003, 0.0, 0.358, 0.08, -0.04,...     1.0\n",
       "5350  [[17.883, 3.0, -0.978, -0.05, -0.14, -0.14, 31...     3.0\n",
       "5351  [[5.8886, 6.0, 0.958, -0.08, -0.13, 0.24, 43.9...     9.0\n",
       "5352  [[15.144, 1.0, 0.026, -0.21, -0.06, 0.97, 189....     5.0\n",
       "5353  [[13.802, 7.0, 0.82, 0.25, 0.41, 0.31, 70.51, ...    10.0\n",
       "5354  [[36.62, 0.0, 0.008, 0.01, 0.04, 1.0, 193.14, ...     0.0\n",
       "5355  [[2.931, 3.0, -0.085, -0.03, -0.06, 0.99, 203....     4.0\n",
       "5356  [[3.09, 7.0, 0.805, 0.04, 0.01, -0.59, 301.55,...    13.0\n",
       "5357  [[3.531, 5.0, -0.7959999999999999, -0.05, -0.1...     4.0\n",
       "5358  [[4.9597, 6.0, -0.047, -0.23, -0.07, 0.97, 197...     7.0\n",
       "5359  [[0.275, 6.0, -0.833, 0.05, -0.54, 0.11, 343.0...     4.0\n",
       "5360  [[2.055, 2.0, 0.425, 0.2, 0.64, -0.61, 218.7, ...    13.0\n",
       "5361  [[1.931, 1.0, 0.006, 0.0, -0.04, -1.0, 194.74,...    13.0\n",
       "5362  [[2.296, 1.0, -0.085, 0.53, -0.02, 0.84, 214.9...     4.0\n",
       "5363  [[0.912, 0.0, -0.114, 0.06, -0.06, -0.99, 181....     4.0\n",
       "5364  [[0.138, 5.0, -0.731, 0.02, -0.19, -0.65, 99.8...    13.0\n",
       "5365  [[10.424, 1.0, 0.054000000000000006, 0.05, 0.0...    10.0\n",
       "5366  [[1.3, 4.0, -0.387, 0.66, -0.54, 0.35, 294.23,...     4.0\n",
       "5367  [[0.215, 0.0, 0.736, 0.14, 0.08, -0.66, 291.59...     0.0\n",
       "5368  [[44.11, 2.0, 0.8590000000000001, -0.2, -0.43,...     5.0\n",
       "5369  [[28.82, 4.0, -0.245, -0.02, -0.3, 0.92, 220.5...    14.0\n",
       "\n",
       "[5370 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the features and labels\n",
    "k = list(zip(features, labels))\n",
    "activity_data = pd.DataFrame(k)\n",
    "activity_data.columns = ['features', 'labels']\n",
    "activity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of activity. The final output of neural net \n",
    "# has to have max_index + 1 output\n",
    "max_index = activity_data['labels'].max()\n",
    "label_size = int(max_index + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Training on GPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"CUDA is available! Training on GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Training on CPU...\")\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to test, validation, and train\n",
    "valid_size = 0.2\n",
    "test_size = 0.2\n",
    "activity_data.columns = [\"features\", \"labels\"]\n",
    "activity_data_train = activity_data[:int(len(activity_data)*(1-valid_size-test_size))]\n",
    "activity_data_valid = activity_data[int(len(activity_data)*(1-valid_size-test_size)):int(len(activity_data)*(1-test_size))]\n",
    "activity_data_test = activity_data[int(len(activity_data)*(1-test_size)):]\n",
    "# activity_data_train.to_csv(\"./activity_data_train.csv\", encoding='utf-8-sig')\n",
    "# activity_data_valid.to_csv(\"./activity_data_valid.csv\", encoding='utf-8-sig')\n",
    "# activity_data_train.to_csv(\"./activity_data_test.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our dataset in pytorch\n",
    "class DatasetSpineTrack(Dataset):\n",
    "    \n",
    "    def __init__(self, file, transform=None):\n",
    "        #self.data = pd.read_csv(file_path)\n",
    "        self.data = file\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # load image as ndarray type (Height * Width * Channels)\n",
    "        # be carefull for converting dtype to np.uint8 [Unsigned integer (0 to 255)]\n",
    "        # in this example, i don't use ToTensor() method of torchvision.transforms\n",
    "        # so you can convert numpy ndarray shape to tensor in PyTorch (H, W, C) --> (C, H, W)\n",
    "        \n",
    "        features = torch.tensor(self.data[\"features\"].iloc[index])\n",
    "        features = features.view(channel, window_size, 19) \n",
    "        labels = torch.tensor(self.data[\"labels\"].iloc[index], dtype=torch.long)\n",
    "        #print(labels.type())\n",
    "        \n",
    "#         if self.transform is not None:\n",
    "#             image = self.transform(image)\n",
    "            \n",
    "        return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training and testing dataset in csv\n",
    "# train_dataset = DatasetSpineTrack(\"./activity_data_train.csv\")\n",
    "# valid_dataset = DatasetSpineTrack(\"./activity_data_valid.csv\")\n",
    "# test_dataset = DatasetSpineTrack(\"./activity_data_test.csv\")\n",
    "train_dataset = DatasetSpineTrack(activity_data_train)\n",
    "valid_dataset = DatasetSpineTrack(activity_data_valid)\n",
    "test_dataset = DatasetSpineTrack(activity_data_test)\n",
    "feature, label = train_dataset.__getitem__(0)\n",
    "#feature\n",
    "#label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Network Architechture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_on_gpu = False\n",
    "# train_on_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=15, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "# window_size channels\n",
    "# model.conv1 = torch.nn.Conv2d(window_size, batch_size, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.conv1 = torch.nn.Conv2d(channel, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "model.fc = torch.nn.Linear(512, label_size, bias=True)\n",
    "model.add_module(\"dropout\", torch.nn.Dropout(p=0.5))\n",
    "model = model.double()\n",
    "\n",
    "# move tensors to GPU is CUDA is available\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Run all above \n",
    "<a name='bookmark' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the Network\n",
    "\n",
    "Remember to look at how the training and validation loss decreases over time; if the validation loss ever increases it indicates possible overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.192740 \tValidation Loss: 1.376116\n",
      "Validation loss decreased (inf --> 1.376116).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.653408 \tValidation Loss: 0.992900\n",
      "Validation loss decreased (1.376116 --> 0.992900).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.563575 \tValidation Loss: 1.880785\n",
      "Epoch: 4 \tTraining Loss: 0.397641 \tValidation Loss: 8.896060\n",
      "Epoch: 5 \tTraining Loss: 0.312598 \tValidation Loss: 2.344358\n",
      "Epoch: 6 \tTraining Loss: 0.343913 \tValidation Loss: 0.491321\n",
      "Validation loss decreased (0.992900 --> 0.491321).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.259472 \tValidation Loss: 0.472596\n",
      "Validation loss decreased (0.491321 --> 0.472596).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.215603 \tValidation Loss: 2.101978\n",
      "Epoch: 9 \tTraining Loss: 0.191101 \tValidation Loss: 0.380959\n",
      "Validation loss decreased (0.472596 --> 0.380959).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.168735 \tValidation Loss: 0.331388\n",
      "Validation loss decreased (0.380959 --> 0.331388).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.152675 \tValidation Loss: 2.297750\n",
      "Epoch: 12 \tTraining Loss: 0.142520 \tValidation Loss: 0.539306\n",
      "Epoch: 13 \tTraining Loss: 0.091712 \tValidation Loss: 0.562057\n",
      "Epoch: 14 \tTraining Loss: 0.142127 \tValidation Loss: 8.571674\n",
      "Epoch: 15 \tTraining Loss: 0.124649 \tValidation Loss: 0.484514\n",
      "Epoch: 16 \tTraining Loss: 0.080519 \tValidation Loss: 0.367611\n",
      "Epoch: 17 \tTraining Loss: 0.073572 \tValidation Loss: 2.697078\n",
      "Epoch: 18 \tTraining Loss: 0.084267 \tValidation Loss: 1.850162\n",
      "Epoch: 19 \tTraining Loss: 0.059538 \tValidation Loss: 1.328342\n",
      "Epoch: 20 \tTraining Loss: 0.076272 \tValidation Loss: 1.806874\n",
      "Epoch: 21 \tTraining Loss: 0.064706 \tValidation Loss: 10.341392\n",
      "Epoch: 22 \tTraining Loss: 0.072195 \tValidation Loss: 0.337069\n",
      "Epoch: 23 \tTraining Loss: 0.050196 \tValidation Loss: 0.311751\n",
      "Validation loss decreased (0.331388 --> 0.311751).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.055269 \tValidation Loss: 0.785280\n",
      "Epoch: 25 \tTraining Loss: 0.115044 \tValidation Loss: 0.372656\n",
      "Epoch: 26 \tTraining Loss: 0.042289 \tValidation Loss: 0.326639\n",
      "Epoch: 27 \tTraining Loss: 0.045222 \tValidation Loss: 0.520045\n",
      "Epoch: 28 \tTraining Loss: 0.078994 \tValidation Loss: 0.233756\n",
      "Validation loss decreased (0.311751 --> 0.233756).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.041823 \tValidation Loss: 0.192920\n",
      "Validation loss decreased (0.233756 --> 0.192920).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.066713 \tValidation Loss: 8.701684\n",
      "Training time: 23 min 58 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for features, labels in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*features.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for features, labels in validloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            features, labels = features.cuda(), labels.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(features)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*features.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_Spinetrack_data.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "# output running time\n",
    "running_time = time.time() - start_time\n",
    "sec = running_time % 60\n",
    "miniute = running_time / 60\n",
    "print(\"Training time: {} min {} sec\".format(int(miniute), int(sec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load the Model with the Lowest Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load on gpu\n",
    "# model.load_state_dict(torch.load('model_Spinetrack_3.pt'))\n",
    "\n",
    "# load on cpu\n",
    "model.load_state_dict(torch.load('model_Spinetrack_6.pt', map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the Trained Network\n",
    "\n",
    "Test your trained model on previously unseen data! A \"good\" result will be a result that gets more than 70% accuracy on these test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release all the GPU memory cache that can be freed\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy and each class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.252560\n",
      "\n",
      "Test Accuracy of Pulling_OneH: 95% (58/61)\n",
      "Test Accuracy of Overhead: 95% (64/67)\n",
      "Test Accuracy of Pulling: 95% (40/42)\n",
      "Test Accuracy of Sitting: 100% (24/24)\n",
      "Test Accuracy of Lifting: 96% (328/339)\n",
      "Test Accuracy of Crawling: 94% (34/36)\n",
      "Test Accuracy of Standing: 100% (31/31)\n",
      "Test Accuracy of Carrying: 93% (83/89)\n",
      "Test Accuracy of Walking: 100% (42/42)\n",
      "Test Accuracy of Pushing: 86% (58/67)\n",
      "Test Accuracy of Reaching: 92% (82/89)\n",
      "Test Accuracy of Static_Stoop: 96% (27/28)\n",
      "Test Accuracy of Kneeling: 100% (26/26)\n",
      "Test Accuracy of Lifting_OneH: 75% (72/95)\n",
      "Test Accuracy of Crouching: 71% (27/38)\n",
      "\n",
      "Test Accuracy (Overall): 92% (996/1074)\n"
     ]
    }
   ],
   "source": [
    "# track test loss\n",
    "test_loss = 0.0\n",
    "class_correct = list(0. for i in range(len(index_dict)))\n",
    "class_total = list(0. for i in range(len(index_dict)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "model.eval()\n",
    "torch.no_grad()\n",
    "# iterate over test data\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model(features)\n",
    "    # calculate the batch loss\n",
    "    loss = criterion(output, labels)\n",
    "    # update test loss \n",
    "    test_loss += loss.item()*features.size(0)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output, 1)    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    # calculate test accuracy for each object class\n",
    "    for i in range(batch_size):\n",
    "        try:\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(testloader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(len(index_dict)):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            list(index_dict.keys())[i], 100 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (list(index_dict.keys())[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall accuracy (different calculation method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off gradients for validation, saves memory and computations\n",
    "torch.no_grad()\n",
    "accuracy = 0\n",
    "for features, labels in testloader:\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if train_on_gpu:\n",
    "        features, labels = features.cuda(), labels.cuda()\n",
    "    loss = model(features)\n",
    "    test_loss += criterion(loss, labels)\n",
    "\n",
    "#     ps = torch.exp(loss)\n",
    "    top_p, top_class = loss.topk(1, dim=1)\n",
    "    equals = top_class == labels.view(*top_class.shape)\n",
    "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "print(\"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Figure out pulling_OneH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
